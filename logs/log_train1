Setting up Multi Scale Gradient loss...
Done
---- Distributed Training ----
Added key: store_based_barrier_key:1 to store for rank: 0
Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.
/root/miniconda3/envs/scv/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Trainable parameters: 20545951
S2DepthTransformerUNetConv(
  (encoder): LongSpikeStreamEncoderConv(
    (swin3d): SwinTransformer3D(
      (patch_embed): PatchEmbedLocalGlobal(
        (head): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (global_head): Conv2d(128, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (residual_encoding): residual_feature_generator(
          (resblock1): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (resblock2): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (resblock3): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (resblock4): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (global_residual_encoding): residual_feature_generator(
          (resblock1): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (resblock2): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (resblock3): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (resblock4): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (proj): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (global_proj): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock3D(
              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): Identity()
              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock3D(
              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=384, out_features=192, bias=False)
            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock3D(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock3D(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=768, out_features=384, bias=False)
            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock3D(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock3D(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (2): SwinTransformerBlock3D(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (3): SwinTransformerBlock3D(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (4): SwinTransformerBlock3D(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (5): SwinTransformerBlock3D(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (conv_layers): ModuleList(
      (0): ModuleList(
        (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
        (1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
        (3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ModuleList(
        (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
        (1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
        (2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
        (3): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ModuleList(
        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
        (3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (resblocks): ModuleList(
    (0): ResidualBlock(
      (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu): ReLU()
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (1): ResidualBlock(
      (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu): ReLU()
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (decoders): ModuleList(
    (0): UpsampleConvLayer(
      (conv2d): Conv2d(384, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    )
    (1): UpsampleConvLayer(
      (conv2d): Conv2d(192, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    )
    (2): UpsampleConvLayer(
      (conv2d): Conv2d(96, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    )
  )
  (pred): ConvLayer(
    (conv2d): Conv2d(48, 1, kernel_size=(1, 1), stride=(1, 1))
  )
)
Setting up Multi Scale Gradient loss...
Done
Use GPU: 0 for training
Found 26 samples in /root/autodl-tmp/Spike-Stero/train
Found 8 samples in /root/autodl-tmp/Spike-Stero/validation
-----  [3, 6, 12]
---- new version 4 ----
Model Initialized on GPU: 0
Using scale_invariant_loss with config {'weight': 1.0, 'n_lambda': 1.0}
Will not use phased architecture
Using Multi Scale Gradient loss with weight=0.25
Will not use MSE loss
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:347] Warning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1, 48, 1, 1], strides() = [48, 1, 48, 48]
bucket_view.sizes() = [1, 48, 1, 1], strides() = [48, 1, 1, 1] (function operator())
Train Epoch: 1 [0/810 (0%)] loss: 0.0116 L_si: 0.0003 L_grad: 0.0113 
Train Epoch: 1 [12/810 (1%)] loss: 0.0073 L_si: 0.0003 L_grad: 0.0070 
Train Epoch: 1 [24/810 (3%)] loss: 0.0024 L_si: 0.0000 L_grad: 0.0024 
Train Epoch: 1 [36/810 (4%)] loss: 0.0020 L_si: 0.0000 L_grad: 0.0020 
Train Epoch: 1 [48/810 (6%)] loss: 0.0017 L_si: 0.0000 L_grad: 0.0017 
Train Epoch: 1 [60/810 (7%)] loss: 0.0011 L_si: 0.0000 L_grad: 0.0011 
Train Epoch: 1 [72/810 (9%)] loss: 0.0009 L_si: 0.0000 L_grad: 0.0009 
Train Epoch: 1 [84/810 (10%)] loss: 0.0007 L_si: 0.0000 L_grad: 0.0007 
Train Epoch: 1 [96/810 (12%)] loss: 0.0006 L_si: 0.0000 L_grad: 0.0006 
Train Epoch: 1 [108/810 (13%)] loss: 0.0005 L_si: 0.0000 L_grad: 0.0005 
Train Epoch: 1 [120/810 (15%)] loss: 0.0004 L_si: 0.0000 L_grad: 0.0004 
Train Epoch: 1 [132/810 (16%)] loss: 0.0004 L_si: 0.0000 L_grad: 0.0004 
Train Epoch: 1 [144/810 (18%)] loss: 0.0003 L_si: 0.0000 L_grad: 0.0003 
Train Epoch: 1 [156/810 (19%)] loss: 0.0003 L_si: 0.0000 L_grad: 0.0003 
Train Epoch: 1 [168/810 (21%)] loss: 0.0003 L_si: 0.0000 L_grad: 0.0003 
Train Epoch: 1 [180/810 (22%)] loss: 0.0003 L_si: 0.0000 L_grad: 0.0003 
Train Epoch: 1 [192/810 (24%)] loss: 0.0003 L_si: 0.0000 L_grad: 0.0003 
Train Epoch: 1 [204/810 (25%)] loss: 0.0003 L_si: 0.0000 L_grad: 0.0003 
Train Epoch: 1 [216/810 (27%)] loss: 0.0002 L_si: 0.0000 L_grad: 0.0002 
Train Epoch: 1 [228/810 (28%)] loss: 0.0002 L_si: 0.0000 L_grad: 0.0002 
Train Epoch: 1 [240/810 (30%)] loss: 0.0002 L_si: 0.0000 L_grad: 0.0002 
Train Epoch: 1 [252/810 (31%)] loss: 0.0002 L_si: 0.0000 L_grad: 0.0002 
Train Epoch: 1 [264/810 (33%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [276/810 (34%)] loss: 0.0002 L_si: 0.0000 L_grad: 0.0002 
Train Epoch: 1 [288/810 (36%)] loss: 0.0002 L_si: 0.0000 L_grad: 0.0002 
Train Epoch: 1 [300/810 (37%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [312/810 (39%)] loss: 0.0002 L_si: 0.0000 L_grad: 0.0002 
Train Epoch: 1 [324/810 (40%)] loss: 0.0002 L_si: 0.0000 L_grad: 0.0002 
Train Epoch: 1 [336/810 (41%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [348/810 (43%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [360/810 (44%)] loss: 0.0002 L_si: 0.0000 L_grad: 0.0002 
Train Epoch: 1 [372/810 (46%)] loss: 0.0002 L_si: 0.0000 L_grad: 0.0002 
Train Epoch: 1 [384/810 (47%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [396/810 (49%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [408/810 (50%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [420/810 (52%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [432/810 (53%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [444/810 (55%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [456/810 (56%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [468/810 (58%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [480/810 (59%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [492/810 (61%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [504/810 (62%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [516/810 (64%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [528/810 (65%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [540/810 (67%)] loss: 0.0001 L_si: -0.0000 L_grad: 0.0001 
Train Epoch: 1 [552/810 (68%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [564/810 (70%)] loss: 0.0001 L_si: -0.0000 L_grad: 0.0001 
Train Epoch: 1 [576/810 (71%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [588/810 (73%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [600/810 (74%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [612/810 (76%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [624/810 (77%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [636/810 (79%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [648/810 (80%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [660/810 (81%)] loss: 0.0001 L_si: -0.0000 L_grad: 0.0001 
Train Epoch: 1 [672/810 (83%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [684/810 (84%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [696/810 (86%)] loss: 0.0001 L_si: -0.0000 L_grad: 0.0001 
Train Epoch: 1 [708/810 (87%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [720/810 (89%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [732/810 (90%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [744/810 (92%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [756/810 (93%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [768/810 (95%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [780/810 (96%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 1 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 1 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [4.719076241599396e-05, 4.332277239882387e-05, 4.070311115356162e-05, 5.25054638274014e-05, 5.1684779464267194e-05, 4.87076431454625e-05, 4.704202729044482e-05, 4.03190242650453e-05, 4.9534417485119775e-05, 4.278995038475841e-05, 4.501316652749665e-05, 4.1355688153998926e-05, 5.0670609198277816e-05, 4.843239003093913e-05, 4.251821883372031e-05, 4.8493293434148654e-05, 4.9347407184541225e-05, 4.623379572876729e-05, 5.01619724673219e-05, 4.7017878387123346e-05, 4.6904271584935486e-05, 5.329689884092659e-05, 5.064477227278985e-05, 4.702143633039668e-05, 3.730045136762783e-05, 4.2750158172566444e-05, 3.818509867414832e-05, 4.475559398997575e-05, 5.0650221965042874e-05, 4.0776922105578706e-05, 4.023170185973868e-05, 4.08746964239981e-05, 4.909319250145927e-05, 3.9290491258725524e-05, 4.2881263652816415e-05, 4.181330223218538e-05, 2.298541585332714e-05], 'L_si': [0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 8.940696716308594e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 5.960464477539063e-08, 8.940696716308594e-08, 0.0, 0.0, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 0.0, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 8.940696716308594e-08, 0.0, 0.0, -2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, -2.9802322387695312e-08], 'L_grad': [4.719076241599396e-05, 4.329297007643618e-05, 4.0673308831173927e-05, 5.25054638274014e-05, 5.16549771418795e-05, 4.86778408230748e-05, 4.6952620323281735e-05, 4.0289221942657605e-05, 4.9534417485119775e-05, 4.276014806237072e-05, 4.501316652749665e-05, 4.132588583161123e-05, 5.070041152066551e-05, 4.846219235332683e-05, 4.245861418894492e-05, 4.8433688789373264e-05, 4.925800021737814e-05, 4.623379572876729e-05, 5.01619724673219e-05, 4.7017878387123346e-05, 4.687446926254779e-05, 5.329689884092659e-05, 5.064477227278985e-05, 4.702143633039668e-05, 3.730045136762783e-05, 4.272035585017875e-05, 3.8214900996536016e-05, 4.4785396312363446e-05, 5.062041964265518e-05, 4.068751513841562e-05, 4.023170185973868e-05, 4.08746964239981e-05, 4.9122994823846966e-05, 3.926068893633783e-05, 4.2881263652816415e-05, 4.1843104554573074e-05, 2.3015218175714836e-05]}
Train Epoch: 2 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [24/810 (3%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 2 [36/810 (4%)] loss: 0.0001 L_si: -0.0000 L_grad: 0.0001 
Train Epoch: 2 [48/810 (6%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 2 [60/810 (7%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 2 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [96/810 (12%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 2 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [156/810 (19%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [216/810 (27%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [240/810 (30%)] loss: 0.0001 L_si: 0.0000 L_grad: 0.0001 
Train Epoch: 2 [252/810 (31%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [372/810 (46%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [384/810 (47%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [492/810 (61%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [528/810 (65%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [540/810 (67%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [612/810 (76%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [636/810 (79%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [720/810 (89%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [744/810 (92%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 2 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 2 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [2.971727190015372e-05, 2.7468497137306258e-05, 2.6117757442989387e-05, 3.249399742344394e-05, 3.161117638228461e-05, 3.0261511710705236e-05, 2.9189681299612857e-05, 2.587248309282586e-05, 3.0736518965568393e-05, 2.702081110328436e-05, 2.8191427190904506e-05, 2.6277193683199584e-05, 3.124256181763485e-05, 3.0124901968520135e-05, 2.6886318664764985e-05, 3.020057556568645e-05, 3.0672272259835154e-05, 2.8795697289751843e-05, 3.114444189122878e-05, 2.9396265745162964e-05, 2.949894405901432e-05, 3.2866075343918055e-05, 3.167041722917929e-05, 2.938312172773294e-05, 2.459817005728837e-05, 2.7166974177816883e-05, 2.4711589503567666e-05, 2.825969386321958e-05, 3.122070120298304e-05, 2.6294603230780922e-05, 2.5855904823401943e-05, 2.633093754411675e-05, 3.0742172384634614e-05, 2.534355371608399e-05, 2.7387539375922643e-05, 2.659092569956556e-05, 1.433259603800252e-05], 'L_si': [2.9802322387695312e-08, 2.9802322387695312e-08, 8.940696716308594e-08, 5.960464477539063e-08, -2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, -5.960464477539063e-08, 0.0, -5.960464477539063e-08, -2.9802322387695312e-08, 0.0, 0.0, 0.0, 5.960464477539063e-08, -5.960464477539063e-08, 5.960464477539063e-08, -5.960464477539063e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 8.940696716308594e-08, 5.960464477539063e-08, 0.0, 0.0, 2.9802322387695312e-08, 8.940696716308594e-08, -8.940696716308594e-08, -2.9802322387695312e-08], 'L_grad': [2.9687469577766024e-05, 2.7438694814918563e-05, 2.60283504758263e-05, 3.243439277866855e-05, 3.164097870467231e-05, 3.0261511710705236e-05, 2.9219483622000553e-05, 2.593208773760125e-05, 3.0736518965568393e-05, 2.708041574805975e-05, 2.82212295132922e-05, 2.6277193683199584e-05, 3.124256181763485e-05, 3.0124901968520135e-05, 2.6826714019989595e-05, 3.026018021046184e-05, 3.061266761505976e-05, 2.8855301934527233e-05, 3.1114639568841085e-05, 2.9396265745162964e-05, 2.9469141736626625e-05, 3.2866075343918055e-05, 3.16108125844039e-05, 2.9353319405345246e-05, 2.453856541251298e-05, 2.7166974177816883e-05, 2.468178718117997e-05, 2.825969386321958e-05, 3.122070120298304e-05, 2.6205196263617836e-05, 2.5796300178626552e-05, 2.633093754411675e-05, 3.0742172384634614e-05, 2.5313751393696293e-05, 2.7298132408759557e-05, 2.6680332666728646e-05, 1.4362398360390216e-05]}
Train Epoch: 3 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 3 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [96/810 (12%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 3 [108/810 (13%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 3 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [192/810 (24%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 3 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [276/810 (34%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 3 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [348/810 (43%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 3 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [396/810 (49%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 3 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [552/810 (68%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 3 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [588/810 (73%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 3 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [720/810 (89%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 3 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 3 [792/810 (98%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 3 [804/810 (99%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [1.8519471268518828e-05, 1.736191552481614e-05, 1.626143784960732e-05, 2.0497445802902803e-05, 2.0343566575320438e-05, 1.902151052490808e-05, 1.8508841094444506e-05, 1.6209125533350743e-05, 1.9671913833008148e-05, 1.7166772522614338e-05, 1.764155604178086e-05, 1.6629002857371233e-05, 1.968338619917631e-05, 1.892105137812905e-05, 1.6972107914625667e-05, 1.925878314068541e-05, 1.92512379726395e-05, 1.8227490727440454e-05, 1.977150168386288e-05, 1.8596540030557662e-05, 1.867955688794609e-05, 2.0529199900920503e-05, 1.976474595721811e-05, 1.8664733943296596e-05, 1.5275027180905454e-05, 1.737522688927129e-05, 1.552285539219156e-05, 1.7915803255164064e-05, 1.973499274754431e-05, 1.6384892660425976e-05, 1.6172380128409714e-05, 1.6805130144348368e-05, 1.9754612367250957e-05, 1.62258274940541e-05, 1.705301838228479e-05, 1.6779385987319984e-05, 9.028221938933711e-06], 'L_si': [-2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 5.960464477539063e-08, -5.960464477539063e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 0.0], 'L_grad': [1.8549273590906523e-05, 1.7332113202428445e-05, 1.632104249438271e-05, 2.0527248125290498e-05, 2.0313764252932742e-05, 1.8991708202520385e-05, 1.847903877205681e-05, 1.623892785573844e-05, 1.9642111510620452e-05, 1.7136970200226642e-05, 1.770116068655625e-05, 1.6629002857371233e-05, 1.9653583876788616e-05, 1.886144673335366e-05, 1.7031712559401058e-05, 1.9288585463073105e-05, 1.9221435650251806e-05, 1.819768840505276e-05, 1.977150168386288e-05, 1.8596540030557662e-05, 1.8649754565558396e-05, 2.0559002223308198e-05, 1.9734943634830415e-05, 1.8664733943296596e-05, 1.530482950329315e-05, 1.737522688927129e-05, 1.5493053069803864e-05, 1.788600093277637e-05, 1.973499274754431e-05, 1.635509033803828e-05, 1.620218245079741e-05, 1.6775327821960673e-05, 1.972481004486326e-05, 1.6196025171666406e-05, 1.7023216059897095e-05, 1.6779385987319984e-05, 9.028221938933711e-06]}
Train Epoch: 4 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [36/810 (4%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [96/810 (12%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [120/810 (15%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [168/810 (21%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [252/810 (31%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [300/810 (37%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [312/810 (39%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [456/810 (56%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [492/810 (61%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [564/810 (70%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [612/810 (76%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [636/810 (79%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [648/810 (80%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [732/810 (90%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 4 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 4 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
Saving checkpoint: s2d_checkpoints/train_s2d_SpikeTransformer/checkpoint-epoch004-loss-0.0000.pth.tar ...
all losses in batch in validation:  {'loss': [1.3981473784951959e-05, 1.276377406611573e-05, 1.1984319826296996e-05, 1.529178916825913e-05, 1.5077571333677042e-05, 1.4175060641719028e-05, 1.3718364243686665e-05, 1.1899638593604323e-05, 1.4654032383987214e-05, 1.2545006029540673e-05, 1.303825501963729e-05, 1.2278056601644494e-05, 1.4630257282988168e-05, 1.3967272025183775e-05, 1.2447704648366198e-05, 1.423732828698121e-05, 1.4196302799973637e-05, 1.3411456166068092e-05, 1.467477068217704e-05, 1.3748398487223312e-05, 1.3865091204934288e-05, 1.542273639643099e-05, 1.4819725038250908e-05, 1.393710590491537e-05, 1.1245697351114359e-05, 1.2796334885933902e-05, 1.1343389815010596e-05, 1.3156264685676433e-05, 1.4780405763303861e-05, 1.2130572940804996e-05, 1.180507297249278e-05, 1.2434207746991888e-05, 1.475924455007771e-05, 1.1920447832380887e-05, 1.2495812370616477e-05, 1.2416709068929777e-05, 6.651976036664564e-06], 'L_si': [5.960464477539063e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 0.0, -2.9802322387695312e-08, -5.960464477539063e-08, 2.9802322387695312e-08, 0.0, -5.960464477539063e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 0.0, 0.0, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, -2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0], 'L_grad': [1.3921869140176568e-05, 1.2793576388503425e-05, 1.1954517503909301e-05, 1.5261986845871434e-05, 1.5137175978452433e-05, 1.4175060641719028e-05, 1.374816656607436e-05, 1.1959243238379713e-05, 1.4624230061599519e-05, 1.2545006029540673e-05, 1.3097859664412681e-05, 1.2218451956869103e-05, 1.4600454960600473e-05, 1.393746970279608e-05, 1.2507309293141589e-05, 1.423732828698121e-05, 1.4196302799973637e-05, 1.3441258488455787e-05, 1.4704573004564736e-05, 1.3748398487223312e-05, 1.3835288882546593e-05, 1.53631317516556e-05, 1.4819725038250908e-05, 1.3907303582527675e-05, 1.1245697351114359e-05, 1.2796334885933902e-05, 1.13135874926229e-05, 1.3156264685676433e-05, 1.472080111852847e-05, 1.21007706184173e-05, 1.1834875294880476e-05, 1.2404405424604192e-05, 1.469963990530232e-05, 1.1950250154768582e-05, 1.2495812370616477e-05, 1.2386906746542081e-05, 6.651976036664564e-06]}
Train Epoch: 5 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [24/810 (3%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [60/810 (7%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [204/810 (25%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [228/810 (28%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [240/810 (30%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [348/810 (43%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [372/810 (46%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [396/810 (49%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [504/810 (62%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [516/810 (64%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [540/810 (67%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [576/810 (71%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [624/810 (77%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [720/810 (89%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [732/810 (90%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [744/810 (92%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 5 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 5 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
all losses in batch in validation:  {'loss': [1.807417174859438e-05, 1.7430647858418524e-05, 1.6832669643918052e-05, 1.9503964722389355e-05, 1.9403532860451378e-05, 1.8492826711735688e-05, 1.8245893443236127e-05, 1.675571002124343e-05, 1.888860970211681e-05, 1.7344094885629602e-05, 1.761776366038248e-05, 1.695738683338277e-05, 1.887475445983e-05, 1.8322973119211383e-05, 1.7316149751422927e-05, 1.8575405192677863e-05, 1.8682558220461942e-05, 1.7970312910620123e-05, 1.9103970771539025e-05, 1.823474303819239e-05, 1.8182416170020588e-05, 1.9362512830412015e-05, 1.8786839063977823e-05, 1.8380002074991353e-05, 1.619289832888171e-05, 1.7566246242495254e-05, 1.6263960787910037e-05, 1.7746046069078147e-05, 1.8938455468742177e-05, 1.6828362277010456e-05, 1.6647716620354913e-05, 1.7309423128608614e-05, 1.9112536392640322e-05, 1.6770180081948638e-05, 1.7188685887958854e-05, 1.6986490663839504e-05, 8.976428034657147e-06], 'L_si': [-2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 0.0, 0.0, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, -5.960464477539063e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 8.940696716308594e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 0.0, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 5.960464477539063e-08, -5.960464477539063e-08, 0.0, -5.960464477539063e-08, 8.940696716308594e-08], 'L_grad': [1.8103974070982076e-05, 1.740084553603083e-05, 1.677306499914266e-05, 1.9503964722389355e-05, 1.9403532860451378e-05, 1.8492826711735688e-05, 1.8216091120848432e-05, 1.6785512343631126e-05, 1.8918412024504505e-05, 1.7314292563241906e-05, 1.761776366038248e-05, 1.701699147815816e-05, 1.8844952137442306e-05, 1.8382577763986774e-05, 1.728634742903523e-05, 1.8605207515065558e-05, 1.862295357568655e-05, 1.7910708265844733e-05, 1.913377309392672e-05, 1.8264545360580087e-05, 1.8152613847632892e-05, 1.939231515279971e-05, 1.8816641386365518e-05, 1.8409804397379048e-05, 1.6103491361718625e-05, 1.753644392010756e-05, 1.6323565432685427e-05, 1.7746046069078147e-05, 1.8938455468742177e-05, 1.679855995462276e-05, 1.667751894274261e-05, 1.7249818483833224e-05, 1.9052931747864932e-05, 1.682978472672403e-05, 1.7188685887958854e-05, 1.7046095308614895e-05, 8.88702106749406e-06]}
Train Epoch: 6 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [60/810 (7%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 6 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [96/810 (12%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 6 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [180/810 (22%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 6 [192/810 (24%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 6 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [228/810 (28%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 6 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 6 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [468/810 (58%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 6 [480/810 (59%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 6 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 6 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [636/810 (79%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 6 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 6 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [744/810 (92%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 6 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 6 [792/810 (98%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 6 [804/810 (99%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [1.1950626685575116e-05, 1.1043516678910237e-05, 1.056405199051369e-05, 1.2602683455043007e-05, 1.2387938113533892e-05, 1.1772188372560777e-05, 1.1610130059125368e-05, 1.0537873095017858e-05, 1.2154455362178851e-05, 1.0882260539801791e-05, 1.1285495020274539e-05, 1.0631487384671345e-05, 1.2233176676090807e-05, 1.1765121598728001e-05, 1.0963146451103967e-05, 1.2134635653637815e-05, 1.2039447028655559e-05, 1.1488953532534651e-05, 1.2116992365918122e-05, 1.1726240700227208e-05, 1.1713323146977928e-05, 1.2785251783498097e-05, 1.2449995665519964e-05, 1.170665018435102e-05, 1.0104272405442316e-05, 1.1013356925104745e-05, 1.0180287972616497e-05, 1.1395056390028913e-05, 1.2228318155393936e-05, 1.0611222023726441e-05, 1.0563166142674163e-05, 1.0626270523061976e-05, 1.209001311508473e-05, 1.0496807590243407e-05, 1.0876177839236334e-05, 1.0702957297326066e-05, 5.698939276044257e-06], 'L_si': [2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, -5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, 5.960464477539063e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 5.960464477539063e-08, -5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 0.0], 'L_grad': [1.1920824363187421e-05, 1.1073319001297932e-05, 1.0534249668125995e-05, 1.2632485777430702e-05, 1.2387938113533892e-05, 1.1831793017336167e-05, 1.1580327736737672e-05, 1.0508070772630163e-05, 1.2184257684566546e-05, 1.0882260539801791e-05, 1.1225890375499148e-05, 1.060168506228365e-05, 1.2173572031315416e-05, 1.1794923921115696e-05, 1.0933344128716271e-05, 1.2134635653637815e-05, 1.2039447028655559e-05, 1.1459151210146956e-05, 1.2146794688305818e-05, 1.1666636055451818e-05, 1.1743125469365623e-05, 1.2844856428273488e-05, 1.247979798790766e-05, 1.1676847861963324e-05, 1.0044667760666925e-05, 1.0953752280329354e-05, 1.0150485650228802e-05, 1.1335451745253522e-05, 1.2228318155393936e-05, 1.0581419701338746e-05, 1.0503561497898772e-05, 1.0685875167837366e-05, 1.2119815437472425e-05, 1.0526609912631102e-05, 1.090598016162403e-05, 1.0702957297326066e-05, 5.698939276044257e-06]}
Train Epoch: 7 [0/810 (0%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [24/810 (3%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [48/810 (6%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [96/810 (12%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [144/810 (18%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [192/810 (24%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [204/810 (25%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [324/810 (40%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [360/810 (44%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [468/810 (58%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [504/810 (62%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [516/810 (64%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [528/810 (65%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [564/810 (70%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [588/810 (73%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [648/810 (80%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [684/810 (84%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [744/810 (92%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 7 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 7 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [9.02443025552202e-06, 8.263653398898896e-06, 7.941442163428292e-06, 9.694485015643295e-06, 9.509361916570924e-06, 8.988357876660302e-06, 8.742254067328759e-06, 7.953559361340012e-06, 9.168874385068193e-06, 8.363013876078185e-06, 8.506845915690064e-06, 8.001023161341436e-06, 9.3554008344654e-06, 8.99545648280764e-06, 8.381286534131505e-06, 9.064548976311926e-06, 9.222153494192753e-06, 8.723491191631183e-06, 9.289328772865701e-06, 8.87995156517718e-06, 8.847752724250313e-06, 9.869851055555046e-06, 9.529934686725028e-06, 8.95141602086369e-06, 7.516415280406363e-06, 8.178770258382428e-06, 7.773677680233959e-06, 8.546911885787267e-06, 9.56896383286221e-06, 7.988619472598657e-06, 7.9746878327569e-06, 8.093954420473892e-06, 9.109143320529256e-06, 7.904477570264135e-06, 8.335805432579946e-06, 8.217402864829637e-06, 4.364343112683855e-06], 'L_si': [2.9802322387695312e-08, -5.960464477539063e-08, -5.960464477539063e-08, 0.0, 5.960464477539063e-08, 0.0, 0.0, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 8.940696716308594e-08, 5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, 5.960464477539063e-08, -2.9802322387695312e-08, 8.940696716308594e-08, 0.0, -2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 8.940696716308594e-08, 2.9802322387695312e-08], 'L_grad': [8.994627933134325e-06, 8.323258043674286e-06, 8.001046808203682e-06, 9.694485015643295e-06, 9.449757271795534e-06, 8.988357876660302e-06, 8.742254067328759e-06, 7.983361683727708e-06, 9.198676707455888e-06, 8.39281619846588e-06, 8.566450560465455e-06, 8.030825483729132e-06, 9.3554008344654e-06, 9.025258805195335e-06, 8.35148421174381e-06, 9.124153621087316e-06, 9.192351171805058e-06, 8.723491191631183e-06, 9.259526450478006e-06, 8.909753887564875e-06, 8.817950401862618e-06, 9.78044408839196e-06, 9.470330041949637e-06, 8.921613698475994e-06, 7.546217602794059e-06, 8.238374903157819e-06, 7.714073035458568e-06, 8.576714208174963e-06, 9.479556865699124e-06, 7.988619472598657e-06, 8.004490155144595e-06, 8.064152098086197e-06, 9.07934099814156e-06, 7.844872925488744e-06, 8.30600311019225e-06, 8.127995897666551e-06, 4.33454079029616e-06]}
Train Epoch: 8 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [24/810 (3%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [108/810 (13%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [156/810 (19%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [228/810 (28%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [264/810 (33%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [300/810 (37%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [384/810 (47%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [408/810 (50%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [456/810 (56%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [492/810 (61%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [528/810 (65%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [564/810 (70%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [636/810 (79%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [660/810 (81%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [684/810 (84%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [756/810 (93%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [768/810 (95%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 8 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 8 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
Saving checkpoint: s2d_checkpoints/train_s2d_SpikeTransformer/checkpoint-epoch008-loss-0.0000.pth.tar ...
all losses in batch in validation:  {'loss': [7.165302122302819e-06, 6.592568752239458e-06, 6.266721356951166e-06, 8.024687303986866e-06, 7.956844456202816e-06, 7.344845471379813e-06, 7.1312151703750715e-06, 6.225902779988246e-06, 7.479502528440207e-06, 6.590603788936278e-06, 6.790950465074275e-06, 6.383464096870739e-06, 7.646296580787748e-06, 7.257628567458596e-06, 6.617542567255441e-06, 7.289403583854437e-06, 7.460301731043728e-06, 6.9650222940254025e-06, 7.682167051825672e-06, 7.1854351517686155e-06, 7.083810942276614e-06, 7.952143278089352e-06, 7.6441174314823e-06, 7.235172233777121e-06, 5.785047960671363e-06, 6.698097422486171e-06, 5.993387276248541e-06, 6.860000667074928e-06, 7.634844223503023e-06, 6.195060450409073e-06, 6.21865638095187e-06, 6.426235358958365e-06, 7.505756911996286e-06, 6.175263479235582e-06, 6.539979494846193e-06, 6.4305609157599974e-06, 3.4645370305952383e-06], 'L_si': [2.9802322387695312e-08, 0.0, 0.0, 0.0, 5.960464477539063e-08, 5.960464477539063e-08, 5.960464477539063e-08, 0.0, -5.960464477539063e-08, 0.0, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -8.940696716308594e-08, -2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 5.960464477539063e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08], 'L_grad': [7.135499799915124e-06, 6.592568752239458e-06, 6.266721356951166e-06, 8.024687303986866e-06, 7.897239811427426e-06, 7.285240826604422e-06, 7.071610525599681e-06, 6.225902779988246e-06, 7.539107173215598e-06, 6.590603788936278e-06, 6.790950465074275e-06, 6.353661774483044e-06, 7.616493803652702e-06, 7.287430889846291e-06, 6.647344889643136e-06, 7.378810551017523e-06, 7.490104053431423e-06, 6.9650222940254025e-06, 7.711969374213368e-06, 7.15563282938092e-06, 7.083810942276614e-06, 7.952143278089352e-06, 7.614315563841956e-06, 7.205369911389425e-06, 5.814850283059059e-06, 6.6384927777107805e-06, 5.963584953860845e-06, 6.830198344687233e-06, 7.605041446367977e-06, 6.254665095184464e-06, 6.15905173617648e-06, 6.456037681346061e-06, 7.565361556771677e-06, 6.205065801623277e-06, 6.539979494846193e-06, 6.4305609157599974e-06, 3.434734708207543e-06]}
Train Epoch: 9 [0/810 (0%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [96/810 (12%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [108/810 (13%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [132/810 (16%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [156/810 (19%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [192/810 (24%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [240/810 (30%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [324/810 (40%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [348/810 (43%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [360/810 (44%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [528/810 (65%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [540/810 (67%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [672/810 (83%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [756/810 (93%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [768/810 (95%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 9 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 9 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [4.837466804019641e-06, 4.3774507503258064e-06, 4.130910383537412e-06, 5.335665719030658e-06, 5.35690469405381e-06, 4.988824912288692e-06, 4.750928383145947e-06, 4.114573130209465e-06, 5.05809794049128e-06, 4.446130787982838e-06, 4.57297892353381e-06, 4.192870164843043e-06, 5.217228590481682e-06, 4.931298462906852e-06, 4.437311872607097e-06, 4.957098099112045e-06, 5.0531198212411255e-06, 4.756382622872479e-06, 5.110110578243621e-06, 4.753936536872061e-06, 4.8143506319320295e-06, 5.406547188613331e-06, 5.070084625913296e-06, 4.802881903742673e-06, 3.715551201821654e-06, 4.338935013947776e-06, 3.905788616975769e-06, 4.519411504588788e-06, 5.1855072342732456e-06, 4.161031938565429e-06, 4.179318239039276e-06, 4.303934474592097e-06, 4.9414993554819375e-06, 4.012907083961181e-06, 4.364503183751367e-06, 4.321093456383096e-06, 2.2730005184712354e-06], 'L_si': [5.960464477539063e-08, 0.0, 0.0, -2.9802322387695312e-08, 5.960464477539063e-08, 5.960464477539063e-08, 0.0, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 0.0, 8.940696716308594e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 5.960464477539063e-08, -2.9802322387695312e-08, -5.960464477539063e-08, 5.960464477539063e-08, 2.9802322387695312e-08, -5.960464477539063e-08, -2.9802322387695312e-08, -8.940696716308594e-08, 0.0, 0.0, 0.0, 0.0, 0.0, 5.960464477539063e-08, 5.960464477539063e-08, -5.960464477539063e-08, 2.9802322387695312e-08, 0.0, 5.960464477539063e-08, -2.9802322387695312e-08], 'L_grad': [4.7778621592442505e-06, 4.3774507503258064e-06, 4.130910383537412e-06, 5.365468041418353e-06, 5.2973000492784195e-06, 4.929220267513301e-06, 4.750928383145947e-06, 4.114573130209465e-06, 5.028295618103584e-06, 4.416328465595143e-06, 4.543176601146115e-06, 4.222672487230739e-06, 5.157623945706291e-06, 4.931298462906852e-06, 4.347904905444011e-06, 4.897493454336654e-06, 5.02331749885343e-06, 4.696777978097089e-06, 5.139912900631316e-06, 4.813541181647452e-06, 4.754745987156639e-06, 5.376744866225636e-06, 5.1296892706886865e-06, 4.832684226130368e-06, 3.80495816898474e-06, 4.338935013947776e-06, 3.905788616975769e-06, 4.519411504588788e-06, 5.1855072342732456e-06, 4.161031938565429e-06, 4.119713594263885e-06, 4.2443298298167065e-06, 5.001104000257328e-06, 3.983104761573486e-06, 4.364503183751367e-06, 4.2614888116077054e-06, 2.3028028408589307e-06]}
Train Epoch: 10 [0/810 (0%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [84/810 (10%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [108/810 (13%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [192/810 (24%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [264/810 (33%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [276/810 (34%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [312/810 (39%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [396/810 (49%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [468/810 (58%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [516/810 (64%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [528/810 (65%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [540/810 (67%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [612/810 (76%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [624/810 (77%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [636/810 (79%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [648/810 (80%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [684/810 (84%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 10 [780/810 (96%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [792/810 (98%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 10 [804/810 (99%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [3.924792963516666e-06, 3.475699031696422e-06, 3.3167748370033223e-06, 4.331299351179041e-06, 4.210959559713956e-06, 3.925465080101276e-06, 3.7899287690379424e-06, 3.345249297126429e-06, 4.116539003007347e-06, 3.4651989153644536e-06, 3.5981663586426293e-06, 3.3361593523295596e-06, 4.21570666730986e-06, 3.904949608113384e-06, 3.4551749195088632e-06, 4.03131753046182e-06, 4.061125764565077e-06, 3.747275513887871e-06, 4.116052878089249e-06, 3.8396647141780704e-06, 3.894275778293377e-06, 4.419741799210897e-06, 4.203482603770681e-06, 3.796112196141621e-06, 3.0649352993350476e-06, 3.513129286147887e-06, 3.1990748539101332e-06, 3.644134721980663e-06, 4.157735929766204e-06, 3.302448931208346e-06, 3.347976871737046e-06, 3.3953772344830213e-06, 4.04760521632852e-06, 3.361768449394731e-06, 3.5047391975240316e-06, 3.38133349941927e-06, 1.858488644757017e-06], 'L_si': [0.0, -5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 8.940696716308594e-08, 0.0, -5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, -5.960464477539063e-08, 0.0, 0.0, 5.960464477539063e-08, -5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 0.0, 0.0, 5.960464477539063e-08, 0.0, 0.0, 0.0], 'L_grad': [3.924792963516666e-06, 3.5353036764718127e-06, 3.3167748370033223e-06, 4.301497028791346e-06, 4.210959559713956e-06, 3.925465080101276e-06, 3.7899287690379424e-06, 3.315446974738734e-06, 4.0867366806196515e-06, 3.495001237752149e-06, 3.6279686810303247e-06, 3.3361593523295596e-06, 4.126299700146774e-06, 3.904949608113384e-06, 3.514779564284254e-06, 4.03131753046182e-06, 4.031323442177381e-06, 3.747275513887871e-06, 4.086250555701554e-06, 3.8396647141780704e-06, 3.8644734559056815e-06, 4.389939476823201e-06, 4.203482603770681e-06, 3.855716840917012e-06, 3.0649352993350476e-06, 3.513129286147887e-06, 3.1394702091347426e-06, 3.7037393667560536e-06, 4.127933607378509e-06, 3.3322512535960414e-06, 3.2883722269616555e-06, 3.3953772344830213e-06, 4.04760521632852e-06, 3.3021638046193402e-06, 3.5047391975240316e-06, 3.38133349941927e-06, 1.858488644757017e-06]}
Train Epoch: 11 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [24/810 (3%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [96/810 (12%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [144/810 (18%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [240/810 (30%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [252/810 (31%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [264/810 (33%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [276/810 (34%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [300/810 (37%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [312/810 (39%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [348/810 (43%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [384/810 (47%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [432/810 (53%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [480/810 (59%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [492/810 (61%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [612/810 (76%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [756/810 (93%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 11 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 11 [804/810 (99%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
all losses in batch in validation:  {'loss': [6.415370989998337e-06, 5.991634679958224e-06, 5.920641342527233e-06, 6.870535798952915e-06, 6.7994146775163244e-06, 6.385531378327869e-06, 6.282793037826195e-06, 5.880139724467881e-06, 6.537377885251772e-06, 5.9730209613917395e-06, 6.144320195744513e-06, 5.9356193560233805e-06, 6.601140739803668e-06, 6.45902309770463e-06, 6.126331754785497e-06, 6.461623343056999e-06, 6.5638378146104515e-06, 6.281313289946411e-06, 6.680236765532754e-06, 6.357916845445288e-06, 6.3225170379155315e-06, 6.906429007358383e-06, 6.640651008638088e-06, 6.448581189033575e-06, 5.617486749542877e-06, 6.055573066987563e-06, 5.71014970773831e-06, 6.21136678091716e-06, 6.619484338443726e-06, 5.8440900829737075e-06, 5.846987733093556e-06, 5.98497399550979e-06, 6.616216523980256e-06, 5.890464763069758e-06, 6.033271347405389e-06, 5.9038493418483995e-06, 3.1057370506459847e-06], 'L_si': [0.0, -8.940696716308594e-08, 5.960464477539063e-08, 0.0, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, -2.9802322387695312e-08, -5.960464477539063e-08, 0.0, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, -5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 2.9802322387695312e-08], 'L_grad': [6.415370989998337e-06, 6.08104164712131e-06, 5.8610366977518424e-06, 6.870535798952915e-06, 6.7994146775163244e-06, 6.3557290559401736e-06, 6.312595360213891e-06, 5.82053507969249e-06, 6.567180207639467e-06, 6.03262560616713e-06, 6.144320195744513e-06, 5.9356193560233805e-06, 6.571338417415973e-06, 6.429220775316935e-06, 6.126331754785497e-06, 6.52122798783239e-06, 6.5638378146104515e-06, 6.2515109675587155e-06, 6.680236765532754e-06, 6.387719167832984e-06, 6.292714715527836e-06, 6.846824362582993e-06, 6.640651008638088e-06, 6.41877886664588e-06, 5.587684427155182e-06, 6.055573066987563e-06, 5.71014970773831e-06, 6.181564458529465e-06, 6.589682016056031e-06, 5.814287760586012e-06, 5.817185410705861e-06, 5.955171673122095e-06, 6.586414201592561e-06, 5.860662440682063e-06, 6.003469025017694e-06, 5.96345398662379e-06, 3.0759347282582894e-06]}
Train Epoch: 12 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [24/810 (3%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [36/810 (4%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [48/810 (6%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [72/810 (9%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [132/810 (16%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [168/810 (21%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [240/810 (30%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [324/810 (40%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [432/810 (53%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [528/810 (65%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [540/810 (67%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [552/810 (68%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [576/810 (71%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [756/810 (93%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 12 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 12 [804/810 (99%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
Saving checkpoint: s2d_checkpoints/train_s2d_SpikeTransformer/checkpoint-epoch012-loss-0.0000.pth.tar ...
all losses in batch in validation:  {'loss': [3.4816300740203587e-06, 3.303054654679727e-06, 3.0681958378409036e-06, 3.8591424527112395e-06, 3.822251983365277e-06, 3.672159436973743e-06, 3.5803122955258004e-06, 3.121443342024577e-06, 3.6969740904169157e-06, 3.2185148484131787e-06, 3.392765847820556e-06, 3.1405529625772033e-06, 3.794437134274631e-06, 3.634378117567394e-06, 3.3168748814205173e-06, 3.5867715268977918e-06, 3.644766820798395e-06, 3.4674271773837972e-06, 3.7586794405797264e-06, 3.588486151784309e-06, 3.5394368751440197e-06, 3.869999090966303e-06, 3.717124400282046e-06, 3.6246351555746514e-06, 2.9593124963867012e-06, 3.286974106231355e-06, 2.9909081149526173e-06, 3.3615697248023935e-06, 3.801543698500609e-06, 3.1844110708334483e-06, 3.012614570252481e-06, 3.168434432154754e-06, 3.6646642911364324e-06, 3.1421320727531565e-06, 3.2248110528598772e-06, 3.1885197131487075e-06, 1.7085551462514559e-06], 'L_si': [-5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 8.940696716308594e-08, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 5.960464477539063e-08, 5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 8.940696716308594e-08, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, -8.940696716308594e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 0.0], 'L_grad': [3.5412347187957494e-06, 3.2732523322920315e-06, 3.097998160228599e-06, 3.888944775098935e-06, 3.822251983365277e-06, 3.6125547921983525e-06, 3.4909053283627145e-06, 3.121443342024577e-06, 3.6671717680292204e-06, 3.248317170800874e-06, 3.3629635254328605e-06, 3.1703552849648986e-06, 3.73483248949924e-06, 3.5747734727920033e-06, 3.2572702366451267e-06, 3.616573849285487e-06, 3.6745691431860905e-06, 3.437624854996102e-06, 3.728877118192031e-06, 3.5288815070089186e-06, 3.5096345527563244e-06, 3.899801413353998e-06, 3.746926722669741e-06, 3.5352281884115655e-06, 2.9593124963867012e-06, 3.25717178384366e-06, 2.9909081149526173e-06, 3.3615697248023935e-06, 3.7419390537252184e-06, 3.154608748445753e-06, 3.102021537415567e-06, 3.1982367545424495e-06, 3.634861968748737e-06, 3.082527427977766e-06, 3.2546133752475725e-06, 3.158717390761012e-06, 1.7085551462514559e-06]}
Train Epoch: 13 [0/810 (0%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [48/810 (6%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [144/810 (18%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [168/810 (21%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [192/810 (24%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [216/810 (27%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [228/810 (28%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [312/810 (39%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [324/810 (40%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [360/810 (44%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [372/810 (46%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [408/810 (50%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [540/810 (67%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [576/810 (71%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [648/810 (80%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [756/810 (93%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [780/810 (96%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 13 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 13 [804/810 (99%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
all losses in batch in validation:  {'loss': [4.102431375940796e-06, 3.902265689248452e-06, 3.6718174669658765e-06, 4.603480647347169e-06, 4.618409548129421e-06, 4.252731287124334e-06, 4.197887392365374e-06, 3.657733486761572e-06, 4.384341082186438e-06, 3.917513822671026e-06, 4.069721398991533e-06, 3.7407644413178787e-06, 4.531328158918768e-06, 4.2977208067895845e-06, 3.848575488518691e-06, 4.3497179831319954e-06, 4.403604179969989e-06, 4.0803442971082404e-06, 4.530006208369741e-06, 4.193236236460507e-06, 4.1849389162962325e-06, 4.659216301661218e-06, 4.4542885007103905e-06, 4.214854925521649e-06, 3.3868500395328738e-06, 3.869077772833407e-06, 3.5502957871358376e-06, 4.093376446689945e-06, 4.5515016608987935e-06, 3.6599890336219687e-06, 3.6957069369236706e-06, 3.7638408230122877e-06, 4.405886556924088e-06, 3.6630012800742406e-06, 3.88870648748707e-06, 3.757697413675487e-06, 2.077804538203054e-06], 'L_si': [-5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0, -5.960464477539063e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 5.960464477539063e-08, 0.0, 5.960464477539063e-08, 0.0, 0.0, 0.0, 2.9802322387695312e-08, 0.0, 5.960464477539063e-08], 'L_grad': [4.162036020716187e-06, 3.872463366860757e-06, 3.701619789353572e-06, 4.66308529212256e-06, 4.588607225741725e-06, 4.252731287124334e-06, 4.168085069977678e-06, 3.657733486761572e-06, 4.443945726961829e-06, 3.887711500283331e-06, 4.010116754216142e-06, 3.7109621189301834e-06, 4.501525836531073e-06, 4.267918484401889e-06, 3.908180133294081e-06, 4.3199156607443e-06, 4.403604179969989e-06, 4.110146619495936e-06, 4.500203885982046e-06, 4.193236236460507e-06, 4.155136593908537e-06, 4.659216301661218e-06, 4.424486178322695e-06, 4.244657247909345e-06, 3.416652361920569e-06, 3.898880095221102e-06, 3.5502957871358376e-06, 4.063574124302249e-06, 4.491897016123403e-06, 3.6599890336219687e-06, 3.63610229214828e-06, 3.7638408230122877e-06, 4.405886556924088e-06, 3.6630012800742406e-06, 3.858904165099375e-06, 3.757697413675487e-06, 2.0181998934276635e-06]}
Train Epoch: 14 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [48/810 (6%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [60/810 (7%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [72/810 (9%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [96/810 (12%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [144/810 (18%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [180/810 (22%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [276/810 (34%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [408/810 (50%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [432/810 (53%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [456/810 (56%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [504/810 (62%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [528/810 (65%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [552/810 (68%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [564/810 (70%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [612/810 (76%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [624/810 (77%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [648/810 (80%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [660/810 (81%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [732/810 (90%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [768/810 (95%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 14 [792/810 (98%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 14 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [3.10645282297628e-06, 2.8773665690096095e-06, 2.7774929094448453e-06, 3.2793400350783486e-06, 3.2460993679706007e-06, 3.103933067905018e-06, 3.0482447073154617e-06, 2.7410974325903226e-06, 3.158882464049384e-06, 2.8510626179922838e-06, 2.951739588752389e-06, 2.793160547298612e-06, 3.1745562409923878e-06, 3.0218907340895385e-06, 2.917906385846436e-06, 3.1187391869025305e-06, 3.2132761589309666e-06, 2.9774346330668777e-06, 3.2217485568253323e-06, 3.08215248878696e-06, 3.0179016903275624e-06, 3.3023682135535637e-06, 3.2152895528270165e-06, 3.0745979984203586e-06, 2.6037698717118474e-06, 2.844092250597896e-06, 2.6478264771867543e-06, 2.9218977033451665e-06, 3.173745881213108e-06, 2.747320650087204e-06, 2.6912357498076744e-06, 2.877124188671587e-06, 3.203228970960481e-06, 2.769685579551151e-06, 2.865540409402456e-06, 2.771724211925175e-06, 1.509190610704536e-06], 'L_si': [5.960464477539063e-08, 0.0, 2.9802322387695312e-08, -5.960464477539063e-08, -5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 0.0, 0.0, 2.9802322387695312e-08, 0.0, 0.0, -5.960464477539063e-08, 2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 0.0, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 0.0, 0.0, 0.0, 0.0, -2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08], 'L_grad': [3.0468481782008894e-06, 2.8773665690096095e-06, 2.74769058705715e-06, 3.3389446798537392e-06, 3.3057040127459913e-06, 3.074130745517323e-06, 3.0184423849277664e-06, 2.7410974325903226e-06, 3.158882464049384e-06, 2.8510626179922838e-06, 2.9219372663646936e-06, 2.793160547298612e-06, 3.1745562409923878e-06, 3.081495378864929e-06, 2.8881040634587407e-06, 3.1187391869025305e-06, 3.153671514155576e-06, 2.9774346330668777e-06, 3.2217485568253323e-06, 3.0523501663992647e-06, 3.0179016903275624e-06, 3.3023682135535637e-06, 3.185487230439321e-06, 3.0447956760326633e-06, 2.6037698717118474e-06, 2.8738945729855914e-06, 2.6478264771867543e-06, 2.9218977033451665e-06, 3.173745881213108e-06, 2.747320650087204e-06, 2.7210380721953698e-06, 2.8175195438961964e-06, 3.1734266485727858e-06, 2.7398832571634557e-06, 2.835738087014761e-06, 2.8015265343128704e-06, 1.4793882883168408e-06]}
Train Epoch: 15 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [48/810 (6%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [60/810 (7%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [132/810 (16%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [192/810 (24%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [204/810 (25%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [228/810 (28%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [240/810 (30%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [252/810 (31%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [276/810 (34%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [300/810 (37%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [420/810 (52%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [444/810 (55%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [504/810 (62%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [564/810 (70%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [624/810 (77%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [660/810 (81%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [732/810 (90%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [768/810 (95%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [780/810 (96%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 15 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 15 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [2.5144993287540274e-06, 2.2691363028570777e-06, 2.0911131741740974e-06, 2.8089552870369516e-06, 2.683699676708784e-06, 2.5090052986342926e-06, 2.4421071884717094e-06, 2.12803956856078e-06, 2.579037982286536e-06, 2.25833127842634e-06, 2.389413566561416e-06, 2.24480641008995e-06, 2.7000428417522926e-06, 2.524715910112718e-06, 2.2729950615030248e-06, 2.591163365650573e-06, 2.68050939666864e-06, 2.4040659809543286e-06, 2.6370373689132975e-06, 2.4973912786663277e-06, 2.4374610347877024e-06, 2.766603074633167e-06, 2.590897793197655e-06, 2.4928531274781562e-06, 1.999082542170072e-06, 2.28014619096939e-06, 2.0534710074571194e-06, 2.3709894776402507e-06, 2.647727342264261e-06, 2.160415988328168e-06, 2.114727976731956e-06, 2.1935825316177215e-06, 2.559518634370761e-06, 2.1585888134723064e-06, 2.2900749172549695e-06, 2.162788405257743e-06, 1.1522238310135435e-06], 'L_si': [5.960464477539063e-08, 0.0, -5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 8.940696716308594e-08, 2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 5.960464477539063e-08, 0.0, -2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08], 'L_grad': [2.4548946839786367e-06, 2.2691363028570777e-06, 2.150717818949488e-06, 2.7791529646492563e-06, 2.7135019990964793e-06, 2.538807621021988e-06, 2.4719095108594047e-06, 2.12803956856078e-06, 2.6088403046742314e-06, 2.25833127842634e-06, 2.3596112441737205e-06, 2.155399442926864e-06, 2.6702405193645973e-06, 2.524715910112718e-06, 2.2729950615030248e-06, 2.5613610432628775e-06, 2.6209047518932493e-06, 2.4040659809543286e-06, 2.666839691300993e-06, 2.4675889562786324e-06, 2.4672633571753977e-06, 2.766603074633167e-06, 2.6207001155853504e-06, 2.4928531274781562e-06, 1.999082542170072e-06, 2.28014619096939e-06, 2.0534710074571194e-06, 2.3709894776402507e-06, 2.647727342264261e-06, 2.160415988328168e-06, 2.114727976731956e-06, 2.1935825316177215e-06, 2.6191232791461516e-06, 2.128786491084611e-06, 2.260272594867274e-06, 2.192590727645438e-06, 1.1820261534012388e-06]}
Train Epoch: 16 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [24/810 (3%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [72/810 (9%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [96/810 (12%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [108/810 (13%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [300/810 (37%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [324/810 (40%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [372/810 (46%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [480/810 (59%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [516/810 (64%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [528/810 (65%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [540/810 (67%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [564/810 (70%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [684/810 (84%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [744/810 (92%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 16 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 16 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
Saving checkpoint: s2d_checkpoints/train_s2d_SpikeTransformer/checkpoint-epoch016-loss-0.0000.pth.tar ...
all losses in batch in validation:  {'loss': [1.5061443718877854e-06, 1.3738803090745932e-06, 1.299900077356142e-06, 1.7977413335756864e-06, 1.7319301832685596e-06, 1.5375410384876886e-06, 1.4395317293747212e-06, 1.2993950804229826e-06, 1.6914506204557256e-06, 1.3442371482597082e-06, 1.4379547792486846e-06, 1.2646137292904314e-06, 1.5944089000186068e-06, 1.543279040561174e-06, 1.4621565469497e-06, 1.6366834643122274e-06, 1.607822923688218e-06, 1.4341335372591857e-06, 1.6966323528322391e-06, 1.4936763363948558e-06, 1.4673053101432743e-06, 1.7584310398888192e-06, 1.6880958355613984e-06, 1.6118077610371984e-06, 1.1393933618819574e-06, 1.3217544392318814e-06, 1.1667735861919937e-06, 1.440365281268896e-06, 1.6528507558177807e-06, 1.2817383776564384e-06, 1.3425540146272397e-06, 1.285093389924441e-06, 1.6607526731604594e-06, 1.2508879763117875e-06, 1.33915943933971e-06, 1.3030844456807245e-06, 7.139170179470966e-07], 'L_si': [-2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 0.0, -5.960464477539063e-08, 2.9802322387695312e-08, 5.960464477539063e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 5.960464477539063e-08, 0.0, -5.960464477539063e-08, -2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 8.940696716308594e-08, -5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 0.0, 0.0], 'L_grad': [1.5359466942754807e-06, 1.3738803090745932e-06, 1.2700977549684467e-06, 1.7679390111879911e-06, 1.7319301832685596e-06, 1.5375410384876886e-06, 1.4991363741501118e-06, 1.2695927580352873e-06, 1.631845975680335e-06, 1.3740394706474035e-06, 1.4081524568609893e-06, 1.2944160516781267e-06, 1.6242112224063021e-06, 1.543279040561174e-06, 1.4025519021743094e-06, 1.6068811419245321e-06, 1.607822923688218e-06, 1.463935859646881e-06, 1.6668300304445438e-06, 1.523478658782551e-06, 1.4971076325309696e-06, 1.7584310398888192e-06, 1.658293513173703e-06, 1.5522031162618077e-06, 1.1393933618819574e-06, 1.381359084007272e-06, 1.196575908579689e-06, 1.440365281268896e-06, 1.6528507558177807e-06, 1.251936055268743e-06, 1.2531470474641537e-06, 1.3446980346998316e-06, 1.630950350772764e-06, 1.2806902986994828e-06, 1.33915943933971e-06, 1.3030844456807245e-06, 7.139170179470966e-07]}
Train Epoch: 17 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [48/810 (6%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [144/810 (18%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [216/810 (27%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [228/810 (28%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [408/810 (50%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [432/810 (53%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [456/810 (56%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [504/810 (62%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [540/810 (67%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [624/810 (77%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [636/810 (79%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [648/810 (80%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [660/810 (81%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [780/810 (96%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 17 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 17 [804/810 (99%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
all losses in batch in validation:  {'loss': [1.8418298850519932e-06, 1.6908882116695167e-06, 1.6609792510280386e-06, 1.984025175261195e-06, 1.9375061128812376e-06, 1.9152032564306865e-06, 1.8367944676356274e-06, 1.574155589878501e-06, 1.8970240489579737e-06, 1.7135982943727868e-06, 1.7452266547479667e-06, 1.6843276853251155e-06, 1.9214485291740857e-06, 1.8754349184746388e-06, 1.7303848380834097e-06, 1.8066166376229376e-06, 1.8939233541459544e-06, 1.8491829223421519e-06, 1.9587137103371788e-06, 1.7917479908646783e-06, 1.8541568351793103e-06, 2.0164850411674706e-06, 1.9733256522158626e-06, 1.767391040630173e-06, 1.5175307908066316e-06, 1.6956082617980428e-06, 1.4737579476786777e-06, 1.7709721760184038e-06, 1.9408360003581038e-06, 1.5790319594088942e-06, 1.5551186152151786e-06, 1.6753710951888934e-06, 1.915514303618693e-06, 1.6040138461903553e-06, 1.7004240362439305e-06, 1.630254928386421e-06, 9.126916893364978e-07], 'L_si': [2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, 5.960464477539063e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 0.0, 0.0, 0.0, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 5.960464477539063e-08, -5.960464477539063e-08, -2.9802322387695312e-08, 0.0, -8.940696716308594e-08, 2.9802322387695312e-08, 0.0, -5.960464477539063e-08, -5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 0.0, 0.0, -2.9802322387695312e-08, 2.9802322387695312e-08], 'L_grad': [1.8120275626642979e-06, 1.6908882116695167e-06, 1.6311769286403432e-06, 2.01382749764889e-06, 1.9971107576566283e-06, 1.8555986116552958e-06, 1.806992145247932e-06, 1.6337602346538915e-06, 1.8970240489579737e-06, 1.7135982943727868e-06, 1.7452266547479667e-06, 1.6545253629374201e-06, 1.9214485291740857e-06, 1.8456325960869435e-06, 1.7303848380834097e-06, 1.8364189600106329e-06, 1.8939233541459544e-06, 1.7895782775667612e-06, 1.9289113879494835e-06, 1.8215503132523736e-06, 1.7945521904039197e-06, 1.9866827187797753e-06, 1.913721007440472e-06, 1.8269956854055636e-06, 1.5473331131943269e-06, 1.6956082617980428e-06, 1.5631649148417637e-06, 1.7411698536307085e-06, 1.9408360003581038e-06, 1.6386366041842848e-06, 1.6147232599905692e-06, 1.6753710951888934e-06, 1.8857119812309975e-06, 1.6040138461903553e-06, 1.7004240362439305e-06, 1.6600572507741163e-06, 8.828893669488025e-07]}
Train Epoch: 18 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [72/810 (9%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [84/810 (10%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [120/810 (15%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [132/810 (16%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [360/810 (44%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [372/810 (46%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [384/810 (47%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [408/810 (50%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [444/810 (55%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [540/810 (67%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 18 [780/810 (96%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [792/810 (98%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 18 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
all losses in batch in validation:  {'loss': [1.8177443052991293e-06, 1.5864256965869572e-06, 1.4432874877456925e-06, 2.144454811059404e-06, 2.0381614831421757e-06, 1.830898440857709e-06, 1.843643985921517e-06, 1.4927322808944155e-06, 1.9732431155716768e-06, 1.6153969681909075e-06, 1.6787279264462995e-06, 1.487517465648125e-06, 2.065803073492134e-06, 1.8576508864498464e-06, 1.647655608394416e-06, 1.917682766361395e-06, 1.983759602808277e-06, 1.6864642020664178e-06, 2.039937498921063e-06, 1.8111524013875169e-06, 1.8136751123165595e-06, 2.1841512989340117e-06, 1.8786777218338102e-06, 1.8627047211339232e-06, 1.3524025916922255e-06, 1.5849088867980754e-06, 1.3520633501684642e-06, 1.7379616110702045e-06, 1.9942413018725347e-06, 1.480639866713318e-06, 1.4313399105958524e-06, 1.5523847878284869e-06, 1.9887136204488343e-06, 1.4981603726482717e-06, 1.5942657682899153e-06, 1.592126750438183e-06, 8.809005294097005e-07], 'L_si': [2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, -5.960464477539063e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 0.0, 5.960464477539063e-08, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 0.0, 0.0, 0.0, 5.960464477539063e-08, -8.940696716308594e-08, 0.0, 5.960464477539063e-08, -5.960464477539063e-08, -2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 0.0, 0.0, 2.9802322387695312e-08, 8.940696716308594e-08, 2.9802322387695312e-08], 'L_grad': [1.787941982911434e-06, 1.6162280189746525e-06, 1.4730898101333878e-06, 2.144454811059404e-06, 2.0977661279175663e-06, 1.8607007632454042e-06, 1.8138416635338217e-06, 1.4629299585067201e-06, 2.003045437959372e-06, 1.5855946458032122e-06, 1.6787279264462995e-06, 1.487517465648125e-06, 2.0061984287167434e-06, 1.8576508864498464e-06, 1.6178532860067207e-06, 1.94748508874909e-06, 1.953957280420582e-06, 1.7460688468418084e-06, 2.039937498921063e-06, 1.8111524013875169e-06, 1.8136751123165595e-06, 2.124546654158621e-06, 1.968084688996896e-06, 1.8627047211339232e-06, 1.292797946916835e-06, 1.644513531573466e-06, 1.3818656725561596e-06, 1.7379616110702045e-06, 1.9942413018725347e-06, 1.4508375443256227e-06, 1.4313399105958524e-06, 1.5523847878284869e-06, 1.9887136204488343e-06, 1.4981603726482717e-06, 1.56446344590222e-06, 1.5027197832750971e-06, 8.510982070220052e-07]}
Train Epoch: 19 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [120/810 (15%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [156/810 (19%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [168/810 (21%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [180/810 (22%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [192/810 (24%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [240/810 (30%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [324/810 (40%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [348/810 (43%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [384/810 (47%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [420/810 (52%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [480/810 (59%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [492/810 (61%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [552/810 (68%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [564/810 (70%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [636/810 (79%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [648/810 (80%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [660/810 (81%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [684/810 (84%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [732/810 (90%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [768/810 (95%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 19 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 19 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [1.3312503597262548e-06, 1.2352713838481577e-06, 1.1187806876478135e-06, 1.5895906244622893e-06, 1.5303867257898673e-06, 1.3689513025383349e-06, 1.4531395891026477e-06, 1.1227820095882635e-06, 1.4122933862381615e-06, 1.1651530940071098e-06, 1.1872294862769195e-06, 1.1352196906955214e-06, 1.5164756632657372e-06, 1.4209215351002058e-06, 1.256682253369945e-06, 1.3844678505847696e-06, 1.4279605693445774e-06, 1.2875721040472854e-06, 1.4826084679953055e-06, 1.3404493302004994e-06, 1.3836584003001917e-06, 1.5125684740269207e-06, 1.4983532992118853e-06, 1.3561547120843898e-06, 1.0151436526939506e-06, 1.2807066696041147e-06, 1.085032636183314e-06, 1.2462646736821625e-06, 1.4431907402467914e-06, 1.1088925475633005e-06, 1.0330916211387375e-06, 1.2019952464470407e-06, 1.4428546819544863e-06, 1.18848788588366e-06, 1.1830983339677914e-06, 1.2042850130455918e-06, 6.628689561694046e-07], 'L_si': [0.0, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 1.1920928955078125e-07, 0.0, -2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, 0.0, 5.960464477539063e-08, 5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 0.0, 0.0, 0.0, 5.960464477539063e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 0.0, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 0.0, -5.960464477539063e-08, 2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08], 'L_grad': [1.3312503597262548e-06, 1.2054690614604624e-06, 1.1187806876478135e-06, 1.559788302074594e-06, 1.5303867257898673e-06, 1.3689513025383349e-06, 1.3339302995518665e-06, 1.1227820095882635e-06, 1.4420957086258568e-06, 1.194955416394805e-06, 1.2468341310523101e-06, 1.1352196906955214e-06, 1.4568710184903466e-06, 1.3613168903248152e-06, 1.2268799309822498e-06, 1.414270172972465e-06, 1.4279605693445774e-06, 1.2875721040472854e-06, 1.4826084679953055e-06, 1.3404493302004994e-06, 1.324053755524801e-06, 1.542370796414616e-06, 1.4387486544364947e-06, 1.3561547120843898e-06, 1.0151436526939506e-06, 1.221102024828724e-06, 1.0552303137956187e-06, 1.2760669960698579e-06, 1.4431907402467914e-06, 1.1088925475633005e-06, 1.0926962659141282e-06, 1.1721929240593454e-06, 1.4428546819544863e-06, 1.1288832411082694e-06, 1.1830983339677914e-06, 1.1446803682702011e-06, 6.330666337817092e-07]}
Train Epoch: 20 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [60/810 (7%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [84/810 (10%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [108/810 (13%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [120/810 (15%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [252/810 (31%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [264/810 (33%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [324/810 (40%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [372/810 (46%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [408/810 (50%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [432/810 (53%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [516/810 (64%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [564/810 (70%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [612/810 (76%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [624/810 (77%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [636/810 (79%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [672/810 (83%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [684/810 (84%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [756/810 (93%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 20 [792/810 (98%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 20 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
Saving checkpoint: s2d_checkpoints/train_s2d_SpikeTransformer/checkpoint-epoch020-loss-0.0000.pth.tar ...
all losses in batch in validation:  {'loss': [1.3007554571231594e-06, 1.0229875897493912e-06, 9.559236104905722e-07, 1.3852172742190305e-06, 1.4529877034874517e-06, 1.1596138165259617e-06, 1.1758679647755343e-06, 9.115958050642803e-07, 1.3638107247970765e-06, 1.0898781965806847e-06, 1.0617394536893698e-06, 9.985874385165516e-07, 1.3083571275274153e-06, 1.2406447922330699e-06, 1.153958010036149e-06, 1.3352146197576076e-06, 1.2533173503470607e-06, 1.0849543059521238e-06, 1.335380261480168e-06, 1.1815487823696458e-06, 1.1652161902020453e-06, 1.4474868521574535e-06, 1.3834544461133191e-06, 1.2714405102087767e-06, 8.500392709720472e-07, 1.062454089151288e-06, 8.426408157902188e-07, 1.1111277444797452e-06, 1.3784924703941215e-06, 8.712958106116275e-07, 8.327075420311303e-07, 1.0617263797030319e-06, 1.3401853493633098e-06, 9.828819429458235e-07, 9.916511771734804e-07, 9.774819318408845e-07, 5.031576506553392e-07], 'L_si': [5.960464477539063e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 5.960464477539063e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 0.0, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 0.0, 0.0, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, 0.0, 0.0, -2.9802322387695312e-08, 0.0, 0.0, -2.9802322387695312e-08], 'L_grad': [1.2411508123477688e-06, 1.0527899121370865e-06, 9.261212881028769e-07, 1.4150195966067258e-06, 1.4529877034874517e-06, 1.1596138165259617e-06, 1.146065642387839e-06, 9.413981274519756e-07, 1.3638107247970765e-06, 1.0600758741929894e-06, 1.0617394536893698e-06, 9.985874385165516e-07, 1.2487524827520247e-06, 1.1810401474576793e-06, 1.1241556876484537e-06, 1.275609974982217e-06, 1.2533173503470607e-06, 1.1147566283398191e-06, 1.3651825838678633e-06, 1.1815487823696458e-06, 1.1652161902020453e-06, 1.4176845297697582e-06, 1.3238498013379285e-06, 1.2416381878210814e-06, 7.904346261966566e-07, 1.062454089151288e-06, 8.426408157902188e-07, 1.1111277444797452e-06, 1.3486901480064262e-06, 9.010981329993228e-07, 8.923121868065209e-07, 1.0617263797030319e-06, 1.3401853493633098e-06, 1.0126842653335189e-06, 9.916511771734804e-07, 9.774819318408845e-07, 5.329599730430346e-07]}
Train Epoch: 21 [0/810 (0%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [24/810 (3%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [72/810 (9%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [132/810 (16%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [156/810 (19%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [168/810 (21%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [204/810 (25%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [264/810 (33%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [324/810 (40%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [372/810 (46%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [432/810 (53%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [468/810 (58%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [516/810 (64%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [528/810 (65%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [552/810 (68%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [636/810 (79%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [744/810 (92%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [768/810 (95%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 21 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 21 [804/810 (99%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [1.0625306003930746e-06, 8.750763527132221e-07, 9.165586334347608e-07, 1.2783821148332208e-06, 1.2323291684879223e-06, 1.018339503389143e-06, 1.0598836297504022e-06, 8.804747722024331e-07, 1.1891694384758011e-06, 9.526914936941466e-07, 1.017393287838786e-06, 8.346351592081191e-07, 1.189323029393563e-06, 1.0841442872333573e-06, 9.505156413069926e-07, 1.1058655218221247e-06, 1.1396638228688971e-06, 9.602679256204283e-07, 1.1365236787241884e-06, 1.0671747077140026e-06, 1.0500871212570928e-06, 1.2661877235586871e-06, 1.1796898888860596e-06, 1.1195918432349572e-06, 7.834702273612493e-07, 9.742365136844455e-07, 8.33011483791779e-07, 9.85643623607757e-07, 1.1565913382582949e-06, 8.306988092954271e-07, 8.872701755535672e-07, 8.958883199738921e-07, 1.2285711363801965e-06, 8.961144430941204e-07, 9.31105546442268e-07, 9.387127875015722e-07, 4.939868745168496e-07], 'L_si': [0.0, -8.940696716308594e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, -5.960464477539063e-08, 0.0, 0.0, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, -5.960464477539063e-08, 2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, -5.960464477539063e-08, -5.960464477539063e-08, 0.0, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 0.0, 0.0, -2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 0.0, 0.0, 2.9802322387695312e-08, 0.0], 'L_grad': [1.0625306003930746e-06, 9.64483319876308e-07, 8.867563110470655e-07, 1.2485797924455255e-06, 1.2323291684879223e-06, 1.0779441481645335e-06, 1.0598836297504022e-06, 8.804747722024331e-07, 1.1593671160881058e-06, 9.526914936941466e-07, 9.875909654510906e-07, 8.942398039835098e-07, 1.1595207070058677e-06, 1.0841442872333573e-06, 9.80317963694688e-07, 1.13566784420982e-06, 1.1396638228688971e-06, 1.019872570395819e-06, 1.196128323499579e-06, 1.0671747077140026e-06, 1.0500871212570928e-06, 1.2363854011709918e-06, 1.1498875664983643e-06, 1.0897895208472619e-06, 7.834702273612493e-07, 9.742365136844455e-07, 8.33011483791779e-07, 1.0154459459954523e-06, 1.1565913382582949e-06, 8.605011316831224e-07, 8.574678531658719e-07, 9.256906423615874e-07, 1.168966491604806e-06, 8.961144430941204e-07, 9.31105546442268e-07, 9.089104651138769e-07, 4.939868745168496e-07]}
Train Epoch: 22 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [36/810 (4%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [60/810 (7%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [84/810 (10%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [96/810 (12%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [120/810 (15%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [144/810 (18%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [156/810 (19%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [168/810 (21%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [192/810 (24%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [204/810 (25%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [228/810 (28%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [252/810 (31%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [264/810 (33%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [372/810 (46%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [384/810 (47%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [408/810 (50%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [504/810 (62%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [576/810 (71%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [624/810 (77%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [720/810 (89%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [768/810 (95%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [780/810 (96%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 22 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 22 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [9.350609389002784e-07, 8.043533057389141e-07, 7.488614528483595e-07, 1.0296785148966592e-06, 8.825702479953179e-07, 9.209310292135342e-07, 8.890160074770392e-07, 7.846919629628246e-07, 9.147711352852639e-07, 8.601822969467321e-07, 8.465344194519275e-07, 7.523518092966697e-07, 9.368267228637706e-07, 8.581146175856702e-07, 8.270681632893684e-07, 8.670907050145615e-07, 9.795483038033126e-07, 8.13811027455813e-07, 9.68351514529786e-07, 8.81897619819938e-07, 8.820384209684562e-07, 1.0163223578274483e-06, 1.0013592373070423e-06, 8.871406862454023e-07, 7.520126246163272e-07, 7.494751912417996e-07, 7.662418965992401e-07, 8.995015150503605e-07, 9.511933285466512e-07, 7.781678164064942e-07, 8.018618586902448e-07, 7.571768492198316e-07, 9.38284074436524e-07, 8.203528523154091e-07, 8.23270795535791e-07, 7.378473583230516e-07, 3.7243765405037266e-07], 'L_si': [5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, -5.960464477539063e-08, 5.960464477539063e-08, 5.960464477539063e-08, 5.960464477539063e-08, 0.0, 8.940696716308594e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, 8.940696716308594e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 8.940696716308594e-08, -2.9802322387695312e-08, 8.940696716308594e-08, 8.940696716308594e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 8.940696716308594e-08, 0.0, 2.9802322387695312e-08, 8.940696716308594e-08, 5.960464477539063e-08, 0.0, -2.9802322387695312e-08], 'L_grad': [8.754562941248878e-07, 7.745509833512187e-07, 7.190591304606642e-07, 9.700738701212686e-07, 9.421748927707085e-07, 8.613263844381436e-07, 8.294113627016486e-07, 7.25087318187434e-07, 9.147711352852639e-07, 7.707753297836462e-07, 7.869297746765369e-07, 7.225494869089744e-07, 9.070244004760752e-07, 8.581146175856702e-07, 7.972658409016731e-07, 8.968930274022568e-07, 8.901413934836455e-07, 8.13811027455813e-07, 9.087469265978143e-07, 8.520952974322427e-07, 8.522360985807609e-07, 9.86520035439753e-07, 9.417545925316517e-07, 8.57338363857707e-07, 6.626056574532413e-07, 7.792775136294949e-07, 6.768349294361542e-07, 8.100945478872745e-07, 9.213910061589559e-07, 7.185631716311036e-07, 7.124548915271589e-07, 7.571768492198316e-07, 9.084817520488286e-07, 7.309458851523232e-07, 7.636661507604003e-07, 7.378473583230516e-07, 4.0223997643806797e-07]}
Train Epoch: 23 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [36/810 (4%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [60/810 (7%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [216/810 (27%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [252/810 (31%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [264/810 (33%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [384/810 (47%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [444/810 (55%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [456/810 (56%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [480/810 (59%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [516/810 (64%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [612/810 (76%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [624/810 (77%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [768/810 (95%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [780/810 (96%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 23 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 23 [804/810 (99%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [8.305370897687681e-07, 7.726454782641667e-07, 7.632443157490343e-07, 1.0070891676150495e-06, 9.212162694893777e-07, 8.366695851691475e-07, 8.254826866505027e-07, 7.106754651431402e-07, 9.039965789270354e-07, 7.839670388420927e-07, 7.655302738385217e-07, 6.804042413932621e-07, 9.063094807970629e-07, 8.679866709826456e-07, 6.936096497156541e-07, 8.214777835746645e-07, 8.948688901000423e-07, 8.17696786725719e-07, 9.3043780680091e-07, 8.207534847315401e-07, 8.08748723102326e-07, 1.0259557257086271e-06, 8.090996743703727e-07, 9.038898269864148e-07, 5.025889322496369e-07, 7.768853720335755e-07, 6.293718683991756e-07, 8.117150400721584e-07, 9.810536312215845e-07, 6.579821274499409e-07, 6.829811241004791e-07, 7.455244031007169e-07, 9.336916377833404e-07, 6.808429589000298e-07, 7.397999866043392e-07, 7.493493967558607e-07, 4.4095551743339456e-07], 'L_si': [0.0, 2.9802322387695312e-08, 8.940696716308594e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 0.0, 2.9802322387695312e-08, -5.960464477539063e-08, -5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 0.0, 5.960464477539063e-08, -8.940696716308594e-08, 5.960464477539063e-08, -8.940696716308594e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 5.960464477539063e-08, 5.960464477539063e-08], 'L_grad': [8.305370897687681e-07, 7.428431558764714e-07, 6.738373485859483e-07, 9.772868452273542e-07, 9.51018591877073e-07, 8.366695851691475e-07, 8.254826866505027e-07, 6.808731427554449e-07, 9.039965789270354e-07, 7.541647164543974e-07, 7.655302738385217e-07, 6.804042413932621e-07, 9.063094807970629e-07, 8.381843485949503e-07, 7.532142944910447e-07, 8.810824283500551e-07, 8.948688901000423e-07, 7.878944643380237e-07, 9.3043780680091e-07, 8.207534847315401e-07, 8.08748723102326e-07, 9.663510809332365e-07, 8.985066415334586e-07, 8.442851822110242e-07, 5.919958994127228e-07, 7.470830496458802e-07, 6.293718683991756e-07, 7.819127176844631e-07, 9.214489864461939e-07, 6.579821274499409e-07, 6.531788017127838e-07, 7.157220807130216e-07, 9.038893153956451e-07, 6.808429589000298e-07, 7.099976642166439e-07, 6.8974475198047e-07, 3.8135087265800394e-07]}
Train Epoch: 24 [0/810 (0%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 24 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [84/810 (10%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 24 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [120/810 (15%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 24 [132/810 (16%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 24 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [372/810 (46%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 24 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [444/810 (55%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 24 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [516/810 (64%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 24 [528/810 (65%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 24 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [552/810 (68%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 24 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 24 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [672/810 (83%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 24 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [768/810 (95%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 24 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 24 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving checkpoint: s2d_checkpoints/train_s2d_SpikeTransformer/checkpoint-epoch024-loss-0.0000.pth.tar ...
all losses in batch in validation:  {'loss': [1.8284408724866807e-06, 1.417895759914245e-06, 1.164383206742059e-06, 1.9179676655767253e-06, 1.9259284727013437e-06, 1.3157969078747556e-06, 1.4531836995956837e-06, 1.1282011200819397e-06, 1.7562121001901687e-06, 1.372400447507971e-06, 1.354885739601741e-06, 1.2998777947359486e-06, 1.5277914826583583e-06, 1.501700921835436e-06, 1.5004915212557535e-06, 1.7766333257895894e-06, 1.702026338534779e-06, 1.4194041568771354e-06, 1.7828796217145282e-06, 1.5869079561525723e-06, 1.4805375485593686e-06, 2.0069121546839597e-06, 1.786898565114825e-06, 1.7405895960109774e-06, 8.911637223718571e-07, 1.4602227338400553e-06, 1.103612817132671e-06, 1.4883290759826195e-06, 1.6118079884108738e-06, 1.0667760079741129e-06, 1.1599998970268643e-06, 1.4045760963199427e-06, 1.8514099338062806e-06, 1.388329337714822e-06, 1.238848653883906e-06, 1.3041192232776666e-06, 6.947921065147966e-07], 'L_si': [5.960464477539063e-08, 2.9802322387695312e-08, 0.0, 0.0, 0.0, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0, -5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, -5.960464477539063e-08, 5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, -5.960464477539063e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08], 'L_grad': [1.7688362277112901e-06, 1.3880934375265497e-06, 1.164383206742059e-06, 1.9179676655767253e-06, 1.9259284727013437e-06, 1.3455992302624509e-06, 1.482986021983379e-06, 1.158003442469635e-06, 1.8158167449655593e-06, 1.3127958027325803e-06, 1.3250834172140458e-06, 1.2998777947359486e-06, 1.5575938050460536e-06, 1.501700921835436e-06, 1.4706891988680582e-06, 1.7766333257895894e-06, 1.6722240161470836e-06, 1.4194041568771354e-06, 1.8424842664899188e-06, 1.5869079561525723e-06, 1.4507352261716733e-06, 1.9771098322962644e-06, 1.8167008875025203e-06, 1.6809849512355868e-06, 9.507683671472478e-07, 1.4006180890646647e-06, 1.103612817132671e-06, 1.4585267535949242e-06, 1.6118079884108738e-06, 1.0667760079741129e-06, 1.130197574639169e-06, 1.4045760963199427e-06, 1.8216076114185853e-06, 1.4479339824902127e-06, 1.238848653883906e-06, 1.244514578502276e-06, 6.649897841271013e-07]}
Train Epoch: 25 [0/810 (0%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [60/810 (7%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [108/810 (13%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [252/810 (31%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [264/810 (33%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [276/810 (34%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [324/810 (40%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [384/810 (47%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [408/810 (50%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [528/810 (65%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [672/810 (83%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [720/810 (89%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 25 [768/810 (95%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [780/810 (96%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [792/810 (98%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 25 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [6.07384436079883e-07, 4.7135057457126095e-07, 4.325862050791329e-07, 7.007620297372341e-07, 6.64033905195538e-07, 6.41035057924455e-07, 5.991126386106771e-07, 4.3032258645325783e-07, 6.764554427718394e-07, 4.678778395827976e-07, 6.317054612736683e-07, 5.287142244014831e-07, 6.438195896407706e-07, 6.1828779962525e-07, 5.472109023685334e-07, 6.367649802996311e-07, 6.406884835996607e-07, 5.887163752049673e-07, 6.598453410333605e-07, 5.784288532595383e-07, 6.014357722960995e-07, 7.165258466557134e-07, 6.479981493612286e-07, 6.498930815723725e-07, 4.1533925809744687e-07, 5.615727900476486e-07, 4.069047463417519e-07, 5.801369979963056e-07, 6.521439104290039e-07, 4.211756277072709e-07, 4.515717364483862e-07, 5.487513021762425e-07, 6.20011178398272e-07, 5.27445820353023e-07, 4.520973675425921e-07, 4.4470920101957745e-07, 2.4056893721535744e-07], 'L_si': [2.9802322387695312e-08, -5.960464477539063e-08, -5.960464477539063e-08, 2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 5.960464477539063e-08, -5.960464477539063e-08, 8.940696716308594e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 5.960464477539063e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, -5.960464477539063e-08, -5.960464477539063e-08, -2.9802322387695312e-08], 'L_grad': [5.775821136921877e-07, 5.309552193466516e-07, 4.921908498545235e-07, 6.709597073495388e-07, 6.64033905195538e-07, 5.814304131490644e-07, 5.693103162229818e-07, 4.899272312286485e-07, 6.168507979964488e-07, 5.274824843581882e-07, 5.422984941105824e-07, 4.989119020137878e-07, 6.140172672530753e-07, 5.884854772375547e-07, 5.472109023685334e-07, 6.069626579119358e-07, 6.108861612119654e-07, 5.58914052817272e-07, 6.300430186456651e-07, 5.784288532595383e-07, 5.716334499084041e-07, 6.569212018803228e-07, 6.181958269735333e-07, 5.902884367969818e-07, 4.451415804851422e-07, 5.317704676599533e-07, 4.665093911171425e-07, 5.503346756086103e-07, 6.223415880413086e-07, 4.807802724826615e-07, 4.813740588360815e-07, 5.189489797885471e-07, 6.20011178398272e-07, 4.976434979653277e-07, 5.117020123179827e-07, 5.043138457949681e-07, 2.7037125960305275e-07]}
Train Epoch: 26 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [96/810 (12%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [156/810 (19%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [204/810 (25%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [228/810 (28%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [264/810 (33%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [300/810 (37%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [384/810 (47%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [396/810 (49%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [480/810 (59%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [492/810 (61%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [588/810 (73%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [660/810 (81%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [672/810 (83%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [720/810 (89%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [732/810 (90%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [744/810 (92%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 26 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 26 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
all losses in batch in validation:  {'loss': [7.241935691126855e-07, 5.14815383212408e-07, 4.347650133240677e-07, 7.456306434505677e-07, 7.046730843285332e-07, 5.720398803532589e-07, 6.295157390923123e-07, 4.777402864419855e-07, 7.519860787397192e-07, 5.049865876571857e-07, 5.311543986863398e-07, 5.095023993817449e-07, 6.842857374067535e-07, 6.481718060058483e-07, 6.131943450782273e-07, 7.195819762273459e-07, 6.246533530429588e-07, 5.49428193608037e-07, 6.546404165419517e-07, 6.039586537553987e-07, 6.26057271801983e-07, 7.895157523307716e-07, 7.974671234478592e-07, 6.917402401995787e-07, 3.4882287991422345e-07, 5.801160227747459e-07, 4.0647134369464766e-07, 6.390307589754229e-07, 7.381129307759693e-07, 4.5391223579827056e-07, 4.6593029878749803e-07, 5.367304538594908e-07, 6.675343229289865e-07, 4.800135116056481e-07, 4.6665189756822656e-07, 4.80224287002784e-07, 2.6883952841672e-07], 'L_si': [2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 0.0, 8.940696716308594e-08, -2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 5.960464477539063e-08, 5.960464477539063e-08, 5.960464477539063e-08, 5.960464477539063e-08, 0.0, 0.0, 0.0, 0.0, 5.960464477539063e-08, 0.0, 5.960464477539063e-08, 8.940696716308594e-08, -5.960464477539063e-08, 5.960464477539063e-08, -2.9802322387695312e-08, 8.940696716308594e-08, 8.940696716308594e-08, 0.0, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 0.0, 0.0], 'L_grad': [6.943912467249902e-07, 5.14815383212408e-07, 4.64567335711763e-07, 7.158283210628724e-07, 6.748707619408378e-07, 5.720398803532589e-07, 5.699110943169217e-07, 4.777402864419855e-07, 6.625791115766333e-07, 5.34788910044881e-07, 5.311543986863398e-07, 4.797000769940496e-07, 6.246810926313628e-07, 5.885671612304577e-07, 5.535897003028367e-07, 6.599773314519553e-07, 6.246533530429588e-07, 5.49428193608037e-07, 6.546404165419517e-07, 6.039586537553987e-07, 5.664526270265924e-07, 7.895157523307716e-07, 7.378624786724686e-07, 6.023332730364928e-07, 4.084275246896141e-07, 5.205113779993553e-07, 4.3627366608234297e-07, 5.49623791812337e-07, 6.487059636128834e-07, 4.5391223579827056e-07, 4.6593029878749803e-07, 5.069281314717955e-07, 6.377320005412912e-07, 4.800135116056481e-07, 4.964542199559219e-07, 4.80224287002784e-07, 2.6883952841672e-07]}
Train Epoch: 27 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [48/810 (6%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [120/810 (15%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [180/810 (22%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [216/810 (27%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [252/810 (31%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [264/810 (33%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [324/810 (40%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [384/810 (47%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [444/810 (55%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [456/810 (56%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [660/810 (81%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [672/810 (83%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 27 [768/810 (95%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [780/810 (96%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [792/810 (98%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 27 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [5.329023906597286e-07, 4.912550934932369e-07, 4.0968583903122635e-07, 5.751208504989336e-07, 5.028238092563697e-07, 4.68586222268641e-07, 4.558660293696448e-07, 4.161448714512517e-07, 5.770881443822873e-07, 5.227715860200988e-07, 4.393757535581244e-07, 4.0928244970928063e-07, 4.896227210338111e-07, 4.663550612349354e-07, 5.331129955266078e-07, 4.795184622707893e-07, 5.706283445761073e-07, 4.5001075932304957e-07, 4.846182264373056e-07, 5.210763447394129e-07, 5.210151243772998e-07, 5.809918661725533e-07, 5.585462758972426e-07, 4.614777253664215e-07, 4.45389332526247e-07, 4.906302137897001e-07, 4.5114140334590047e-07, 4.4494441908682347e-07, 5.538121286008391e-07, 4.67487268451805e-07, 4.051188113862736e-07, 4.2189640225842595e-07, 5.414724455476971e-07, 4.077609503383428e-07, 4.2519036469457205e-07, 4.7427025151591806e-07, 2.6825824761544936e-07], 'L_si': [8.940696716308594e-08, 8.940696716308594e-08, 2.9802322387695312e-08, 8.940696716308594e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 1.1920928955078125e-07, 1.1920928955078125e-07, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 1.1920928955078125e-07, 2.9802322387695312e-08, 1.1920928955078125e-07, 2.9802322387695312e-08, 2.9802322387695312e-08, 8.940696716308594e-08, 8.940696716308594e-08, 8.940696716308594e-08, 8.940696716308594e-08, 2.9802322387695312e-08, 8.940696716308594e-08, 8.940696716308594e-08, 8.940696716308594e-08, 2.9802322387695312e-08, 8.940696716308594e-08, 8.940696716308594e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 8.940696716308594e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 8.940696716308594e-08, 5.960464477539063e-08], 'L_grad': [4.434954234966426e-07, 4.01848126330151e-07, 3.7988351664353104e-07, 4.857138833358476e-07, 4.730214868686744e-07, 4.387838998809457e-07, 4.260637069819495e-07, 3.8634254906355636e-07, 4.5787885483150603e-07, 4.035622964693175e-07, 4.095734311704291e-07, 3.794801273215853e-07, 4.598203986461158e-07, 4.365527388472401e-07, 4.139037059758266e-07, 4.4971611146138457e-07, 4.514190550253261e-07, 4.2020843693535426e-07, 4.548158756279008e-07, 4.316693491546175e-07, 4.3160815721421386e-07, 4.915848990094673e-07, 4.691393371558661e-07, 4.316754029787262e-07, 3.559823653631611e-07, 4.0122324662661413e-07, 3.6173443618281453e-07, 4.1514209669912816e-07, 4.644051330160437e-07, 3.7808030128871906e-07, 3.753164889985783e-07, 3.9209407987073064e-07, 4.5206550680632063e-07, 3.779586279506475e-07, 3.9538804230687674e-07, 3.848632843528321e-07, 2.0865361705091345e-07]}
Train Epoch: 28 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [36/810 (4%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [84/810 (10%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [96/810 (12%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [120/810 (15%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [240/810 (30%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [252/810 (31%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [276/810 (34%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [384/810 (47%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [468/810 (58%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [504/810 (62%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [684/810 (84%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [732/810 (90%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [756/810 (93%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 28 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 28 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving checkpoint: s2d_checkpoints/train_s2d_SpikeTransformer/checkpoint-epoch028-loss-0.0000.pth.tar ...
all losses in batch in validation:  {'loss': [6.827142442489276e-07, 5.414671022663242e-07, 4.5722151753579965e-07, 5.880934281776717e-07, 6.251208901630889e-07, 5.632361990137724e-07, 5.501786972672562e-07, 4.337919108365895e-07, 6.987671099523141e-07, 5.186564067116706e-07, 5.112067356094485e-07, 5.051841753811459e-07, 5.849276476510568e-07, 5.694178639714664e-07, 4.772525130647409e-07, 6.343141762954474e-07, 5.811128289678891e-07, 5.350028118300543e-07, 6.201347559908754e-07, 5.742498387917294e-07, 5.386265797824308e-07, 7.487835205211013e-07, 7.098193464116775e-07, 6.212692937879183e-07, 2.970508035105013e-07, 5.219565650804725e-07, 3.6073384990231716e-07, 5.60068542654335e-07, 6.310769435913244e-07, 3.7233098737488035e-07, 3.840542035504768e-07, 5.366522941585572e-07, 6.389516329363687e-07, 5.157749569661974e-07, 4.77665366815927e-07, 5.235069124864822e-07, 2.351665102651168e-07], 'L_si': [2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, -8.940696716308594e-08, -5.960464477539063e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 0.0], 'L_grad': [6.529119218612323e-07, 4.818624574909336e-07, 4.2741919514810434e-07, 6.775003953407577e-07, 6.847255349384795e-07, 5.036315542383818e-07, 5.203763748795609e-07, 4.337919108365895e-07, 6.391624651769234e-07, 4.888540843239753e-07, 4.814044132217532e-07, 4.7538182457174116e-07, 5.551253252633614e-07, 5.396155415837711e-07, 5.368571578401315e-07, 6.045118539077521e-07, 5.811128289678891e-07, 5.05200489442359e-07, 6.499370783785707e-07, 5.742498387917294e-07, 5.088242573947355e-07, 7.18981198133406e-07, 6.800170240239822e-07, 5.91466971400223e-07, 3.566554482858919e-07, 4.921542426927772e-07, 3.9053617229001247e-07, 5.004638978789444e-07, 6.012746212036291e-07, 4.0213330976257566e-07, 4.138565259381721e-07, 5.068499717708619e-07, 6.389516329363687e-07, 4.85972634578502e-07, 4.478630728499411e-07, 4.6390226771109155e-07, 2.351665102651168e-07]}
Train Epoch: 29 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [24/810 (3%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [36/810 (4%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [132/810 (16%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [144/810 (18%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [156/810 (19%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [168/810 (21%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [204/810 (25%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [264/810 (33%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [312/810 (39%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [432/810 (53%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [456/810 (56%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [492/810 (61%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [516/810 (64%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [540/810 (67%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [552/810 (68%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [624/810 (77%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [672/810 (83%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [720/810 (89%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [744/810 (92%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [768/810 (95%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 29 [792/810 (98%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 29 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [4.3773064817287377e-07, 4.0207740426012606e-07, 3.513788726650091e-07, 3.9875146740087075e-07, 4.5048199126540567e-07, 4.147401000409445e-07, 4.0303538639818726e-07, 4.0266138512379257e-07, 4.3114141590194777e-07, 3.729768138782674e-07, 4.171532737018424e-07, 3.8607629448961234e-07, 4.645677904591139e-07, 4.4248332642382593e-07, 4.47972354322701e-07, 4.511716440447344e-07, 4.554174779514142e-07, 4.2545136125227145e-07, 3.765493943319598e-07, 4.061078300310328e-07, 4.032581841784122e-07, 4.2672297695389716e-07, 4.340767247867916e-07, 4.083175326741184e-07, 2.971371202420414e-07, 4.322621691699169e-07, 3.044922323169885e-07, 3.877288747844432e-07, 4.655329064462421e-07, 3.840198701254849e-07, 3.471448621894524e-07, 3.332927747123904e-07, 3.928788885332324e-07, 3.7866527691221563e-07, 3.6920988577548997e-07, 3.5754848681790463e-07, 2.8615079372684704e-07], 'L_si': [2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, -5.960464477539063e-08, 0.0, 0.0, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -5.960464477539063e-08, 0.0, 0.0, -2.9802322387695312e-08, 0.0, 0.0, -2.9802322387695312e-08, 5.960464477539063e-08, -2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 0.0, 8.940696716308594e-08], 'L_grad': [4.0792832578517846e-07, 3.7227508187243075e-07, 3.513788726650091e-07, 4.583561121762614e-07, 4.5048199126540567e-07, 4.147401000409445e-07, 4.0303538639818726e-07, 3.7285906273609726e-07, 4.3114141590194777e-07, 3.729768138782674e-07, 3.873509513141471e-07, 3.56273972101917e-07, 4.347654680714186e-07, 4.126810040361306e-07, 3.8836770954731037e-07, 4.213693216570391e-07, 4.2561515556371887e-07, 3.9564903886457614e-07, 4.361540391073504e-07, 4.061078300310328e-07, 4.032581841784122e-07, 4.5652529934159247e-07, 4.340767247867916e-07, 4.083175326741184e-07, 3.2693944262973673e-07, 3.7265752439452626e-07, 3.3429455470468383e-07, 3.877288747844432e-07, 4.357305840585468e-07, 3.5421754773778957e-07, 3.471448621894524e-07, 3.630950971000857e-07, 4.226812109209277e-07, 3.488629545245203e-07, 3.6920988577548997e-07, 3.5754848681790463e-07, 1.967438123529064e-07]}
Train Epoch: 30 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [36/810 (4%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [48/810 (6%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [108/810 (13%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [156/810 (19%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [240/810 (30%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [276/810 (34%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [300/810 (37%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [312/810 (39%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [360/810 (44%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [456/810 (56%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [552/810 (68%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [588/810 (73%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [660/810 (81%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [720/810 (89%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [780/810 (96%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 30 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 30 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [2.8530806162052613e-07, 2.4393864350713557e-07, 2.509369778636028e-07, 3.370803085545049e-07, 3.6140306747256545e-07, 3.0030219022592064e-07, 2.9122367095624213e-07, 2.740989089033974e-07, 3.282309251062543e-07, 2.753152159584715e-07, 2.768146885046008e-07, 2.2743452632312255e-07, 3.154408716454782e-07, 2.9697730496991426e-07, 2.585842082680756e-07, 3.140156081826717e-07, 3.07557343148801e-07, 2.859809171695815e-07, 3.174820335516415e-07, 2.961144218716072e-07, 2.969572392430564e-07, 3.50211564636993e-07, 3.3592664294701535e-07, 3.0024386887816945e-07, 1.7554688724885636e-07, 2.752069292455417e-07, 1.7641490046571562e-07, 2.837474823991215e-07, 3.2552119932915957e-07, 2.2220979190024082e-07, 2.1912569536652882e-07, 2.411415778169612e-07, 3.207546512840054e-07, 1.9983048105132184e-07, 2.656504420883721e-07, 2.576193196546228e-07, 1.4078665344641195e-07], 'L_si': [-2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2.9802322387695312e-08, 0.0, 0.0, -2.9802322387695312e-08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.960464477539063e-08, 0.0, -5.960464477539063e-08, 0.0, 0.0, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, -5.960464477539063e-08, 0.0, 0.0, 0.0], 'L_grad': [3.1511038400822144e-07, 2.737409658948309e-07, 2.509369778636028e-07, 3.370803085545049e-07, 3.3160074508487014e-07, 3.0030219022592064e-07, 2.9122367095624213e-07, 2.740989089033974e-07, 3.282309251062543e-07, 2.753152159584715e-07, 2.768146885046008e-07, 2.5723684871081787e-07, 3.154408716454782e-07, 2.9697730496991426e-07, 2.883865306557709e-07, 3.140156081826717e-07, 3.07557343148801e-07, 2.859809171695815e-07, 3.174820335516415e-07, 2.961144218716072e-07, 2.969572392430564e-07, 3.50211564636993e-07, 3.3592664294701535e-07, 3.0024386887816945e-07, 2.35151532024247e-07, 2.752069292455417e-07, 2.3601954524110624e-07, 2.837474823991215e-07, 3.2552119932915957e-07, 2.5201211428793613e-07, 2.4892801775422413e-07, 2.7094390020465653e-07, 3.207546512840054e-07, 2.5943512582671246e-07, 2.656504420883721e-07, 2.576193196546228e-07, 1.4078665344641195e-07]}
Train Epoch: 31 [0/810 (0%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [36/810 (4%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [60/810 (7%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [72/810 (9%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [144/810 (18%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [168/810 (21%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [180/810 (22%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [192/810 (24%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [204/810 (25%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [228/810 (28%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [252/810 (31%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [324/810 (40%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [348/810 (43%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [372/810 (46%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [396/810 (49%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [432/810 (53%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [444/810 (55%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [456/810 (56%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [468/810 (58%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [516/810 (64%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [540/810 (67%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [576/810 (71%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [708/810 (87%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [720/810 (89%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [732/810 (90%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 31 [780/810 (96%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [792/810 (98%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 31 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
all losses in batch in validation:  {'loss': [3.4485532296457677e-07, 3.7422375953610754e-07, 3.257116532040527e-07, 3.8409837088693166e-07, 3.434335269503208e-07, 3.663623999727861e-07, 3.018837446688849e-07, 2.6882227643909573e-07, 3.32806337155489e-07, 3.181064869295369e-07, 3.2131157468029414e-07, 3.54009102920827e-07, 3.2443458053421637e-07, 3.1070717909642553e-07, 3.8533119095518487e-07, 3.2891122714318044e-07, 3.2139206496140105e-07, 3.5785399177257204e-07, 3.858249044697004e-07, 3.078536678913224e-07, 4.277268885743979e-07, 3.5324447367202083e-07, 3.9654861438975786e-07, 3.7617434145431616e-07, 2.9961353220642195e-07, 3.7245791872919654e-07, 3.1180559290078236e-07, 2.685533218027558e-07, 3.361879521435185e-07, 2.8941241225766134e-07, 2.621712269501586e-07, 3.9631547110730025e-07, 3.246130688694393e-07, 3.347660424424248e-07, 3.9568146803503623e-07, 3.579285703381174e-07, 1.0052144716610201e-07], 'L_si': [0.0, 5.960464477539063e-08, 2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 0.0, 5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 8.940696716308594e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, -5.960464477539063e-08, -2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, 8.940696716308594e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 8.940696716308594e-08, 5.960464477539063e-08, -5.960464477539063e-08], 'L_grad': [3.4485532296457677e-07, 3.146191147607169e-07, 2.959093308163574e-07, 3.8409837088693166e-07, 3.732358493380161e-07, 3.3656007758509077e-07, 3.316860670565802e-07, 2.9862459882679104e-07, 3.6260865954318433e-07, 3.181064869295369e-07, 3.2131157468029414e-07, 2.9440445814543637e-07, 3.542369029219117e-07, 3.4050950148412085e-07, 3.2572654617979424e-07, 3.5871354953087575e-07, 3.5119438734909636e-07, 3.2805166938487673e-07, 3.560225820820051e-07, 3.376559902790177e-07, 3.3831992141131195e-07, 3.8304679605971614e-07, 3.6674629200206255e-07, 3.4637201906662085e-07, 2.6981120981872664e-07, 3.128532739538059e-07, 2.8200327051308705e-07, 3.2815796657814644e-07, 3.659902745312138e-07, 2.8941241225766134e-07, 2.919735493378539e-07, 3.069085039442143e-07, 3.544153912571346e-07, 3.049637200547295e-07, 3.062745008719503e-07, 2.983239255627268e-07, 1.6012609194149263e-07]}
Train Epoch: 32 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [60/810 (7%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [144/810 (18%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [216/810 (27%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [240/810 (30%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [276/810 (34%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [312/810 (39%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [408/810 (50%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [420/810 (52%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [564/810 (70%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [576/810 (71%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [588/810 (73%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [720/810 (89%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [732/810 (90%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 32 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 32 [804/810 (99%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
Saving checkpoint: s2d_checkpoints/train_s2d_SpikeTransformer/checkpoint-epoch032-loss-0.0000.pth.tar ...
all losses in batch in validation:  {'loss': [2.5934593850251986e-07, 2.644687810970936e-07, 2.450260296882334e-07, 2.753204171312973e-07, 3.3050611136786756e-07, 2.0055838945154392e-07, 2.8846778832303244e-07, 2.2265579957547743e-07, 2.2508319830194523e-07, 2.6235747441205604e-07, 2.697678951335547e-07, 2.497071704965492e-07, 2.805781207371183e-07, 2.0375985343434877e-07, 2.7514482781043625e-07, 2.768024387478363e-07, 2.7726244411496737e-07, 1.8881372909618221e-07, 3.520560483138979e-07, 2.602069173462951e-07, 2.245065928718759e-07, 3.31075625581434e-07, 2.7888205522685894e-07, 2.063603972146666e-07, 2.5103301481976814e-07, 1.7943125385500025e-07, 2.3317767272601486e-07, 2.1713593412187038e-07, 2.2067348481868976e-07, 2.39117923683807e-07, 2.0830854907671892e-07, 1.9945676399402146e-07, 3.152538283757167e-07, 2.198626702920592e-07, 2.5511923240628676e-07, 2.502719382846408e-07, 1.6570217553635302e-07], 'L_si': [2.9802322387695312e-08, 5.960464477539063e-08, 5.960464477539063e-08, 0.0, 5.960464477539063e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 5.960464477539063e-08, 5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 8.940696716308594e-08, 2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 8.940696716308594e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 0.0, -2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 5.960464477539063e-08, 5.960464477539063e-08], 'L_grad': [2.2954361611482454e-07, 2.0486413632170297e-07, 1.8542138491284277e-07, 2.753204171312973e-07, 2.7090146659247694e-07, 2.3036071183923923e-07, 2.288631435476418e-07, 1.9285347718778212e-07, 2.5488552068964054e-07, 2.027528296366654e-07, 2.101632645690188e-07, 1.901025257211586e-07, 2.50775798349423e-07, 2.3356217582204408e-07, 2.1554018303504563e-07, 2.4700011636014096e-07, 2.4746012172727205e-07, 2.1861605148387753e-07, 2.62649081150812e-07, 2.3040459495859977e-07, 2.245065928718759e-07, 2.7147098080604337e-07, 2.490797328391636e-07, 2.361627196023619e-07, 1.616260476566822e-07, 2.0923357624269556e-07, 1.7357302795062424e-07, 2.1713593412187038e-07, 2.5047580720638507e-07, 1.795132789084164e-07, 1.785062266890236e-07, 1.9945676399402146e-07, 2.5564918360032607e-07, 1.9006034790436388e-07, 1.9551460184175085e-07, 1.9066729350925016e-07, 1.060975307609624e-07]}
Train Epoch: 33 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [72/810 (9%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [84/810 (10%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [132/810 (16%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [252/810 (31%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [264/810 (33%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [312/810 (39%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [348/810 (43%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [360/810 (44%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [396/810 (49%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [408/810 (50%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [420/810 (52%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [444/810 (55%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [492/810 (61%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [516/810 (64%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [528/810 (65%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [564/810 (70%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [672/810 (83%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [732/810 (90%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [756/810 (93%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 33 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 33 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [2.4584954871897935e-07, 1.9789888483501272e-07, 1.856053728488405e-07, 2.3947708882587904e-07, 2.341541005534964e-07, 2.115951929226867e-07, 2.0883227591639297e-07, 2.1060948540707614e-07, 2.3929470671646413e-07, 2.116679382879738e-07, 2.0178157456030021e-07, 1.8914113297796575e-07, 2.211252194683766e-07, 2.1121985582794878e-07, 2.504330041119829e-07, 2.2334795346523606e-07, 2.1717676190746715e-07, 2.0473466122439277e-07, 2.2589387072002864e-07, 2.1382228965194372e-07, 2.1045187281742983e-07, 2.6406743813822686e-07, 2.2666947074867494e-07, 2.459330517012859e-07, 1.7495393933586456e-07, 2.0080696572222223e-07, 1.773928630655064e-07, 2.0462039174162783e-07, 2.2978267111284367e-07, 1.847566721835392e-07, 1.8353257758008112e-07, 1.9985975541203516e-07, 2.2493399853829033e-07, 1.93740220311156e-07, 1.9288545161089132e-07, 1.902289028521409e-07, 1.1642913477771799e-07], 'L_si': [2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08], 'L_grad': [2.1604724054213875e-07, 1.680965624473174e-07, 1.5580305046114518e-07, 2.0967476643818372e-07, 2.0435177816580108e-07, 1.8179287053499138e-07, 1.7902995352869766e-07, 1.8080716301938082e-07, 2.094923701179141e-07, 1.818656159002785e-07, 1.719792521726049e-07, 1.5933881059027044e-07, 1.9132289708068129e-07, 1.8141753344025346e-07, 1.9082835933659226e-07, 1.9354563107754075e-07, 1.8737443951977184e-07, 1.7493233883669745e-07, 1.9609154833233333e-07, 1.840199672642484e-07, 1.8064955042973452e-07, 2.3426511575053155e-07, 2.2666947074867494e-07, 1.8632842113674997e-07, 1.4515161694816925e-07, 1.7100464333452692e-07, 1.4759054067781108e-07, 1.7481806935393251e-07, 1.9998034872514836e-07, 1.5495434979584388e-07, 1.537302551923858e-07, 1.7005743302433984e-07, 1.95131676150595e-07, 1.639378979234607e-07, 1.63083129223196e-07, 1.6042658046444558e-07, 8.662681239002268e-08]}
Train Epoch: 34 [0/810 (0%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [60/810 (7%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [84/810 (10%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [120/810 (15%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [132/810 (16%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [180/810 (22%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [372/810 (46%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [420/810 (52%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [456/810 (56%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [588/810 (73%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [648/810 (80%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [672/810 (83%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [684/810 (84%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [720/810 (89%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [756/810 (93%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 34 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 34 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [2.180131701834398e-07, 1.7129457319242647e-07, 1.2828732565139944e-07, 1.9145852547808317e-07, 2.1955898432679533e-07, 1.865737431216985e-07, 1.8604096396757086e-07, 1.6332452901224315e-07, 2.6366808469902026e-07, 1.6957729087607731e-07, 1.7434612686884066e-07, 1.9157776875999843e-07, 2.3013348027234315e-07, 1.611167590453988e-07, 1.8091589026880683e-07, 2.599740014375129e-07, 1.706519157096409e-07, 1.7997265899794002e-07, 1.5217619875329547e-07, 1.893504872896301e-07, 1.8359150999458507e-07, 2.1574147979208647e-07, 1.709950936401583e-07, 2.52832194291841e-07, 1.1066484262300946e-07, 2.037920410202787e-07, 1.4929571534594288e-07, 1.7862294043879956e-07, 2.00967150476572e-07, 9.291572666825232e-08, 9.329005479230545e-08, 1.9786779148489586e-07, 2.0707696535282594e-07, 1.9261295847172732e-07, 1.3350025085401285e-07, 1.6284350579098827e-07, 1.0136128025806102e-07], 'L_si': [5.960464477539063e-08, 2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 8.940696716308594e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 8.940696716308594e-08, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 8.940696716308594e-08, 0.0, 5.960464477539063e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 2.9802322387695312e-08, 5.960464477539063e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08], 'L_grad': [1.5840852540804917e-07, 1.4149225080473116e-07, 1.2828732565139944e-07, 1.9145852547808317e-07, 1.897566619391e-07, 1.5677142073400319e-07, 1.5623864157987555e-07, 1.3352220662454783e-07, 1.7426111753593432e-07, 1.39774968488382e-07, 1.4454380448114534e-07, 1.319731239846078e-07, 1.7052883549695252e-07, 1.611167590453988e-07, 1.5111356788111152e-07, 1.70567034274427e-07, 1.706519157096409e-07, 1.501703366102447e-07, 1.8197852114099078e-07, 1.595481649019348e-07, 1.5378918760688975e-07, 1.8593915740439115e-07, 1.709950936401583e-07, 1.6342522712875507e-07, 1.1066484262300946e-07, 1.4418739624488808e-07, 1.1949339295824757e-07, 1.4882061805110425e-07, 1.7116482808887667e-07, 1.2271804905594763e-07, 1.2309237718000077e-07, 1.3826314670950524e-07, 1.7727464296513062e-07, 1.330083136963367e-07, 1.3350025085401285e-07, 1.3304118340329296e-07, 7.15589578703657e-08]}
Train Epoch: 35 [0/810 (0%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [24/810 (3%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [72/810 (9%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [120/810 (15%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [144/810 (18%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [180/810 (22%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [216/810 (27%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [276/810 (34%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [336/810 (41%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [348/810 (43%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [408/810 (50%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [420/810 (52%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [432/810 (53%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [444/810 (55%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [468/810 (58%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [528/810 (65%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [540/810 (67%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [552/810 (68%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [612/810 (76%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [732/810 (90%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [744/810 (92%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [756/810 (93%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [780/810 (96%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 35 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 35 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [7.28483229295307e-08, 3.525422442862691e-08, 2.890524797294347e-08, 8.87594922005519e-08, 8.680278540396102e-08, 4.523931096400702e-08, 4.2594344051849475e-08, 3.002389803441474e-08, 8.03224367018629e-08, 3.5547557786230755e-08, 3.668052528382759e-08, 5.962458260455605e-08, 8.06058224611661e-08, 4.403653974804911e-08, 6.841843003257964e-08, 7.848284866440736e-08, 7.807214785771066e-08, 3.920277436009201e-08, 8.24460215653744e-08, 7.295228243719976e-08, 7.267207990935276e-08, 8.684948227255518e-08, 7.92287266904168e-08, 7.426312720326678e-08, 2.200329163315473e-08, 3.63540522130279e-08, 2.4434520184968278e-08, 3.8469110563710274e-08, 8.148995789269975e-08, 2.7983574568679614e-08, 2.6934607433304336e-08, 6.391786655512988e-08, 8.040841237288987e-08, 3.018725891479335e-08, 3.219880539973019e-08, 3.068610965328844e-08, -1.1014599010650272e-08], 'L_si': [-2.9802322387695312e-08, -5.960464477539063e-08, -5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -5.960464477539063e-08, -5.960464477539063e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -5.960464477539063e-08, -5.960464477539063e-08, -5.960464477539063e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -5.960464477539063e-08, -5.960464477539063e-08, -5.960464477539063e-08], 'L_grad': [1.0265064531722601e-07, 9.485886920401754e-08, 8.85098927483341e-08, 1.1856181458824722e-07, 1.1660510779165634e-07, 1.0484395573939764e-07, 1.021989888272401e-07, 8.962854280980537e-08, 1.1012475908955821e-07, 9.515220256162138e-08, 9.628517005921822e-08, 8.942690499225137e-08, 1.1040814484886141e-07, 1.0364118452343973e-07, 9.822075242027495e-08, 1.0828517105210267e-07, 1.0787447024540597e-07, 9.880741913548263e-08, 1.1224834395306971e-07, 1.0275460482489507e-07, 1.0247440229704807e-07, 1.1665180466025049e-07, 1.0903104907811212e-07, 1.0406544959096209e-07, 8.160793640854536e-08, 9.595869698841852e-08, 8.40391649603589e-08, 9.80737553391009e-08, 1.1129228028039506e-07, 8.758821934407024e-08, 8.653925220869496e-08, 9.372018894282519e-08, 1.1021073476058518e-07, 8.979190369018397e-08, 9.180345017512082e-08, 9.029075442867907e-08, 4.859004576474035e-08]}
Train Epoch: 36 [0/810 (0%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [60/810 (7%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [96/810 (12%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [132/810 (16%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [156/810 (19%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [204/810 (25%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [324/810 (40%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [348/810 (43%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [360/810 (44%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [384/810 (47%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [396/810 (49%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [420/810 (52%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [432/810 (53%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [456/810 (56%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [480/810 (59%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [492/810 (61%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [552/810 (68%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [624/810 (77%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [636/810 (79%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [660/810 (81%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [684/810 (84%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [720/810 (89%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [744/810 (92%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [768/810 (95%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 36 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 36 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
Saving checkpoint: s2d_checkpoints/train_s2d_SpikeTransformer/checkpoint-epoch036-loss-0.0000.pth.tar ...
all losses in batch in validation:  {'loss': [5.6952416116473614e-09, 6.055665835447144e-08, 5.82749137834071e-08, 1.3250563313249586e-08, 1.2667200621763186e-08, 6.1005351881249226e-09, 4.8542148078922764e-09, 5.929170754370716e-08, 9.363972708342772e-09, 6.225677395832463e-08, 2.6859794388656155e-09, -4.1447378862358164e-10, 8.91218121523707e-09, 5.281407311485964e-09, 3.7941276787023526e-09, 7.699924253756762e-09, 6.7221009203422e-08, 3.8873011476425745e-09, 9.633495778871293e-09, 5.79541392653482e-09, 1.541927900916562e-07, 1.2194711018764792e-08, 8.905942650017096e-09, 5.979288175694819e-09, -4.9713548833096866e-09, 2.690491385237692e-09, 1.4531623548919015e-07, 6.28038918648599e-08, 9.412943313691358e-09, 5.773637212769245e-08, -1.904474800085154e-09, 1.9211370272387285e-09, 9.020368452183902e-09, -7.666329793210025e-10, -2.0451551563382964e-10, 5.899052268887317e-08, -4.3175688801966317e-08], 'L_si': [-8.940696716308594e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -2.9802322387695312e-08, -8.940696716308594e-08, -2.9802322387695312e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -2.9802322387695312e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, 5.960464477539063e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, 5.960464477539063e-08, -2.9802322387695312e-08, -8.940696716308594e-08, -2.9802322387695312e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -2.9802322387695312e-08, -8.940696716308594e-08], 'L_grad': [9.51022087747333e-08, 9.035898074216675e-08, 8.807723617110241e-08, 1.0265753047633552e-07, 1.0207416778484912e-07, 9.550750235121086e-08, 9.426118197097821e-08, 8.909402993140247e-08, 9.877093987142871e-08, 9.205909634601994e-08, 9.209294660195155e-08, 8.899249337446236e-08, 9.831914837832301e-08, 9.46883744745719e-08, 9.320109484178829e-08, 9.71068914168427e-08, 9.702333159111731e-08, 9.329426831072851e-08, 9.904046294195723e-08, 9.520238108962076e-08, 9.458814531626558e-08, 1.0160167818185073e-07, 9.831290981310303e-08, 9.538625533878076e-08, 8.443561227977625e-08, 9.209745854832363e-08, 8.571159071379952e-08, 9.260621425255522e-08, 9.88199104767773e-08, 8.753869451538776e-08, 8.750249236300078e-08, 9.132810419032467e-08, 9.842733561526984e-08, 8.864033418376494e-08, 8.920245164745211e-08, 8.879284507656848e-08, 4.623127836111962e-08]}
Train Epoch: 37 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [84/810 (10%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [96/810 (12%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [156/810 (19%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [168/810 (21%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [240/810 (30%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [276/810 (34%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [312/810 (39%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [348/810 (43%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [420/810 (52%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [432/810 (53%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [456/810 (56%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [480/810 (59%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [504/810 (62%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [516/810 (64%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [540/810 (67%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [552/810 (68%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [588/810 (73%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [636/810 (79%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [672/810 (83%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [720/810 (89%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 37 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 37 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [-1.6148668180449022e-08, 3.133123271936711e-08, 2.5152235849645876e-08, 1.4068152154322888e-07, 8.109816462820163e-08, 6.9995280682633165e-09, 3.847978291560139e-08, 4.823358779049158e-08, -1.0783239190459426e-08, 3.351468080836639e-08, 3.233751755260528e-08, -8.952767416303686e-10, -1.7749890446339123e-08, -2.117575803595173e-08, 1.055207548006365e-07, -1.7967934695661825e-08, -1.6957891091351485e-08, 5.74808467490584e-09, 7.789746092612404e-08, -2.0005110457077535e-08, -2.4202854831401055e-08, -7.748809593977057e-09, -1.285141593143635e-08, -1.6418155723840755e-08, 1.891457657166029e-08, 2.5299087269559095e-09, 2.141939958733019e-08, -2.6171704803346074e-08, 1.564824003708054e-08, 2.3457804587678766e-08, 2.3696557605035196e-08, 2.6727633439804777e-09, 7.509464694521739e-08, -4.574367551413161e-10, 2.7623386245068104e-08, 2.797040821178598e-08, 1.1449117209849646e-09], 'L_si': [-8.940696716308594e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 5.960464477539063e-08, 0.0, -5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -8.940696716308594e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -8.940696716308594e-08, -8.940696716308594e-08, 2.9802322387695312e-08, -8.940696716308594e-08, -8.940696716308594e-08, -5.960464477539063e-08, 0.0, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -8.940696716308594e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -2.9802322387695312e-08, -8.940696716308594e-08, -5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, 0.0, -5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08], 'L_grad': [7.325829898263692e-08, 6.113355510706242e-08, 5.495455823734119e-08, 8.107688387326561e-08, 8.109816462820163e-08, 6.660417284365394e-08, 6.82821053032967e-08, 7.80359101781869e-08, 7.862372797262651e-08, 6.33170031960617e-08, 6.213983994030059e-08, 5.8709368033760256e-08, 7.165707671674681e-08, 6.823120912713421e-08, 7.571843241294118e-08, 7.143903246742411e-08, 7.244907607173445e-08, 6.535272945029647e-08, 7.789746092612404e-08, 6.94018567060084e-08, 6.520411233168488e-08, 8.165815756910888e-08, 7.655555123164959e-08, 7.298881143924518e-08, 4.8716898959355603e-08, 6.213455350234653e-08, 5.12217219750255e-08, 6.323526235973986e-08, 7.525288481247117e-08, 5.326012697537408e-08, 5.349887999273051e-08, 6.22774081193711e-08, 7.509464694521739e-08, 5.914720802024931e-08, 5.742570863276342e-08, 5.777273059948129e-08, 3.094723410868028e-08]}
Train Epoch: 38 [0/810 (0%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [48/810 (6%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [120/810 (15%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [132/810 (16%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [156/810 (19%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [168/810 (21%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [240/810 (30%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [264/810 (33%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [288/810 (36%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [312/810 (39%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [348/810 (43%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [372/810 (46%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [420/810 (52%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [480/810 (59%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [492/810 (61%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [516/810 (64%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [528/810 (65%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [576/810 (71%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [588/810 (73%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [600/810 (74%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [624/810 (77%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [672/810 (83%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [696/810 (86%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [720/810 (89%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [732/810 (90%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [744/810 (92%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [756/810 (93%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 38 [792/810 (98%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 38 [804/810 (99%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
all losses in batch in validation:  {'loss': [3.9274752339224506e-07, 6.974428856665327e-08, 4.720853752360199e-08, 1.906727646883155e-07, 1.3836915968568064e-07, 5.741957664895381e-08, 1.0030088049006736e-07, 8.095393866369704e-08, 2.1571111119556008e-07, 6.416706810341566e-08, 5.759203247635014e-08, 2.9793760347729403e-08, 1.0437511832606106e-07, 8.979293397715082e-08, 1.0164973218707019e-07, 3.660819629658363e-07, 1.3728967473980447e-07, 8.243324600698543e-08, 1.462569798604818e-07, 9.563828484715486e-08, 8.326306044637022e-08, 4.211024418054876e-07, 3.4240883906022646e-07, 1.7821254516547924e-07, 1.9284062346969222e-08, 1.0211942935711704e-07, 2.194615689177226e-08, 1.5373451844880037e-07, 1.244892899876504e-07, 2.7048137951624085e-08, 1.3410542010205972e-08, 8.93401903567792e-08, 1.608074455816677e-07, 1.5276052067747514e-07, 2.0603479811143188e-08, 3.0780604731717176e-08, 3.8984730110769306e-08], 'L_si': [2.9802322387695312e-08, -8.940696716308594e-08, -2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, 5.960464477539063e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, 0.0, -5.960464477539063e-08, -8.940696716308594e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, 2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -5.960464477539063e-08, -2.9802322387695312e-08, 0.0, -2.9802322387695312e-08, -8.940696716308594e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -2.9802322387695312e-08, -8.940696716308594e-08, -2.9802322387695312e-08, -2.9802322387695312e-08], 'L_grad': [3.6294520100454974e-07, 1.591512557297392e-07, 7.70108599112973e-08, 1.906727646883155e-07, 1.0856683729798533e-07, 8.722189903664912e-08, 1.3010320287776267e-07, 1.1075626105139236e-07, 2.455134335832554e-07, 1.2377171287880628e-07, 8.739435486404545e-08, 5.9596082735424716e-08, 1.3417744071375637e-07, 1.1959525636484614e-07, 1.0164973218707019e-07, 3.0647731819044566e-07, 1.6709199712749978e-07, 1.1223556839468074e-07, 1.462569798604818e-07, 1.552429296225455e-07, 1.7267002760945616e-07, 3.913001194177923e-07, 3.7221116144792177e-07, 1.4841022277778393e-07, 4.9086384734664534e-08, 1.3192175174481235e-07, 8.155080166716289e-08, 1.8353684083649568e-07, 1.244892899876504e-07, 5.68504603393194e-08, 1.0281750917329191e-07, 1.1914251274447452e-07, 1.9060976796936302e-07, 1.8256284306517045e-07, 1.1001044697422913e-07, 6.058292711941249e-08, 6.878705249846462e-08]}
Train Epoch: 39 [0/810 (0%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [12/810 (1%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [24/810 (3%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [60/810 (7%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [108/810 (13%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [180/810 (22%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [192/810 (24%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [228/810 (28%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [240/810 (30%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [264/810 (33%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [336/810 (41%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [348/810 (43%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [360/810 (44%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [372/810 (46%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [384/810 (47%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [396/810 (49%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [408/810 (50%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [420/810 (52%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [432/810 (53%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [444/810 (55%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [456/810 (56%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [468/810 (58%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [480/810 (59%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [492/810 (61%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [504/810 (62%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [516/810 (64%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [528/810 (65%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [540/810 (67%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [552/810 (68%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [564/810 (70%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [576/810 (71%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [588/810 (73%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [600/810 (74%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [612/810 (76%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [624/810 (77%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [636/810 (79%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [648/810 (80%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [660/810 (81%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [672/810 (83%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [684/810 (84%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [696/810 (86%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [708/810 (87%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [720/810 (89%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [732/810 (90%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [744/810 (92%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [756/810 (93%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 39 [768/810 (95%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [780/810 (96%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [792/810 (98%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 39 [804/810 (99%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
Validation: [0/222 (0%)]
Validation: [12/222 (5%)]
Validation: [24/222 (11%)]
Validation: [36/222 (16%)]
Validation: [48/222 (22%)]
Validation: [60/222 (27%)]
Validation: [72/222 (32%)]
Validation: [84/222 (38%)]
Validation: [96/222 (43%)]
Validation: [108/222 (49%)]
Validation: [120/222 (54%)]
Validation: [132/222 (59%)]
Validation: [144/222 (65%)]
Validation: [156/222 (70%)]
Validation: [168/222 (76%)]
Validation: [180/222 (81%)]
Validation: [192/222 (86%)]
Validation: [204/222 (92%)]
Validation: [216/222 (97%)]
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
all losses in batch in validation:  {'loss': [4.7119137036588654e-08, 1.6202777786133993e-08, 1.5548916820762315e-08, 5.030995353649814e-08, 4.959209931598707e-08, 4.8112269723787904e-08, 4.768553907297246e-08, 4.682615895035269e-08, 4.8935834939811684e-08, 1.6862811591522586e-08, 4.698538802472285e-08, 1.5260283703355526e-08, 5.000699587753843e-08, 4.81084825310063e-08, 1.8656521660886938e-08, 4.832105560126365e-08, 4.891685634333953e-08, 4.714350154699787e-08, 4.9236870580671166e-08, 4.7344499876089685e-08, 4.738775416512908e-08, 5.0024212328025897e-08, 4.835778710798877e-08, 4.807972331377641e-08, 1.3867289538893601e-08, 4.641039197395003e-08, 1.4959516292378794e-08, 1.7497953308520664e-08, 4.9582872918563226e-08, 1.513134684216766e-08, 1.4822610694409377e-08, 1.5785328599804416e-08, 4.8281954434514773e-08, 4.5203751852795904e-08, 4.5772537760058185e-08, 1.5218287074958425e-08, 3.810461635112006e-08], 'L_si': [2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 2.9802322387695312e-08, 0.0, 0.0, 0.0, 2.9802322387695312e-08, 2.9802322387695312e-08, 2.9802322387695312e-08, 0.0, 2.9802322387695312e-08], 'L_grad': [1.731681642525018e-08, 1.6202777786133993e-08, 1.5548916820762315e-08, 2.050763114880283e-08, 1.9789776928291758e-08, 1.830994733609259e-08, 1.7883216685277148e-08, 1.702383833901422e-08, 1.9133512552116372e-08, 1.6862811591522586e-08, 1.718306563702754e-08, 1.5260283703355526e-08, 2.0204675266199956e-08, 1.830616014331099e-08, 1.8656521660886938e-08, 1.85187314372115e-08, 1.911453395564422e-08, 1.7341177382945716e-08, 1.9434548192975853e-08, 1.7542175712037533e-08, 1.758543177743377e-08, 2.0221891716687423e-08, 1.8555464720293458e-08, 1.82774009260811e-08, 1.3867289538893601e-08, 1.6608069586254715e-08, 1.4959516292378794e-08, 1.7497953308520664e-08, 1.9780552307224752e-08, 1.513134684216766e-08, 1.4822610694409377e-08, 1.5785328599804416e-08, 1.84796338231763e-08, 1.540142946510059e-08, 1.5970215372362873e-08, 1.5218287074958425e-08, 8.302293963424745e-09]}
Train Epoch: 40 [0/810 (0%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [12/810 (1%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [24/810 (3%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 40 [36/810 (4%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [48/810 (6%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [60/810 (7%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [72/810 (9%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [84/810 (10%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [96/810 (12%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [108/810 (13%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 40 [120/810 (15%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [132/810 (16%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [144/810 (18%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [156/810 (19%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [168/810 (21%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [180/810 (22%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [192/810 (24%)] loss: 0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 40 [204/810 (25%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [216/810 (27%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [228/810 (28%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 40 [240/810 (30%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 40 [252/810 (31%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [264/810 (33%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Train Epoch: 40 [276/810 (34%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [288/810 (36%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [300/810 (37%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [312/810 (39%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [324/810 (40%)] loss: 0.0000 L_si: 0.0000 L_grad: 0.0000 
Train Epoch: 40 [336/810 (41%)] loss: -0.0000 L_si: -0.0000 L_grad: 0.0000 
Setting up Multi Scale Gradient loss...
Done
Setting up Multi Scale Gradient loss...
Done
/root/miniconda3/envs/scv/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 2 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
