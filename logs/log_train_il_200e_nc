/root/miniconda3/envs/scv/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Trainable parameters: 20545951
S2DepthTransformerUNetConv(
  (encoder): LongSpikeStreamEncoderConv(
    (swin3d): SwinTransformer3D(
      (patch_embed): PatchEmbedLocalGlobal(
        (head): Conv2d(32, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (global_head): Conv2d(128, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (residual_encoding): residual_feature_generator(
          (resblock1): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (resblock2): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (resblock3): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (resblock4): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (global_residual_encoding): residual_feature_generator(
          (resblock1): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (resblock2): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (resblock3): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
          (resblock4): ResidualBlock(
            (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (relu): ReLU()
            (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          )
        )
        (proj): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (global_proj): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock3D(
              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): Identity()
              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock3D(
              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=384, out_features=192, bias=False)
            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock3D(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock3D(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=768, out_features=384, bias=False)
            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock3D(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock3D(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (2): SwinTransformerBlock3D(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (3): SwinTransformerBlock3D(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (4): SwinTransformerBlock3D(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (5): SwinTransformerBlock3D(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention3D(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath()
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU()
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (conv_layers): ModuleList(
      (0): ModuleList(
        (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
        (1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
        (3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
      )
      (1): ModuleList(
        (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
        (1): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
        (2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
        (3): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))
      )
      (2): ModuleList(
        (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
        (3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (resblocks): ModuleList(
    (0): ResidualBlock(
      (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu): ReLU()
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (1): ResidualBlock(
      (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (relu): ReLU()
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (decoders): ModuleList(
    (0): UpsampleConvLayer(
      (conv2d): Conv2d(384, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    )
    (1): UpsampleConvLayer(
      (conv2d): Conv2d(192, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    )
    (2): UpsampleConvLayer(
      (conv2d): Conv2d(96, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    )
  )
  (pred): ConvLayer(
    (conv2d): Conv2d(48, 1, kernel_size=(1, 1), stride=(1, 1))
  )
)
Setting up Multi Scale Gradient loss...
Done
---- Single Training ----
Use GPU: 0 for training
Found 26 samples in /root/autodl-tmp/Spike-Stero/train
Found 8 samples in /root/autodl-tmp/Spike-Stero/validation
-----  [3, 6, 12]
---- new version 4 ----
Model Initialized
Using scale_invariant_loss with config {'weight': 1.0, 'n_lambda': 1.0}
Will not use phased architecture
Using Multi Scale Gradient loss with weight=0.25
Will not use MSE loss
Train Epoch: 1 [0/816 (0%)] loss: 0.0612 L_si: 0.0164 L_grad: 0.0448 
Train Epoch: 1 [36/816 (4%)] loss: 0.0498 L_si: 0.0139 L_grad: 0.0360 
Train Epoch: 1 [72/816 (9%)] loss: 0.0442 L_si: 0.0093 L_grad: 0.0349 
Train Epoch: 1 [108/816 (13%)] loss: 0.0570 L_si: 0.0202 L_grad: 0.0367 
Train Epoch: 1 [144/816 (18%)] loss: 0.0566 L_si: 0.0201 L_grad: 0.0365 
Train Epoch: 1 [180/816 (22%)] loss: 0.0511 L_si: 0.0215 L_grad: 0.0295 
Train Epoch: 1 [216/816 (26%)] loss: 0.0403 L_si: 0.0100 L_grad: 0.0303 
Train Epoch: 1 [252/816 (31%)] loss: 0.0438 L_si: 0.0110 L_grad: 0.0328 
Train Epoch: 1 [288/816 (35%)] loss: 0.0449 L_si: 0.0150 L_grad: 0.0300 
Train Epoch: 1 [324/816 (40%)] loss: 0.0497 L_si: 0.0176 L_grad: 0.0321 
Train Epoch: 1 [360/816 (44%)] loss: 0.0430 L_si: 0.0168 L_grad: 0.0262 
Train Epoch: 1 [396/816 (49%)] loss: 0.0418 L_si: 0.0108 L_grad: 0.0310 
Train Epoch: 1 [432/816 (53%)] loss: 0.0610 L_si: 0.0234 L_grad: 0.0376 
Train Epoch: 1 [468/816 (57%)] loss: 0.0412 L_si: 0.0098 L_grad: 0.0314 
Train Epoch: 1 [504/816 (62%)] loss: 0.0346 L_si: 0.0070 L_grad: 0.0276 
Train Epoch: 1 [540/816 (66%)] loss: 0.0485 L_si: 0.0170 L_grad: 0.0315 
Train Epoch: 1 [576/816 (71%)] loss: 0.0431 L_si: 0.0099 L_grad: 0.0333 
Train Epoch: 1 [612/816 (75%)] loss: 0.0440 L_si: 0.0133 L_grad: 0.0307 
Train Epoch: 1 [648/816 (79%)] loss: 0.0445 L_si: 0.0122 L_grad: 0.0323 
Train Epoch: 1 [684/816 (84%)] loss: 0.0443 L_si: 0.0107 L_grad: 0.0335 
Train Epoch: 1 [720/816 (88%)] loss: 0.0369 L_si: 0.0065 L_grad: 0.0303 
Train Epoch: 1 [756/816 (93%)] loss: 0.0404 L_si: 0.0095 L_grad: 0.0309 
Train Epoch: 1 [792/816 (97%)] loss: 0.0502 L_si: 0.0172 L_grad: 0.0329 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.05040234699845314, 0.049611881375312805, 0.04912871494889259, 0.04927756264805794, 0.04556046426296234, 0.04874375835061073, 0.04936615377664566, 0.047659892588853836, 0.04361480847001076, 0.04857015609741211, 0.05099163576960564, 0.050282757729291916, 0.0492909736931324, 0.049280546605587006, 0.045515984296798706, 0.048494964838027954, 0.04940866678953171, 0.05008064955472946, 0.0211978517472744], 'L_si': [0.011641829274594784, 0.011379243806004524, 0.011466849595308304, 0.010119329206645489, 0.007962381467223167, 0.010622646659612656, 0.011927260085940361, 0.011290140450000763, 0.007897108793258667, 0.01116800494492054, 0.011196691542863846, 0.010341648012399673, 0.01150540355592966, 0.011813497170805931, 0.008968396112322807, 0.009098023176193237, 0.010507456958293915, 0.011144479736685753, 0.010492691770195961], 'L_grad': [0.03876051679253578, 0.03823263570666313, 0.03766186535358429, 0.039158232510089874, 0.037598080933094025, 0.03812111169099808, 0.03743889182806015, 0.03636975213885307, 0.03571769967675209, 0.03740215301513672, 0.03979494422674179, 0.03994110971689224, 0.037785571068525314, 0.037467047572135925, 0.03654758632183075, 0.03939694166183472, 0.03890120983123779, 0.03893617168068886, 0.010705160908401012]}
Train Epoch: 2 [0/816 (0%)] loss: 0.0367 L_si: 0.0055 L_grad: 0.0312 
Train Epoch: 2 [36/816 (4%)] loss: 0.0408 L_si: 0.0116 L_grad: 0.0292 
Train Epoch: 2 [72/816 (9%)] loss: 0.0519 L_si: 0.0177 L_grad: 0.0343 
Train Epoch: 2 [108/816 (13%)] loss: 0.0424 L_si: 0.0129 L_grad: 0.0295 
Train Epoch: 2 [144/816 (18%)] loss: 0.0446 L_si: 0.0147 L_grad: 0.0299 
Train Epoch: 2 [180/816 (22%)] loss: 0.0343 L_si: 0.0070 L_grad: 0.0273 
Train Epoch: 2 [216/816 (26%)] loss: 0.0443 L_si: 0.0136 L_grad: 0.0307 
Train Epoch: 2 [252/816 (31%)] loss: 0.0398 L_si: 0.0089 L_grad: 0.0309 
Train Epoch: 2 [288/816 (35%)] loss: 0.0351 L_si: 0.0079 L_grad: 0.0272 
Train Epoch: 2 [324/816 (40%)] loss: 0.0490 L_si: 0.0185 L_grad: 0.0305 
Train Epoch: 2 [360/816 (44%)] loss: 0.0407 L_si: 0.0095 L_grad: 0.0313 
Train Epoch: 2 [396/816 (49%)] loss: 0.0468 L_si: 0.0135 L_grad: 0.0333 
Train Epoch: 2 [432/816 (53%)] loss: 0.0434 L_si: 0.0096 L_grad: 0.0339 
Train Epoch: 2 [468/816 (57%)] loss: 0.0373 L_si: 0.0091 L_grad: 0.0281 
Train Epoch: 2 [504/816 (62%)] loss: 0.0453 L_si: 0.0135 L_grad: 0.0318 
Train Epoch: 2 [540/816 (66%)] loss: 0.0486 L_si: 0.0161 L_grad: 0.0325 
Train Epoch: 2 [576/816 (71%)] loss: 0.0491 L_si: 0.0175 L_grad: 0.0316 
Train Epoch: 2 [612/816 (75%)] loss: 0.0520 L_si: 0.0214 L_grad: 0.0306 
Train Epoch: 2 [648/816 (79%)] loss: 0.0417 L_si: 0.0112 L_grad: 0.0305 
Train Epoch: 2 [684/816 (84%)] loss: 0.0384 L_si: 0.0085 L_grad: 0.0299 
Train Epoch: 2 [720/816 (88%)] loss: 0.0418 L_si: 0.0097 L_grad: 0.0321 
Train Epoch: 2 [756/816 (93%)] loss: 0.0446 L_si: 0.0120 L_grad: 0.0326 
Train Epoch: 2 [792/816 (97%)] loss: 0.0477 L_si: 0.0175 L_grad: 0.0302 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.04534702003002167, 0.04514481872320175, 0.048268936574459076, 0.045376427471637726, 0.04294198006391525, 0.04326232522726059, 0.0450718067586422, 0.04559992253780365, 0.04551173374056816, 0.04487238824367523, 0.043022822588682175, 0.042343080043792725, 0.04388309270143509, 0.04472634196281433, 0.04333886504173279, 0.04465702548623085, 0.045562706887722015, 0.044472139328718185, 0.018603194504976273], 'L_si': [0.010532746091485023, 0.010503577068448067, 0.011266926303505898, 0.01060531847178936, 0.009539935737848282, 0.01072598621249199, 0.010962720029056072, 0.009450891986489296, 0.011281982064247131, 0.010063733905553818, 0.010670345276594162, 0.00938127189874649, 0.011270590126514435, 0.009951869025826454, 0.010843407362699509, 0.010193128138780594, 0.010810509324073792, 0.011615466326475143, 0.010559335350990295], 'L_grad': [0.034814272075891495, 0.034641239792108536, 0.03700201213359833, 0.034771110862493515, 0.03340204432606697, 0.0325363390147686, 0.03410908579826355, 0.0361490324139595, 0.03422975167632103, 0.034808654338121414, 0.03235247731208801, 0.032961808145046234, 0.032612502574920654, 0.034774474799633026, 0.03249545767903328, 0.034463897347450256, 0.034752197563648224, 0.03285667300224304, 0.008043859153985977]}
Train Epoch: 3 [0/816 (0%)] loss: 0.0535 L_si: 0.0197 L_grad: 0.0338 
Train Epoch: 3 [36/816 (4%)] loss: 0.0374 L_si: 0.0076 L_grad: 0.0298 
Train Epoch: 3 [72/816 (9%)] loss: 0.0407 L_si: 0.0126 L_grad: 0.0281 
Train Epoch: 3 [108/816 (13%)] loss: 0.0539 L_si: 0.0201 L_grad: 0.0337 
Train Epoch: 3 [144/816 (18%)] loss: 0.0369 L_si: 0.0085 L_grad: 0.0283 
Train Epoch: 3 [180/816 (22%)] loss: 0.0374 L_si: 0.0086 L_grad: 0.0288 
Train Epoch: 3 [216/816 (26%)] loss: 0.0410 L_si: 0.0125 L_grad: 0.0285 
Train Epoch: 3 [252/816 (31%)] loss: 0.0541 L_si: 0.0154 L_grad: 0.0387 
Train Epoch: 3 [288/816 (35%)] loss: 0.0389 L_si: 0.0102 L_grad: 0.0287 
Train Epoch: 3 [324/816 (40%)] loss: 0.0431 L_si: 0.0114 L_grad: 0.0317 
Train Epoch: 3 [360/816 (44%)] loss: 0.0384 L_si: 0.0088 L_grad: 0.0295 
Train Epoch: 3 [396/816 (49%)] loss: 0.0425 L_si: 0.0107 L_grad: 0.0318 
Train Epoch: 3 [432/816 (53%)] loss: 0.0365 L_si: 0.0096 L_grad: 0.0269 
Train Epoch: 3 [468/816 (57%)] loss: 0.0333 L_si: 0.0068 L_grad: 0.0265 
Train Epoch: 3 [504/816 (62%)] loss: 0.0435 L_si: 0.0120 L_grad: 0.0314 
Train Epoch: 3 [540/816 (66%)] loss: 0.0382 L_si: 0.0088 L_grad: 0.0293 
Train Epoch: 3 [576/816 (71%)] loss: 0.0490 L_si: 0.0140 L_grad: 0.0350 
Train Epoch: 3 [612/816 (75%)] loss: 0.0414 L_si: 0.0084 L_grad: 0.0330 
Train Epoch: 3 [648/816 (79%)] loss: 0.0465 L_si: 0.0140 L_grad: 0.0325 
Train Epoch: 3 [684/816 (84%)] loss: 0.0404 L_si: 0.0094 L_grad: 0.0310 
Train Epoch: 3 [720/816 (88%)] loss: 0.0302 L_si: 0.0044 L_grad: 0.0258 
Train Epoch: 3 [756/816 (93%)] loss: 0.0460 L_si: 0.0176 L_grad: 0.0284 
Train Epoch: 3 [792/816 (97%)] loss: 0.0474 L_si: 0.0152 L_grad: 0.0322 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04446516931056976, 0.04454924166202545, 0.04830848425626755, 0.04470425099134445, 0.044893279671669006, 0.045866742730140686, 0.044439733028411865, 0.046234190464019775, 0.04639321565628052, 0.04346452280879021, 0.04617816582322121, 0.04603676497936249, 0.04299550503492355, 0.04453936964273453, 0.04482597857713699, 0.045787133276462555, 0.044748999178409576, 0.0438951775431633, 0.019988304004073143], 'L_si': [0.009145975112915039, 0.010122163221240044, 0.010758230462670326, 0.009814022108912468, 0.010261496528983116, 0.01074533723294735, 0.011082606390118599, 0.011037277057766914, 0.010696031153202057, 0.010082073509693146, 0.011374160647392273, 0.01095440424978733, 0.00829927995800972, 0.0108718890696764, 0.010903695598244667, 0.012232668697834015, 0.00796864926815033, 0.011783339083194733, 0.011023948900401592], 'L_grad': [0.035319194197654724, 0.03442707657814026, 0.03755025565624237, 0.034890227019786835, 0.03463178128004074, 0.03512140363454819, 0.033357128500938416, 0.03519691526889801, 0.03569718450307846, 0.03338244929909706, 0.034804005175828934, 0.035082362592220306, 0.034696225076913834, 0.03366747871041298, 0.033922284841537476, 0.03355446457862854, 0.03678034991025925, 0.03211183845996857, 0.00896435510367155]}
Train Epoch: 4 [0/816 (0%)] loss: 0.0364 L_si: 0.0098 L_grad: 0.0267 
Train Epoch: 4 [36/816 (4%)] loss: 0.0409 L_si: 0.0136 L_grad: 0.0274 
Train Epoch: 4 [72/816 (9%)] loss: 0.0392 L_si: 0.0102 L_grad: 0.0290 
Train Epoch: 4 [108/816 (13%)] loss: 0.0414 L_si: 0.0108 L_grad: 0.0306 
Train Epoch: 4 [144/816 (18%)] loss: 0.0390 L_si: 0.0092 L_grad: 0.0298 
Train Epoch: 4 [180/816 (22%)] loss: 0.0519 L_si: 0.0179 L_grad: 0.0339 
Train Epoch: 4 [216/816 (26%)] loss: 0.0436 L_si: 0.0090 L_grad: 0.0346 
Train Epoch: 4 [252/816 (31%)] loss: 0.0424 L_si: 0.0125 L_grad: 0.0299 
Train Epoch: 4 [288/816 (35%)] loss: 0.0417 L_si: 0.0084 L_grad: 0.0332 
Train Epoch: 4 [324/816 (40%)] loss: 0.0388 L_si: 0.0073 L_grad: 0.0315 
Train Epoch: 4 [360/816 (44%)] loss: 0.0482 L_si: 0.0165 L_grad: 0.0318 
Train Epoch: 4 [396/816 (49%)] loss: 0.0369 L_si: 0.0083 L_grad: 0.0286 
Train Epoch: 4 [432/816 (53%)] loss: 0.0641 L_si: 0.0251 L_grad: 0.0390 
Train Epoch: 4 [468/816 (57%)] loss: 0.0373 L_si: 0.0081 L_grad: 0.0292 
Train Epoch: 4 [504/816 (62%)] loss: 0.0317 L_si: 0.0075 L_grad: 0.0242 
Train Epoch: 4 [540/816 (66%)] loss: 0.0401 L_si: 0.0086 L_grad: 0.0314 
Train Epoch: 4 [576/816 (71%)] loss: 0.0467 L_si: 0.0145 L_grad: 0.0322 
Train Epoch: 4 [612/816 (75%)] loss: 0.0404 L_si: 0.0108 L_grad: 0.0297 
Train Epoch: 4 [648/816 (79%)] loss: 0.0367 L_si: 0.0099 L_grad: 0.0268 
Train Epoch: 4 [684/816 (84%)] loss: 0.0341 L_si: 0.0084 L_grad: 0.0256 
Train Epoch: 4 [720/816 (88%)] loss: 0.0392 L_si: 0.0109 L_grad: 0.0284 
Train Epoch: 4 [756/816 (93%)] loss: 0.0363 L_si: 0.0078 L_grad: 0.0285 
Train Epoch: 4 [792/816 (97%)] loss: 0.0391 L_si: 0.0119 L_grad: 0.0272 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.045856181532144547, 0.04290160536766052, 0.04736270010471344, 0.04441321641206741, 0.040451399981975555, 0.039746351540088654, 0.0414421409368515, 0.04759245738387108, 0.04405360668897629, 0.04564691334962845, 0.04503095895051956, 0.040167659521102905, 0.044952355325222015, 0.0494866706430912, 0.04450537636876106, 0.04400293529033661, 0.046499043703079224, 0.04357616603374481, 0.018280068412423134], 'L_si': [0.011472247540950775, 0.009392989799380302, 0.01246463693678379, 0.011303136125206947, 0.008023649454116821, 0.008343655616044998, 0.00999421626329422, 0.011732395738363266, 0.011285455897450447, 0.011971725150942802, 0.011982938274741173, 0.009131841361522675, 0.010734433308243752, 0.01242038980126381, 0.012655191123485565, 0.011519135907292366, 0.012197239324450493, 0.009769212454557419, 0.010209470987319946], 'L_grad': [0.03438393399119377, 0.03350861370563507, 0.0348980613052845, 0.033110082149505615, 0.032427750527858734, 0.031402695924043655, 0.03144792467355728, 0.03586006164550781, 0.03276815265417099, 0.033675190061330795, 0.03304801881313324, 0.03103581629693508, 0.03421792387962341, 0.03706628084182739, 0.0318501852452755, 0.03248380124568939, 0.03430180624127388, 0.03380695357918739, 0.008070597425103188]}
Train Epoch: 5 [0/816 (0%)] loss: 0.0542 L_si: 0.0210 L_grad: 0.0332 
Train Epoch: 5 [36/816 (4%)] loss: 0.0508 L_si: 0.0207 L_grad: 0.0302 
Train Epoch: 5 [72/816 (9%)] loss: 0.0497 L_si: 0.0188 L_grad: 0.0309 
Train Epoch: 5 [108/816 (13%)] loss: 0.0470 L_si: 0.0192 L_grad: 0.0278 
Train Epoch: 5 [144/816 (18%)] loss: 0.0444 L_si: 0.0143 L_grad: 0.0301 
Train Epoch: 5 [180/816 (22%)] loss: 0.0365 L_si: 0.0074 L_grad: 0.0291 
Train Epoch: 5 [216/816 (26%)] loss: 0.0378 L_si: 0.0080 L_grad: 0.0298 
Train Epoch: 5 [252/816 (31%)] loss: 0.0371 L_si: 0.0090 L_grad: 0.0282 
Train Epoch: 5 [288/816 (35%)] loss: 0.0418 L_si: 0.0106 L_grad: 0.0312 
Train Epoch: 5 [324/816 (40%)] loss: 0.0369 L_si: 0.0082 L_grad: 0.0287 
Train Epoch: 5 [360/816 (44%)] loss: 0.0381 L_si: 0.0094 L_grad: 0.0287 
Train Epoch: 5 [396/816 (49%)] loss: 0.0422 L_si: 0.0113 L_grad: 0.0309 
Train Epoch: 5 [432/816 (53%)] loss: 0.0372 L_si: 0.0104 L_grad: 0.0269 
Train Epoch: 5 [468/816 (57%)] loss: 0.0468 L_si: 0.0150 L_grad: 0.0318 
Train Epoch: 5 [504/816 (62%)] loss: 0.0354 L_si: 0.0098 L_grad: 0.0256 
Train Epoch: 5 [540/816 (66%)] loss: 0.0431 L_si: 0.0110 L_grad: 0.0321 
Train Epoch: 5 [576/816 (71%)] loss: 0.0427 L_si: 0.0147 L_grad: 0.0280 
Train Epoch: 5 [612/816 (75%)] loss: 0.0442 L_si: 0.0122 L_grad: 0.0320 
Train Epoch: 5 [648/816 (79%)] loss: 0.0404 L_si: 0.0100 L_grad: 0.0304 
Train Epoch: 5 [684/816 (84%)] loss: 0.0371 L_si: 0.0109 L_grad: 0.0262 
Train Epoch: 5 [720/816 (88%)] loss: 0.0375 L_si: 0.0065 L_grad: 0.0310 
Train Epoch: 5 [756/816 (93%)] loss: 0.0356 L_si: 0.0079 L_grad: 0.0278 
Train Epoch: 5 [792/816 (97%)] loss: 0.0360 L_si: 0.0076 L_grad: 0.0285 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.045788466930389404, 0.046175532042980194, 0.046109072864055634, 0.04599998891353607, 0.04607521370053291, 0.04528655856847763, 0.04561528563499451, 0.045723844319581985, 0.0434175506234169, 0.046229660511016846, 0.04517759382724762, 0.04490518569946289, 0.04829399660229683, 0.04370031878352165, 0.04389283061027527, 0.04579230025410652, 0.04480896145105362, 0.04377635568380356, 0.016966352239251137], 'L_si': [0.009363828226923943, 0.009654514491558075, 0.00996861420571804, 0.008368462324142456, 0.008343886584043503, 0.008796090260148048, 0.009664805606007576, 0.010347064584493637, 0.008254686370491982, 0.01021558791399002, 0.00911514088511467, 0.00991160236299038, 0.010935455560684204, 0.008488383144140244, 0.008741969242691994, 0.010119475424289703, 0.009517950937151909, 0.00851832889020443, 0.007887296378612518], 'L_grad': [0.03642464056611061, 0.03652101755142212, 0.036140456795692444, 0.037631526589393616, 0.03773132711648941, 0.03649047017097473, 0.03595047816634178, 0.03537677973508835, 0.03516286611557007, 0.036014072597026825, 0.03606245294213295, 0.03499358147382736, 0.037358541041612625, 0.03521193563938141, 0.035150863230228424, 0.03567282482981682, 0.03529100865125656, 0.03525802493095398, 0.009079055860638618]}
Train Epoch: 6 [0/816 (0%)] loss: 0.0516 L_si: 0.0173 L_grad: 0.0343 
Train Epoch: 6 [36/816 (4%)] loss: 0.0350 L_si: 0.0065 L_grad: 0.0285 
Train Epoch: 6 [72/816 (9%)] loss: 0.0422 L_si: 0.0139 L_grad: 0.0283 
Train Epoch: 6 [108/816 (13%)] loss: 0.0463 L_si: 0.0129 L_grad: 0.0334 
Train Epoch: 6 [144/816 (18%)] loss: 0.0482 L_si: 0.0166 L_grad: 0.0317 
Train Epoch: 6 [180/816 (22%)] loss: 0.0466 L_si: 0.0148 L_grad: 0.0318 
Train Epoch: 6 [216/816 (26%)] loss: 0.0488 L_si: 0.0172 L_grad: 0.0315 
Train Epoch: 6 [252/816 (31%)] loss: 0.0317 L_si: 0.0060 L_grad: 0.0258 
Train Epoch: 6 [288/816 (35%)] loss: 0.0442 L_si: 0.0121 L_grad: 0.0321 
Train Epoch: 6 [324/816 (40%)] loss: 0.0404 L_si: 0.0096 L_grad: 0.0308 
Train Epoch: 6 [360/816 (44%)] loss: 0.0417 L_si: 0.0133 L_grad: 0.0284 
Train Epoch: 6 [396/816 (49%)] loss: 0.0338 L_si: 0.0064 L_grad: 0.0274 
Train Epoch: 6 [432/816 (53%)] loss: 0.0460 L_si: 0.0156 L_grad: 0.0304 
Train Epoch: 6 [468/816 (57%)] loss: 0.0404 L_si: 0.0100 L_grad: 0.0304 
Train Epoch: 6 [504/816 (62%)] loss: 0.0367 L_si: 0.0074 L_grad: 0.0294 
Train Epoch: 6 [540/816 (66%)] loss: 0.0353 L_si: 0.0070 L_grad: 0.0284 
Train Epoch: 6 [576/816 (71%)] loss: 0.0359 L_si: 0.0092 L_grad: 0.0267 
Train Epoch: 6 [612/816 (75%)] loss: 0.0360 L_si: 0.0079 L_grad: 0.0281 
Train Epoch: 6 [648/816 (79%)] loss: 0.0431 L_si: 0.0146 L_grad: 0.0285 
Train Epoch: 6 [684/816 (84%)] loss: 0.0433 L_si: 0.0118 L_grad: 0.0315 
Train Epoch: 6 [720/816 (88%)] loss: 0.0331 L_si: 0.0073 L_grad: 0.0258 
Train Epoch: 6 [756/816 (93%)] loss: 0.0390 L_si: 0.0111 L_grad: 0.0279 
Train Epoch: 6 [792/816 (97%)] loss: 0.0440 L_si: 0.0129 L_grad: 0.0311 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.041702788323163986, 0.04158199205994606, 0.04418661445379257, 0.042205192148685455, 0.046238791197538376, 0.04504579305648804, 0.045681923627853394, 0.04448481649160385, 0.045533232390880585, 0.04218897223472595, 0.040783610194921494, 0.04309216886758804, 0.042783528566360474, 0.04311605542898178, 0.04462157189846039, 0.04382225126028061, 0.044868916273117065, 0.044005971401929855, 0.01992129161953926], 'L_si': [0.008661169558763504, 0.009455960243940353, 0.009580632671713829, 0.00738690048456192, 0.010315876454114914, 0.00979241169989109, 0.009512918069958687, 0.009745130315423012, 0.010334877297282219, 0.009449506178498268, 0.007118731737136841, 0.009077489376068115, 0.009420948103070259, 0.00928272120654583, 0.00986073724925518, 0.009354034438729286, 0.008835749700665474, 0.009743425995111465, 0.011406540870666504], 'L_grad': [0.03304161876440048, 0.03212603181600571, 0.03460598364472389, 0.034818291664123535, 0.03592291474342346, 0.0352533794939518, 0.036169007420539856, 0.03473968803882599, 0.03519835323095322, 0.032739464193582535, 0.03366487845778465, 0.03401467949151993, 0.033362582325935364, 0.0338333360850811, 0.03476083278656006, 0.03446821868419647, 0.03603316843509674, 0.03426254540681839, 0.008514750748872757]}
Train Epoch: 7 [0/816 (0%)] loss: 0.0392 L_si: 0.0094 L_grad: 0.0298 
Train Epoch: 7 [36/816 (4%)] loss: 0.0457 L_si: 0.0185 L_grad: 0.0272 
Train Epoch: 7 [72/816 (9%)] loss: 0.0343 L_si: 0.0075 L_grad: 0.0267 
Train Epoch: 7 [108/816 (13%)] loss: 0.0408 L_si: 0.0128 L_grad: 0.0280 
Train Epoch: 7 [144/816 (18%)] loss: 0.0351 L_si: 0.0070 L_grad: 0.0281 
Train Epoch: 7 [180/816 (22%)] loss: 0.0492 L_si: 0.0171 L_grad: 0.0321 
Train Epoch: 7 [216/816 (26%)] loss: 0.0358 L_si: 0.0075 L_grad: 0.0283 
Train Epoch: 7 [252/816 (31%)] loss: 0.0376 L_si: 0.0085 L_grad: 0.0291 
Train Epoch: 7 [288/816 (35%)] loss: 0.0382 L_si: 0.0108 L_grad: 0.0274 
Train Epoch: 7 [324/816 (40%)] loss: 0.0396 L_si: 0.0096 L_grad: 0.0301 
Train Epoch: 7 [360/816 (44%)] loss: 0.0473 L_si: 0.0148 L_grad: 0.0325 
Train Epoch: 7 [396/816 (49%)] loss: 0.0434 L_si: 0.0135 L_grad: 0.0299 
Train Epoch: 7 [432/816 (53%)] loss: 0.0370 L_si: 0.0094 L_grad: 0.0276 
Train Epoch: 7 [468/816 (57%)] loss: 0.0316 L_si: 0.0057 L_grad: 0.0259 
Train Epoch: 7 [504/816 (62%)] loss: 0.0291 L_si: 0.0043 L_grad: 0.0248 
Train Epoch: 7 [540/816 (66%)] loss: 0.0343 L_si: 0.0083 L_grad: 0.0260 
Train Epoch: 7 [576/816 (71%)] loss: 0.0398 L_si: 0.0127 L_grad: 0.0270 
Train Epoch: 7 [612/816 (75%)] loss: 0.0507 L_si: 0.0176 L_grad: 0.0331 
Train Epoch: 7 [648/816 (79%)] loss: 0.0597 L_si: 0.0219 L_grad: 0.0379 
Train Epoch: 7 [684/816 (84%)] loss: 0.0397 L_si: 0.0099 L_grad: 0.0298 
Train Epoch: 7 [720/816 (88%)] loss: 0.0440 L_si: 0.0105 L_grad: 0.0335 
Train Epoch: 7 [756/816 (93%)] loss: 0.0433 L_si: 0.0139 L_grad: 0.0295 
Train Epoch: 7 [792/816 (97%)] loss: 0.0270 L_si: 0.0030 L_grad: 0.0240 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04500788822770119, 0.04658282548189163, 0.04465571418404579, 0.042336177080869675, 0.043919529765844345, 0.04517239332199097, 0.04392338544130325, 0.04242540895938873, 0.04512075334787369, 0.04677324742078781, 0.04599712789058685, 0.04826608672738075, 0.04387988522648811, 0.0437982901930809, 0.04565267637372017, 0.045848358422517776, 0.04671747237443924, 0.0441930778324604, 0.014103077352046967], 'L_si': [0.009247075766324997, 0.010271010920405388, 0.009279083460569382, 0.008529659360647202, 0.009170033037662506, 0.009955808520317078, 0.010057784616947174, 0.008009210228919983, 0.00852883793413639, 0.009637042880058289, 0.010351398959755898, 0.009526774287223816, 0.009201452136039734, 0.008338835090398788, 0.009549107402563095, 0.010363508015871048, 0.01012052409350872, 0.00943027064204216, 0.005005214363336563], 'L_grad': [0.03576081246137619, 0.03631181642413139, 0.03537663072347641, 0.03380651772022247, 0.03474949672818184, 0.03521658480167389, 0.03386560082435608, 0.03441619873046875, 0.036591917276382446, 0.03713620454072952, 0.035645727068185806, 0.03873931244015694, 0.03467843309044838, 0.035459455102682114, 0.036103568971157074, 0.03548485040664673, 0.03659694641828537, 0.03476280719041824, 0.009097862988710403]}
Train Epoch: 8 [0/816 (0%)] loss: 0.0372 L_si: 0.0091 L_grad: 0.0280 
Train Epoch: 8 [36/816 (4%)] loss: 0.0376 L_si: 0.0088 L_grad: 0.0287 
Train Epoch: 8 [72/816 (9%)] loss: 0.0296 L_si: 0.0055 L_grad: 0.0241 
Train Epoch: 8 [108/816 (13%)] loss: 0.0389 L_si: 0.0097 L_grad: 0.0293 
Train Epoch: 8 [144/816 (18%)] loss: 0.0579 L_si: 0.0233 L_grad: 0.0346 
Train Epoch: 8 [180/816 (22%)] loss: 0.0389 L_si: 0.0099 L_grad: 0.0290 
Train Epoch: 8 [216/816 (26%)] loss: 0.0377 L_si: 0.0099 L_grad: 0.0278 
Train Epoch: 8 [252/816 (31%)] loss: 0.0440 L_si: 0.0123 L_grad: 0.0317 
Train Epoch: 8 [288/816 (35%)] loss: 0.0354 L_si: 0.0099 L_grad: 0.0255 
Train Epoch: 8 [324/816 (40%)] loss: 0.0440 L_si: 0.0121 L_grad: 0.0319 
Train Epoch: 8 [360/816 (44%)] loss: 0.0380 L_si: 0.0088 L_grad: 0.0291 
Train Epoch: 8 [396/816 (49%)] loss: 0.0491 L_si: 0.0178 L_grad: 0.0312 
Train Epoch: 8 [432/816 (53%)] loss: 0.0497 L_si: 0.0179 L_grad: 0.0318 
Train Epoch: 8 [468/816 (57%)] loss: 0.0362 L_si: 0.0079 L_grad: 0.0283 
Train Epoch: 8 [504/816 (62%)] loss: 0.0486 L_si: 0.0176 L_grad: 0.0311 
Train Epoch: 8 [540/816 (66%)] loss: 0.0382 L_si: 0.0085 L_grad: 0.0297 
Train Epoch: 8 [576/816 (71%)] loss: 0.0329 L_si: 0.0068 L_grad: 0.0261 
Train Epoch: 8 [612/816 (75%)] loss: 0.0337 L_si: 0.0074 L_grad: 0.0264 
Train Epoch: 8 [648/816 (79%)] loss: 0.0362 L_si: 0.0098 L_grad: 0.0264 
Train Epoch: 8 [684/816 (84%)] loss: 0.0333 L_si: 0.0075 L_grad: 0.0258 
Train Epoch: 8 [720/816 (88%)] loss: 0.0406 L_si: 0.0089 L_grad: 0.0316 
Train Epoch: 8 [756/816 (93%)] loss: 0.0363 L_si: 0.0108 L_grad: 0.0255 
Train Epoch: 8 [792/816 (97%)] loss: 0.0296 L_si: 0.0046 L_grad: 0.0250 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04753240942955017, 0.04776468127965927, 0.043569549918174744, 0.04322227090597153, 0.04598937928676605, 0.046494632959365845, 0.04564140737056732, 0.04680430144071579, 0.04683765769004822, 0.04574820399284363, 0.046849094331264496, 0.04548194259405136, 0.045575011521577835, 0.04576237499713898, 0.045408762991428375, 0.04561275988817215, 0.04409714415669441, 0.04488567262887955, 0.019587520509958267], 'L_si': [0.010174086317420006, 0.010303910821676254, 0.009526636451482773, 0.0089154914021492, 0.009664138779044151, 0.008679820224642754, 0.010115372017025948, 0.009927265346050262, 0.01061740331351757, 0.010769970715045929, 0.009525077417492867, 0.009315187111496925, 0.010827098041772842, 0.009721513837575912, 0.010151118040084839, 0.009245006367564201, 0.009811196476221085, 0.008195571601390839, 0.011092878878116608], 'L_grad': [0.037358321249485016, 0.03746077045798302, 0.03404291346669197, 0.03430677950382233, 0.03632523864507675, 0.03781481087207794, 0.03552603721618652, 0.03687703609466553, 0.036220256239175797, 0.0349782332777977, 0.03732401877641678, 0.036166757345199585, 0.03474791347980499, 0.036040861159563065, 0.035257644951343536, 0.0363677553832531, 0.03428594768047333, 0.03669010102748871, 0.008494642563164234]}
Train Epoch: 9 [0/816 (0%)] loss: 0.0367 L_si: 0.0086 L_grad: 0.0281 
Train Epoch: 9 [36/816 (4%)] loss: 0.0364 L_si: 0.0078 L_grad: 0.0286 
Train Epoch: 9 [72/816 (9%)] loss: 0.0495 L_si: 0.0146 L_grad: 0.0348 
Train Epoch: 9 [108/816 (13%)] loss: 0.0438 L_si: 0.0142 L_grad: 0.0296 
Train Epoch: 9 [144/816 (18%)] loss: 0.0351 L_si: 0.0071 L_grad: 0.0280 
Train Epoch: 9 [180/816 (22%)] loss: 0.0557 L_si: 0.0191 L_grad: 0.0366 
Train Epoch: 9 [216/816 (26%)] loss: 0.0311 L_si: 0.0054 L_grad: 0.0257 
Train Epoch: 9 [252/816 (31%)] loss: 0.0302 L_si: 0.0046 L_grad: 0.0255 
Train Epoch: 9 [288/816 (35%)] loss: 0.0463 L_si: 0.0171 L_grad: 0.0293 
Train Epoch: 9 [324/816 (40%)] loss: 0.0401 L_si: 0.0085 L_grad: 0.0316 
Train Epoch: 9 [360/816 (44%)] loss: 0.0285 L_si: 0.0041 L_grad: 0.0245 
Train Epoch: 9 [396/816 (49%)] loss: 0.0428 L_si: 0.0127 L_grad: 0.0301 
Train Epoch: 9 [432/816 (53%)] loss: 0.0352 L_si: 0.0080 L_grad: 0.0272 
Train Epoch: 9 [468/816 (57%)] loss: 0.0304 L_si: 0.0058 L_grad: 0.0246 
Train Epoch: 9 [504/816 (62%)] loss: 0.0360 L_si: 0.0079 L_grad: 0.0281 
Train Epoch: 9 [540/816 (66%)] loss: 0.0394 L_si: 0.0109 L_grad: 0.0285 
Train Epoch: 9 [576/816 (71%)] loss: 0.0367 L_si: 0.0088 L_grad: 0.0279 
Train Epoch: 9 [612/816 (75%)] loss: 0.0366 L_si: 0.0076 L_grad: 0.0290 
Train Epoch: 9 [648/816 (79%)] loss: 0.0344 L_si: 0.0072 L_grad: 0.0272 
Train Epoch: 9 [684/816 (84%)] loss: 0.0485 L_si: 0.0153 L_grad: 0.0331 
Train Epoch: 9 [720/816 (88%)] loss: 0.0376 L_si: 0.0078 L_grad: 0.0298 
Train Epoch: 9 [756/816 (93%)] loss: 0.0323 L_si: 0.0044 L_grad: 0.0279 
Train Epoch: 9 [792/816 (97%)] loss: 0.0442 L_si: 0.0135 L_grad: 0.0308 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.04198431223630905, 0.04417160153388977, 0.04335610568523407, 0.04274147376418114, 0.042717911303043365, 0.043666236102581024, 0.041465096175670624, 0.04345349967479706, 0.04038617014884949, 0.04503289610147476, 0.04263671487569809, 0.041606876999139786, 0.042683131992816925, 0.041714102029800415, 0.04360036179423332, 0.0466097816824913, 0.043439723551273346, 0.04447127878665924, 0.016199927777051926], 'L_si': [0.007851637899875641, 0.008848899975419044, 0.00983678549528122, 0.008708875626325607, 0.00893806666135788, 0.008945286273956299, 0.008669357746839523, 0.009766623377799988, 0.007647000253200531, 0.008362315595149994, 0.00962531752884388, 0.007455889135599136, 0.008296748623251915, 0.008774086833000183, 0.008834365755319595, 0.009995818138122559, 0.00944620929658413, 0.00979524478316307, 0.00804133340716362], 'L_grad': [0.03413267433643341, 0.03532269969582558, 0.03351932018995285, 0.03403259813785553, 0.033779844641685486, 0.034720949828624725, 0.0327957384288311, 0.03368687629699707, 0.032739169895648956, 0.03667058050632477, 0.03301139548420906, 0.03415098786354065, 0.03438638150691986, 0.03294001519680023, 0.03476599603891373, 0.036613963544368744, 0.033993516117334366, 0.03467603400349617, 0.008158594369888306]}
Train Epoch: 10 [0/816 (0%)] loss: 0.0373 L_si: 0.0091 L_grad: 0.0282 
Train Epoch: 10 [36/816 (4%)] loss: 0.0482 L_si: 0.0140 L_grad: 0.0342 
Train Epoch: 10 [72/816 (9%)] loss: 0.0342 L_si: 0.0080 L_grad: 0.0262 
Train Epoch: 10 [108/816 (13%)] loss: 0.0298 L_si: 0.0048 L_grad: 0.0250 
Train Epoch: 10 [144/816 (18%)] loss: 0.0395 L_si: 0.0119 L_grad: 0.0276 
Train Epoch: 10 [180/816 (22%)] loss: 0.0387 L_si: 0.0103 L_grad: 0.0283 
Train Epoch: 10 [216/816 (26%)] loss: 0.0334 L_si: 0.0066 L_grad: 0.0268 
Train Epoch: 10 [252/816 (31%)] loss: 0.0260 L_si: 0.0030 L_grad: 0.0229 
Train Epoch: 10 [288/816 (35%)] loss: 0.0304 L_si: 0.0060 L_grad: 0.0244 
Train Epoch: 10 [324/816 (40%)] loss: 0.0345 L_si: 0.0052 L_grad: 0.0292 
Train Epoch: 10 [360/816 (44%)] loss: 0.0391 L_si: 0.0104 L_grad: 0.0287 
Train Epoch: 10 [396/816 (49%)] loss: 0.0362 L_si: 0.0085 L_grad: 0.0277 
Train Epoch: 10 [432/816 (53%)] loss: 0.0360 L_si: 0.0093 L_grad: 0.0267 
Train Epoch: 10 [468/816 (57%)] loss: 0.0472 L_si: 0.0147 L_grad: 0.0326 
Train Epoch: 10 [504/816 (62%)] loss: 0.0426 L_si: 0.0092 L_grad: 0.0333 
Train Epoch: 10 [540/816 (66%)] loss: 0.0312 L_si: 0.0052 L_grad: 0.0260 
Train Epoch: 10 [576/816 (71%)] loss: 0.0397 L_si: 0.0108 L_grad: 0.0289 
Train Epoch: 10 [612/816 (75%)] loss: 0.0466 L_si: 0.0138 L_grad: 0.0329 
Train Epoch: 10 [648/816 (79%)] loss: 0.0339 L_si: 0.0080 L_grad: 0.0259 
Train Epoch: 10 [684/816 (84%)] loss: 0.0392 L_si: 0.0136 L_grad: 0.0256 
Train Epoch: 10 [720/816 (88%)] loss: 0.0389 L_si: 0.0104 L_grad: 0.0285 
Train Epoch: 10 [756/816 (93%)] loss: 0.0393 L_si: 0.0095 L_grad: 0.0298 
Train Epoch: 10 [792/816 (97%)] loss: 0.0432 L_si: 0.0136 L_grad: 0.0296 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving checkpoint: /root/autodl-tmp/train_il_200e/train_s2d_SpikeTransformer/checkpoint-epoch010-loss-0.0385.pth.tar ...
all losses in batch in validation:  {'loss': [0.04309550300240517, 0.04250781238079071, 0.04300253093242645, 0.043385036289691925, 0.042346738278865814, 0.042154036462306976, 0.04269344359636307, 0.04379420727491379, 0.04438842087984085, 0.044129226356744766, 0.04441552981734276, 0.0438757985830307, 0.0427657887339592, 0.04371479153633118, 0.041593898087739944, 0.04648234322667122, 0.04318869486451149, 0.04284152388572693, 0.015118456445634365], 'L_si': [0.008951295167207718, 0.008891578763723373, 0.00812721997499466, 0.008740300312638283, 0.008697215467691422, 0.008265949785709381, 0.009297063574194908, 0.008375413715839386, 0.009053671732544899, 0.008708219975233078, 0.008979152888059616, 0.00859316810965538, 0.00856143981218338, 0.008709223940968513, 0.008389372378587723, 0.008644577115774155, 0.008986260741949081, 0.007798314094543457, 0.005430828779935837], 'L_grad': [0.03414420783519745, 0.03361623361706734, 0.03487531095743179, 0.03464473783969879, 0.03364952281117439, 0.033888086676597595, 0.03339638188481331, 0.0354187935590744, 0.0353347510099411, 0.03542100638151169, 0.03543637692928314, 0.03528263047337532, 0.03420434892177582, 0.03500556945800781, 0.03320452570915222, 0.037837766110897064, 0.03420243412256241, 0.03504320979118347, 0.009687627665698528]}
Train Epoch: 11 [0/816 (0%)] loss: 0.0348 L_si: 0.0068 L_grad: 0.0279 
Train Epoch: 11 [36/816 (4%)] loss: 0.0464 L_si: 0.0198 L_grad: 0.0267 
Train Epoch: 11 [72/816 (9%)] loss: 0.0417 L_si: 0.0126 L_grad: 0.0291 
Train Epoch: 11 [108/816 (13%)] loss: 0.0416 L_si: 0.0120 L_grad: 0.0296 
Train Epoch: 11 [144/816 (18%)] loss: 0.0405 L_si: 0.0100 L_grad: 0.0305 
Train Epoch: 11 [180/816 (22%)] loss: 0.0299 L_si: 0.0042 L_grad: 0.0257 
Train Epoch: 11 [216/816 (26%)] loss: 0.0348 L_si: 0.0080 L_grad: 0.0268 
Train Epoch: 11 [252/816 (31%)] loss: 0.0325 L_si: 0.0063 L_grad: 0.0263 
Train Epoch: 11 [288/816 (35%)] loss: 0.0560 L_si: 0.0191 L_grad: 0.0369 
Train Epoch: 11 [324/816 (40%)] loss: 0.0419 L_si: 0.0119 L_grad: 0.0300 
Train Epoch: 11 [360/816 (44%)] loss: 0.0287 L_si: 0.0037 L_grad: 0.0250 
Train Epoch: 11 [396/816 (49%)] loss: 0.0298 L_si: 0.0057 L_grad: 0.0241 
Train Epoch: 11 [432/816 (53%)] loss: 0.0387 L_si: 0.0119 L_grad: 0.0268 
Train Epoch: 11 [468/816 (57%)] loss: 0.0362 L_si: 0.0106 L_grad: 0.0256 
Train Epoch: 11 [504/816 (62%)] loss: 0.0428 L_si: 0.0139 L_grad: 0.0290 
Train Epoch: 11 [540/816 (66%)] loss: 0.0335 L_si: 0.0066 L_grad: 0.0269 
Train Epoch: 11 [576/816 (71%)] loss: 0.0332 L_si: 0.0049 L_grad: 0.0282 
Train Epoch: 11 [612/816 (75%)] loss: 0.0326 L_si: 0.0064 L_grad: 0.0262 
Train Epoch: 11 [648/816 (79%)] loss: 0.0407 L_si: 0.0101 L_grad: 0.0306 
Train Epoch: 11 [684/816 (84%)] loss: 0.0336 L_si: 0.0059 L_grad: 0.0276 
Train Epoch: 11 [720/816 (88%)] loss: 0.0378 L_si: 0.0100 L_grad: 0.0278 
Train Epoch: 11 [756/816 (93%)] loss: 0.0301 L_si: 0.0042 L_grad: 0.0259 
Train Epoch: 11 [792/816 (97%)] loss: 0.0302 L_si: 0.0051 L_grad: 0.0251 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.04056324437260628, 0.04217623919248581, 0.041347257792949677, 0.04321867600083351, 0.04268355295062065, 0.04456803947687149, 0.04173647612333298, 0.03744974359869957, 0.041065722703933716, 0.04386693611741066, 0.04364387318491936, 0.04247380420565605, 0.04582783579826355, 0.04138230159878731, 0.04229377955198288, 0.04138905927538872, 0.04247727617621422, 0.04303557798266411, 0.014140134677290916], 'L_si': [0.008387550711631775, 0.008314898237586021, 0.008591476827859879, 0.009152408689260483, 0.00832546129822731, 0.009069744497537613, 0.008643968030810356, 0.0052267275750637054, 0.007006464526057243, 0.008648645132780075, 0.007930934429168701, 0.009644020348787308, 0.00972600094974041, 0.008721921592950821, 0.009372325614094734, 0.008898358792066574, 0.009434964507818222, 0.00866968184709549, 0.0053304098546504974], 'L_grad': [0.0321756936609745, 0.03386134281754494, 0.0327557809650898, 0.03406626731157303, 0.03435809165239334, 0.03549829497933388, 0.03309250622987747, 0.032223016023635864, 0.03405925631523132, 0.035218290984630585, 0.035712938755750656, 0.032829783856868744, 0.03610183298587799, 0.03266038000583649, 0.032921455800533295, 0.032490700483322144, 0.033042311668395996, 0.03436589613556862, 0.008809724822640419]}
Train Epoch: 12 [0/816 (0%)] loss: 0.0401 L_si: 0.0157 L_grad: 0.0244 
Train Epoch: 12 [36/816 (4%)] loss: 0.0367 L_si: 0.0079 L_grad: 0.0287 
Train Epoch: 12 [72/816 (9%)] loss: 0.0409 L_si: 0.0105 L_grad: 0.0305 
Train Epoch: 12 [108/816 (13%)] loss: 0.0520 L_si: 0.0165 L_grad: 0.0355 
Train Epoch: 12 [144/816 (18%)] loss: 0.0402 L_si: 0.0108 L_grad: 0.0295 
Train Epoch: 12 [180/816 (22%)] loss: 0.0369 L_si: 0.0083 L_grad: 0.0286 
Train Epoch: 12 [216/816 (26%)] loss: 0.0381 L_si: 0.0069 L_grad: 0.0312 
Train Epoch: 12 [252/816 (31%)] loss: 0.0418 L_si: 0.0113 L_grad: 0.0305 
Train Epoch: 12 [288/816 (35%)] loss: 0.0292 L_si: 0.0039 L_grad: 0.0253 
Train Epoch: 12 [324/816 (40%)] loss: 0.0474 L_si: 0.0168 L_grad: 0.0306 
Train Epoch: 12 [360/816 (44%)] loss: 0.0296 L_si: 0.0069 L_grad: 0.0227 
Train Epoch: 12 [396/816 (49%)] loss: 0.0444 L_si: 0.0125 L_grad: 0.0319 
Train Epoch: 12 [432/816 (53%)] loss: 0.0394 L_si: 0.0111 L_grad: 0.0282 
Train Epoch: 12 [468/816 (57%)] loss: 0.0354 L_si: 0.0067 L_grad: 0.0287 
Train Epoch: 12 [504/816 (62%)] loss: 0.0362 L_si: 0.0084 L_grad: 0.0279 
Train Epoch: 12 [540/816 (66%)] loss: 0.0340 L_si: 0.0064 L_grad: 0.0276 
Train Epoch: 12 [576/816 (71%)] loss: 0.0356 L_si: 0.0069 L_grad: 0.0287 
Train Epoch: 12 [612/816 (75%)] loss: 0.0395 L_si: 0.0100 L_grad: 0.0295 
Train Epoch: 12 [648/816 (79%)] loss: 0.0429 L_si: 0.0152 L_grad: 0.0277 
Train Epoch: 12 [684/816 (84%)] loss: 0.0446 L_si: 0.0155 L_grad: 0.0291 
Train Epoch: 12 [720/816 (88%)] loss: 0.0465 L_si: 0.0171 L_grad: 0.0294 
Train Epoch: 12 [756/816 (93%)] loss: 0.0422 L_si: 0.0093 L_grad: 0.0329 
Train Epoch: 12 [792/816 (97%)] loss: 0.0372 L_si: 0.0075 L_grad: 0.0298 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04503569006919861, 0.043192170560359955, 0.04154292866587639, 0.04441846162080765, 0.043207526206970215, 0.0433766171336174, 0.04576952010393143, 0.04523817449808121, 0.043967895209789276, 0.04326730966567993, 0.04451274126768112, 0.042968884110450745, 0.04423828050494194, 0.04212585836648941, 0.0430930033326149, 0.04242348298430443, 0.04426669701933861, 0.04292343184351921, 0.017162589356303215], 'L_si': [0.008537160232663155, 0.008047373965382576, 0.008126321248710155, 0.009214671328663826, 0.00835120677947998, 0.007922379299998283, 0.00939987227320671, 0.00826919823884964, 0.009537247940897942, 0.008406713604927063, 0.009186498820781708, 0.009544454514980316, 0.0077649690210819244, 0.008645566180348396, 0.00861394964158535, 0.008986879140138626, 0.009162601083517075, 0.00851881131529808, 0.00826982595026493], 'L_grad': [0.0364985316991806, 0.03514479845762253, 0.03341660648584366, 0.03520379215478897, 0.034856319427490234, 0.03545423597097397, 0.036369647830724716, 0.03696897625923157, 0.034430649131536484, 0.03486059606075287, 0.035326242446899414, 0.03342442959547043, 0.036473311483860016, 0.033480290323495865, 0.0344790555536747, 0.0334366038441658, 0.03510409593582153, 0.03440462052822113, 0.008892763406038284]}
Train Epoch: 13 [0/816 (0%)] loss: 0.0469 L_si: 0.0134 L_grad: 0.0335 
Train Epoch: 13 [36/816 (4%)] loss: 0.0452 L_si: 0.0137 L_grad: 0.0315 
Train Epoch: 13 [72/816 (9%)] loss: 0.0318 L_si: 0.0046 L_grad: 0.0273 
Train Epoch: 13 [108/816 (13%)] loss: 0.0311 L_si: 0.0053 L_grad: 0.0258 
Train Epoch: 13 [144/816 (18%)] loss: 0.0551 L_si: 0.0202 L_grad: 0.0349 
Train Epoch: 13 [180/816 (22%)] loss: 0.0289 L_si: 0.0037 L_grad: 0.0252 
Train Epoch: 13 [216/816 (26%)] loss: 0.0306 L_si: 0.0050 L_grad: 0.0256 
Train Epoch: 13 [252/816 (31%)] loss: 0.0328 L_si: 0.0058 L_grad: 0.0270 
Train Epoch: 13 [288/816 (35%)] loss: 0.0376 L_si: 0.0105 L_grad: 0.0271 
Train Epoch: 13 [324/816 (40%)] loss: 0.0467 L_si: 0.0123 L_grad: 0.0345 
Train Epoch: 13 [360/816 (44%)] loss: 0.0331 L_si: 0.0059 L_grad: 0.0272 
Train Epoch: 13 [396/816 (49%)] loss: 0.0494 L_si: 0.0185 L_grad: 0.0309 
Train Epoch: 13 [432/816 (53%)] loss: 0.0420 L_si: 0.0127 L_grad: 0.0293 
Train Epoch: 13 [468/816 (57%)] loss: 0.0422 L_si: 0.0120 L_grad: 0.0302 
Train Epoch: 13 [504/816 (62%)] loss: 0.0520 L_si: 0.0181 L_grad: 0.0339 
Train Epoch: 13 [540/816 (66%)] loss: 0.0410 L_si: 0.0082 L_grad: 0.0328 
Train Epoch: 13 [576/816 (71%)] loss: 0.0278 L_si: 0.0039 L_grad: 0.0239 
Train Epoch: 13 [612/816 (75%)] loss: 0.0336 L_si: 0.0072 L_grad: 0.0264 
Train Epoch: 13 [648/816 (79%)] loss: 0.0276 L_si: 0.0039 L_grad: 0.0237 
Train Epoch: 13 [684/816 (84%)] loss: 0.0514 L_si: 0.0165 L_grad: 0.0349 
Train Epoch: 13 [720/816 (88%)] loss: 0.0378 L_si: 0.0118 L_grad: 0.0260 
Train Epoch: 13 [756/816 (93%)] loss: 0.0343 L_si: 0.0076 L_grad: 0.0267 
Train Epoch: 13 [792/816 (97%)] loss: 0.0313 L_si: 0.0058 L_grad: 0.0254 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04399031028151512, 0.043000344187021255, 0.0453694686293602, 0.043876342475414276, 0.03998629003763199, 0.04360264912247658, 0.04386129602789879, 0.0406576506793499, 0.0382193997502327, 0.04411754012107849, 0.042099758982658386, 0.04051532968878746, 0.04216715693473816, 0.04438931867480278, 0.041155748069286346, 0.04436385631561279, 0.04181332886219025, 0.04362276941537857, 0.01902935840189457], 'L_si': [0.009403511881828308, 0.009324353188276291, 0.0100722536444664, 0.010026255622506142, 0.006369132548570633, 0.009610127657651901, 0.010099507868289948, 0.0079209478572011, 0.005956567823886871, 0.009436020627617836, 0.00876876711845398, 0.0077073462307453156, 0.008599579334259033, 0.01051366701722145, 0.009025819599628448, 0.009819991886615753, 0.009903907775878906, 0.008712692186236382, 0.010106133297085762], 'L_grad': [0.03458679839968681, 0.033675990998744965, 0.0352972149848938, 0.033850088715553284, 0.033617157489061356, 0.033992521464824677, 0.03376178815960884, 0.032736703753471375, 0.032262831926345825, 0.034681521356105804, 0.03333099186420441, 0.032807983458042145, 0.033567577600479126, 0.03387565165758133, 0.0321299284696579, 0.03454386442899704, 0.03190942108631134, 0.03491007536649704, 0.008923225104808807]}
Train Epoch: 14 [0/816 (0%)] loss: 0.0524 L_si: 0.0207 L_grad: 0.0317 
Train Epoch: 14 [36/816 (4%)] loss: 0.0436 L_si: 0.0097 L_grad: 0.0339 
Train Epoch: 14 [72/816 (9%)] loss: 0.0403 L_si: 0.0124 L_grad: 0.0279 
Train Epoch: 14 [108/816 (13%)] loss: 0.0390 L_si: 0.0076 L_grad: 0.0314 
Train Epoch: 14 [144/816 (18%)] loss: 0.0487 L_si: 0.0167 L_grad: 0.0321 
Train Epoch: 14 [180/816 (22%)] loss: 0.0365 L_si: 0.0087 L_grad: 0.0278 
Train Epoch: 14 [216/816 (26%)] loss: 0.0449 L_si: 0.0162 L_grad: 0.0287 
Train Epoch: 14 [252/816 (31%)] loss: 0.0350 L_si: 0.0091 L_grad: 0.0259 
Train Epoch: 14 [288/816 (35%)] loss: 0.0388 L_si: 0.0092 L_grad: 0.0296 
Train Epoch: 14 [324/816 (40%)] loss: 0.0328 L_si: 0.0088 L_grad: 0.0239 
Train Epoch: 14 [360/816 (44%)] loss: 0.0375 L_si: 0.0093 L_grad: 0.0282 
Train Epoch: 14 [396/816 (49%)] loss: 0.0392 L_si: 0.0085 L_grad: 0.0307 
Train Epoch: 14 [432/816 (53%)] loss: 0.0368 L_si: 0.0133 L_grad: 0.0235 
Train Epoch: 14 [468/816 (57%)] loss: 0.0391 L_si: 0.0094 L_grad: 0.0297 
Train Epoch: 14 [504/816 (62%)] loss: 0.0396 L_si: 0.0102 L_grad: 0.0294 
Train Epoch: 14 [540/816 (66%)] loss: 0.0309 L_si: 0.0042 L_grad: 0.0267 
Train Epoch: 14 [576/816 (71%)] loss: 0.0402 L_si: 0.0105 L_grad: 0.0297 
Train Epoch: 14 [612/816 (75%)] loss: 0.0354 L_si: 0.0064 L_grad: 0.0290 
Train Epoch: 14 [648/816 (79%)] loss: 0.0330 L_si: 0.0052 L_grad: 0.0277 
Train Epoch: 14 [684/816 (84%)] loss: 0.0372 L_si: 0.0074 L_grad: 0.0298 
Train Epoch: 14 [720/816 (88%)] loss: 0.0250 L_si: 0.0036 L_grad: 0.0214 
Train Epoch: 14 [756/816 (93%)] loss: 0.0320 L_si: 0.0075 L_grad: 0.0245 
Train Epoch: 14 [792/816 (97%)] loss: 0.0315 L_si: 0.0046 L_grad: 0.0270 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.040168989449739456, 0.040335867553949356, 0.040848616510629654, 0.04010699689388275, 0.04077655076980591, 0.040957167744636536, 0.03980310261249542, 0.041512053459882736, 0.0413605272769928, 0.03918127343058586, 0.043393008410930634, 0.03950414061546326, 0.03988637030124664, 0.04343003034591675, 0.0427933931350708, 0.03975807875394821, 0.0427500382065773, 0.04210265725851059, 0.01452228706330061], 'L_si': [0.0075514838099479675, 0.00864475592970848, 0.00875285267829895, 0.006982428953051567, 0.007970567792654037, 0.008201710879802704, 0.0070933997631073, 0.008946452289819717, 0.00874774158000946, 0.006227899342775345, 0.008490761741995811, 0.008541788905858994, 0.007108960300683975, 0.009074287489056587, 0.008891914039850235, 0.008618293330073357, 0.00861416757106781, 0.009724479168653488, 0.005298892967402935], 'L_grad': [0.03261750563979149, 0.031691111624240875, 0.032095763832330704, 0.033124569803476334, 0.03280598297715187, 0.03275545686483383, 0.03270970284938812, 0.03256560117006302, 0.03261278569698334, 0.032953374087810516, 0.03490224480628967, 0.030962349846959114, 0.03277741000056267, 0.03435574471950531, 0.033901479095220566, 0.031139783561229706, 0.03413587063550949, 0.0323781780898571, 0.009223394095897675]}
Train Epoch: 15 [0/816 (0%)] loss: 0.0302 L_si: 0.0072 L_grad: 0.0230 
Train Epoch: 15 [36/816 (4%)] loss: 0.0356 L_si: 0.0055 L_grad: 0.0301 
Train Epoch: 15 [72/816 (9%)] loss: 0.0274 L_si: 0.0032 L_grad: 0.0242 
Train Epoch: 15 [108/816 (13%)] loss: 0.0353 L_si: 0.0082 L_grad: 0.0271 
Train Epoch: 15 [144/816 (18%)] loss: 0.0300 L_si: 0.0073 L_grad: 0.0227 
Train Epoch: 15 [180/816 (22%)] loss: 0.0412 L_si: 0.0110 L_grad: 0.0301 
Train Epoch: 15 [216/816 (26%)] loss: 0.0377 L_si: 0.0094 L_grad: 0.0282 
Train Epoch: 15 [252/816 (31%)] loss: 0.0332 L_si: 0.0046 L_grad: 0.0286 
Train Epoch: 15 [288/816 (35%)] loss: 0.0346 L_si: 0.0063 L_grad: 0.0283 
Train Epoch: 15 [324/816 (40%)] loss: 0.0401 L_si: 0.0091 L_grad: 0.0309 
Train Epoch: 15 [360/816 (44%)] loss: 0.0257 L_si: 0.0036 L_grad: 0.0221 
Train Epoch: 15 [396/816 (49%)] loss: 0.0296 L_si: 0.0054 L_grad: 0.0242 
Train Epoch: 15 [432/816 (53%)] loss: 0.0318 L_si: 0.0059 L_grad: 0.0259 
Train Epoch: 15 [468/816 (57%)] loss: 0.0421 L_si: 0.0140 L_grad: 0.0282 
Train Epoch: 15 [504/816 (62%)] loss: 0.0433 L_si: 0.0134 L_grad: 0.0300 
Train Epoch: 15 [540/816 (66%)] loss: 0.0421 L_si: 0.0112 L_grad: 0.0309 
Train Epoch: 15 [576/816 (71%)] loss: 0.0363 L_si: 0.0066 L_grad: 0.0297 
Train Epoch: 15 [612/816 (75%)] loss: 0.0359 L_si: 0.0070 L_grad: 0.0289 
Train Epoch: 15 [648/816 (79%)] loss: 0.0328 L_si: 0.0047 L_grad: 0.0281 
Train Epoch: 15 [684/816 (84%)] loss: 0.0410 L_si: 0.0136 L_grad: 0.0274 
Train Epoch: 15 [720/816 (88%)] loss: 0.0352 L_si: 0.0101 L_grad: 0.0251 
Train Epoch: 15 [756/816 (93%)] loss: 0.0401 L_si: 0.0097 L_grad: 0.0304 
Train Epoch: 15 [792/816 (97%)] loss: 0.0372 L_si: 0.0096 L_grad: 0.0275 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.0436239056289196, 0.03542446345090866, 0.04273013025522232, 0.04415775090456009, 0.04370460659265518, 0.043935712426900864, 0.044585827738046646, 0.044355303049087524, 0.04394112527370453, 0.04567805305123329, 0.04236814007163048, 0.04406517371535301, 0.04385586455464363, 0.04259387403726578, 0.043826717883348465, 0.04315183684229851, 0.04365544766187668, 0.04322388023138046, 0.0164622999727726], 'L_si': [0.009810376912355423, 0.005030954256653786, 0.009006805717945099, 0.008841078728437424, 0.008974656462669373, 0.008871372789144516, 0.009877573698759079, 0.009723290801048279, 0.009655093774199486, 0.009407978504896164, 0.009602516889572144, 0.010038133710622787, 0.010251056402921677, 0.008161060512065887, 0.009205568581819534, 0.009245790541172028, 0.009340181946754456, 0.00886494666337967, 0.00887356698513031], 'L_grad': [0.03381352871656418, 0.030393507331609726, 0.03372332453727722, 0.035316672176122665, 0.03472995012998581, 0.03506433963775635, 0.03470825403928757, 0.034632012248039246, 0.034286029636859894, 0.03627007454633713, 0.032765623182058334, 0.034027040004730225, 0.033604808151721954, 0.03443281352519989, 0.03462114930152893, 0.03390604630112648, 0.03431526571512222, 0.03435893356800079, 0.0075887334533035755]}
Train Epoch: 16 [0/816 (0%)] loss: 0.0404 L_si: 0.0102 L_grad: 0.0302 
Train Epoch: 16 [36/816 (4%)] loss: 0.0246 L_si: 0.0028 L_grad: 0.0217 
Train Epoch: 16 [72/816 (9%)] loss: 0.0419 L_si: 0.0143 L_grad: 0.0276 
Train Epoch: 16 [108/816 (13%)] loss: 0.0325 L_si: 0.0053 L_grad: 0.0271 
Train Epoch: 16 [144/816 (18%)] loss: 0.0320 L_si: 0.0064 L_grad: 0.0256 
Train Epoch: 16 [180/816 (22%)] loss: 0.0291 L_si: 0.0035 L_grad: 0.0256 
Train Epoch: 16 [216/816 (26%)] loss: 0.0351 L_si: 0.0082 L_grad: 0.0269 
Train Epoch: 16 [252/816 (31%)] loss: 0.0494 L_si: 0.0169 L_grad: 0.0324 
Train Epoch: 16 [288/816 (35%)] loss: 0.0287 L_si: 0.0048 L_grad: 0.0240 
Train Epoch: 16 [324/816 (40%)] loss: 0.0437 L_si: 0.0103 L_grad: 0.0334 
Train Epoch: 16 [360/816 (44%)] loss: 0.0381 L_si: 0.0077 L_grad: 0.0304 
Train Epoch: 16 [396/816 (49%)] loss: 0.0286 L_si: 0.0042 L_grad: 0.0244 
Train Epoch: 16 [432/816 (53%)] loss: 0.0315 L_si: 0.0054 L_grad: 0.0261 
Train Epoch: 16 [468/816 (57%)] loss: 0.0407 L_si: 0.0107 L_grad: 0.0300 
Train Epoch: 16 [504/816 (62%)] loss: 0.0406 L_si: 0.0087 L_grad: 0.0319 
Train Epoch: 16 [540/816 (66%)] loss: 0.0435 L_si: 0.0149 L_grad: 0.0286 
Train Epoch: 16 [576/816 (71%)] loss: 0.0477 L_si: 0.0171 L_grad: 0.0307 
Train Epoch: 16 [612/816 (75%)] loss: 0.0521 L_si: 0.0187 L_grad: 0.0334 
Train Epoch: 16 [648/816 (79%)] loss: 0.0252 L_si: 0.0042 L_grad: 0.0211 
Train Epoch: 16 [684/816 (84%)] loss: 0.0363 L_si: 0.0098 L_grad: 0.0265 
Train Epoch: 16 [720/816 (88%)] loss: 0.0356 L_si: 0.0059 L_grad: 0.0297 
Train Epoch: 16 [756/816 (93%)] loss: 0.0301 L_si: 0.0050 L_grad: 0.0252 
Train Epoch: 16 [792/816 (97%)] loss: 0.0310 L_si: 0.0057 L_grad: 0.0253 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04366657882928848, 0.045041773468256, 0.04720024764537811, 0.04388686642050743, 0.045386724174022675, 0.045873597264289856, 0.04307304322719574, 0.04253009706735611, 0.04396556317806244, 0.04517565295100212, 0.04517923295497894, 0.04278603196144104, 0.043256860226392746, 0.045329272747039795, 0.043712642043828964, 0.04607182741165161, 0.04603048786520958, 0.04424168914556503, 0.01625014655292034], 'L_si': [0.008294781669974327, 0.00853031873703003, 0.01009196788072586, 0.008745066821575165, 0.008349964395165443, 0.009461207315325737, 0.008547971025109291, 0.00889820046722889, 0.007985711097717285, 0.008594527840614319, 0.009363440796732903, 0.008564086630940437, 0.008485723286867142, 0.008556842803955078, 0.008731387555599213, 0.009091868996620178, 0.008783947676420212, 0.009217662736773491, 0.007489345967769623], 'L_grad': [0.035371795296669006, 0.03651145473122597, 0.03710827976465225, 0.035141799598932266, 0.03703675791621208, 0.03641238808631897, 0.0345250703394413, 0.03363189473748207, 0.035979852080345154, 0.0365811251103878, 0.03581579029560089, 0.034221943467855453, 0.034771136939525604, 0.03677242994308472, 0.03498125448822975, 0.03697995841503143, 0.03724654018878937, 0.03502402454614639, 0.008760800585150719]}
Train Epoch: 17 [0/816 (0%)] loss: 0.0351 L_si: 0.0088 L_grad: 0.0263 
Train Epoch: 17 [36/816 (4%)] loss: 0.0390 L_si: 0.0095 L_grad: 0.0295 
Train Epoch: 17 [72/816 (9%)] loss: 0.0316 L_si: 0.0044 L_grad: 0.0271 
Train Epoch: 17 [108/816 (13%)] loss: 0.0251 L_si: 0.0039 L_grad: 0.0213 
Train Epoch: 17 [144/816 (18%)] loss: 0.0403 L_si: 0.0107 L_grad: 0.0296 
Train Epoch: 17 [180/816 (22%)] loss: 0.0258 L_si: 0.0033 L_grad: 0.0225 
Train Epoch: 17 [216/816 (26%)] loss: 0.0332 L_si: 0.0066 L_grad: 0.0265 
Train Epoch: 17 [252/816 (31%)] loss: 0.0407 L_si: 0.0110 L_grad: 0.0296 
Train Epoch: 17 [288/816 (35%)] loss: 0.0436 L_si: 0.0133 L_grad: 0.0304 
Train Epoch: 17 [324/816 (40%)] loss: 0.0392 L_si: 0.0112 L_grad: 0.0280 
Train Epoch: 17 [360/816 (44%)] loss: 0.0332 L_si: 0.0078 L_grad: 0.0254 
Train Epoch: 17 [396/816 (49%)] loss: 0.0282 L_si: 0.0041 L_grad: 0.0242 
Train Epoch: 17 [432/816 (53%)] loss: 0.0331 L_si: 0.0061 L_grad: 0.0270 
Train Epoch: 17 [468/816 (57%)] loss: 0.0340 L_si: 0.0064 L_grad: 0.0276 
Train Epoch: 17 [504/816 (62%)] loss: 0.0375 L_si: 0.0086 L_grad: 0.0289 
Train Epoch: 17 [540/816 (66%)] loss: 0.0350 L_si: 0.0059 L_grad: 0.0292 
Train Epoch: 17 [576/816 (71%)] loss: 0.0478 L_si: 0.0112 L_grad: 0.0366 
Train Epoch: 17 [612/816 (75%)] loss: 0.0357 L_si: 0.0067 L_grad: 0.0291 
Train Epoch: 17 [648/816 (79%)] loss: 0.0403 L_si: 0.0100 L_grad: 0.0303 
Train Epoch: 17 [684/816 (84%)] loss: 0.0485 L_si: 0.0198 L_grad: 0.0286 
Train Epoch: 17 [720/816 (88%)] loss: 0.0301 L_si: 0.0054 L_grad: 0.0247 
Train Epoch: 17 [756/816 (93%)] loss: 0.0351 L_si: 0.0070 L_grad: 0.0281 
Train Epoch: 17 [792/816 (97%)] loss: 0.0307 L_si: 0.0064 L_grad: 0.0243 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.04098913446068764, 0.03988299146294594, 0.039567817002534866, 0.03768346086144447, 0.040607769042253494, 0.04341679438948631, 0.038822442293167114, 0.038909800350666046, 0.03970955312252045, 0.039466969668865204, 0.04246485233306885, 0.040468402206897736, 0.04043899103999138, 0.04083210602402687, 0.041678689420223236, 0.04417942836880684, 0.04219115525484085, 0.04189032316207886, 0.013088451698422432], 'L_si': [0.008125849068164825, 0.007521100342273712, 0.007803227752447128, 0.007124621421098709, 0.008125178515911102, 0.008254025131464005, 0.007406190037727356, 0.006933525204658508, 0.007594553753733635, 0.006473490968346596, 0.008076481521129608, 0.007134664803743362, 0.008195120841264725, 0.007605884224176407, 0.00877351313829422, 0.008024420589208603, 0.008241759613156319, 0.007690273225307465, 0.00447450764477253], 'L_grad': [0.03286328539252281, 0.032361891120672226, 0.03176458925008774, 0.030558839440345764, 0.03248259052634239, 0.03516276925802231, 0.03141625225543976, 0.03197627514600754, 0.03211499750614166, 0.03299348056316376, 0.03438837081193924, 0.03333373740315437, 0.032243870198726654, 0.033226221799850464, 0.032905176281929016, 0.036155007779598236, 0.03394939750432968, 0.03420004993677139, 0.008613944053649902]}
Train Epoch: 18 [0/816 (0%)] loss: 0.0295 L_si: 0.0039 L_grad: 0.0256 
Train Epoch: 18 [36/816 (4%)] loss: 0.0512 L_si: 0.0231 L_grad: 0.0281 
Train Epoch: 18 [72/816 (9%)] loss: 0.0357 L_si: 0.0106 L_grad: 0.0251 
Train Epoch: 18 [108/816 (13%)] loss: 0.0385 L_si: 0.0081 L_grad: 0.0304 
Train Epoch: 18 [144/816 (18%)] loss: 0.0524 L_si: 0.0175 L_grad: 0.0349 
Train Epoch: 18 [180/816 (22%)] loss: 0.0354 L_si: 0.0074 L_grad: 0.0280 
Train Epoch: 18 [216/816 (26%)] loss: 0.0333 L_si: 0.0065 L_grad: 0.0268 
Train Epoch: 18 [252/816 (31%)] loss: 0.0452 L_si: 0.0150 L_grad: 0.0302 
Train Epoch: 18 [288/816 (35%)] loss: 0.0407 L_si: 0.0119 L_grad: 0.0288 
Train Epoch: 18 [324/816 (40%)] loss: 0.0312 L_si: 0.0056 L_grad: 0.0257 
Train Epoch: 18 [360/816 (44%)] loss: 0.0290 L_si: 0.0042 L_grad: 0.0248 
Train Epoch: 18 [396/816 (49%)] loss: 0.0386 L_si: 0.0101 L_grad: 0.0285 
Train Epoch: 18 [432/816 (53%)] loss: 0.0302 L_si: 0.0041 L_grad: 0.0262 
Train Epoch: 18 [468/816 (57%)] loss: 0.0337 L_si: 0.0067 L_grad: 0.0270 
Train Epoch: 18 [504/816 (62%)] loss: 0.0426 L_si: 0.0104 L_grad: 0.0321 
Train Epoch: 18 [540/816 (66%)] loss: 0.0460 L_si: 0.0169 L_grad: 0.0291 
Train Epoch: 18 [576/816 (71%)] loss: 0.0400 L_si: 0.0084 L_grad: 0.0316 
Train Epoch: 18 [612/816 (75%)] loss: 0.0321 L_si: 0.0052 L_grad: 0.0270 
Train Epoch: 18 [648/816 (79%)] loss: 0.0262 L_si: 0.0041 L_grad: 0.0221 
Train Epoch: 18 [684/816 (84%)] loss: 0.0492 L_si: 0.0202 L_grad: 0.0289 
Train Epoch: 18 [720/816 (88%)] loss: 0.0294 L_si: 0.0041 L_grad: 0.0253 
Train Epoch: 18 [756/816 (93%)] loss: 0.0286 L_si: 0.0050 L_grad: 0.0237 
Train Epoch: 18 [792/816 (97%)] loss: 0.0320 L_si: 0.0055 L_grad: 0.0265 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.043051827698946, 0.04239446669816971, 0.04092586785554886, 0.03957971930503845, 0.042345430701971054, 0.041064053773880005, 0.04228763282299042, 0.04259917140007019, 0.038633327931165695, 0.03975347802042961, 0.03812780976295471, 0.04206804186105728, 0.03959878906607628, 0.04103998839855194, 0.039843060076236725, 0.042651839554309845, 0.040045030415058136, 0.040465064346790314, 0.016330517828464508], 'L_si': [0.007761623710393906, 0.009258633479475975, 0.00707392580807209, 0.007480841130018234, 0.008720386773347855, 0.007814675569534302, 0.006468154489994049, 0.007513051852583885, 0.006547506898641586, 0.007250148802995682, 0.007546953856945038, 0.00837969034910202, 0.00792950764298439, 0.008292831480503082, 0.006524888798594475, 0.008439132943749428, 0.00854896567761898, 0.007874315604567528, 0.008008653298020363], 'L_grad': [0.035290203988552094, 0.033135831356048584, 0.03385194018483162, 0.03209887817502022, 0.0336250439286232, 0.0332493782043457, 0.03581947833299637, 0.035086121410131454, 0.03208582103252411, 0.03250332921743393, 0.030580854043364525, 0.03368835151195526, 0.03166928142309189, 0.03274715691804886, 0.0333181694149971, 0.03421270474791527, 0.031496062874794006, 0.03259074687957764, 0.00832186359912157]}
Train Epoch: 19 [0/816 (0%)] loss: 0.0414 L_si: 0.0124 L_grad: 0.0290 
Train Epoch: 19 [36/816 (4%)] loss: 0.0311 L_si: 0.0048 L_grad: 0.0263 
Train Epoch: 19 [72/816 (9%)] loss: 0.0376 L_si: 0.0084 L_grad: 0.0292 
Train Epoch: 19 [108/816 (13%)] loss: 0.0391 L_si: 0.0108 L_grad: 0.0283 
Train Epoch: 19 [144/816 (18%)] loss: 0.0361 L_si: 0.0095 L_grad: 0.0266 
Train Epoch: 19 [180/816 (22%)] loss: 0.0412 L_si: 0.0116 L_grad: 0.0296 
Train Epoch: 19 [216/816 (26%)] loss: 0.0311 L_si: 0.0072 L_grad: 0.0239 
Train Epoch: 19 [252/816 (31%)] loss: 0.0491 L_si: 0.0156 L_grad: 0.0335 
Train Epoch: 19 [288/816 (35%)] loss: 0.0353 L_si: 0.0094 L_grad: 0.0259 
Train Epoch: 19 [324/816 (40%)] loss: 0.0294 L_si: 0.0037 L_grad: 0.0258 
Train Epoch: 19 [360/816 (44%)] loss: 0.0263 L_si: 0.0029 L_grad: 0.0234 
Train Epoch: 19 [396/816 (49%)] loss: 0.0268 L_si: 0.0040 L_grad: 0.0228 
Train Epoch: 19 [432/816 (53%)] loss: 0.0333 L_si: 0.0093 L_grad: 0.0240 
Train Epoch: 19 [468/816 (57%)] loss: 0.0331 L_si: 0.0070 L_grad: 0.0261 
Train Epoch: 19 [504/816 (62%)] loss: 0.0446 L_si: 0.0121 L_grad: 0.0325 
Train Epoch: 19 [540/816 (66%)] loss: 0.0320 L_si: 0.0052 L_grad: 0.0268 
Train Epoch: 19 [576/816 (71%)] loss: 0.0323 L_si: 0.0061 L_grad: 0.0262 
Train Epoch: 19 [612/816 (75%)] loss: 0.0307 L_si: 0.0052 L_grad: 0.0255 
Train Epoch: 19 [648/816 (79%)] loss: 0.0302 L_si: 0.0040 L_grad: 0.0262 
Train Epoch: 19 [684/816 (84%)] loss: 0.0338 L_si: 0.0090 L_grad: 0.0248 
Train Epoch: 19 [720/816 (88%)] loss: 0.0304 L_si: 0.0057 L_grad: 0.0247 
Train Epoch: 19 [756/816 (93%)] loss: 0.0437 L_si: 0.0112 L_grad: 0.0324 
Train Epoch: 19 [792/816 (97%)] loss: 0.0411 L_si: 0.0115 L_grad: 0.0295 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.04353863373398781, 0.03925745189189911, 0.039223745465278625, 0.0391148217022419, 0.038943178951740265, 0.04135619103908539, 0.04003402218222618, 0.039218608289957047, 0.043919920921325684, 0.03891952708363533, 0.03964090347290039, 0.041443832218647, 0.04030942544341087, 0.04063008725643158, 0.04218703880906105, 0.03798835352063179, 0.04091399908065796, 0.041330911219120026, 0.01656471937894821], 'L_si': [0.007737338542938232, 0.006982581689953804, 0.007516438141465187, 0.006992809474468231, 0.008078621700406075, 0.008035428822040558, 0.007223386317491531, 0.00793609768152237, 0.009007975459098816, 0.008018679916858673, 0.0075479838997125626, 0.007773812860250473, 0.007458984851837158, 0.0066862814128398895, 0.007637646049261093, 0.006212611682713032, 0.006285801529884338, 0.007999125868082047, 0.00765937939286232], 'L_grad': [0.035801295191049576, 0.032274868339300156, 0.03170730918645859, 0.032122012227773666, 0.03086455911397934, 0.03332076221704483, 0.03281063586473465, 0.03128251060843468, 0.03491194546222687, 0.030900847166776657, 0.03209291771054268, 0.03367001935839653, 0.032850440591573715, 0.03394380584359169, 0.03454939275979996, 0.03177574276924133, 0.03462819755077362, 0.03333178535103798, 0.008905339986085892]}
Train Epoch: 20 [0/816 (0%)] loss: 0.0265 L_si: 0.0030 L_grad: 0.0235 
Train Epoch: 20 [36/816 (4%)] loss: 0.0290 L_si: 0.0055 L_grad: 0.0235 
Train Epoch: 20 [72/816 (9%)] loss: 0.0426 L_si: 0.0104 L_grad: 0.0322 
Train Epoch: 20 [108/816 (13%)] loss: 0.0280 L_si: 0.0032 L_grad: 0.0248 
Train Epoch: 20 [144/816 (18%)] loss: 0.0357 L_si: 0.0075 L_grad: 0.0282 
Train Epoch: 20 [180/816 (22%)] loss: 0.0300 L_si: 0.0050 L_grad: 0.0250 
Train Epoch: 20 [216/816 (26%)] loss: 0.0317 L_si: 0.0050 L_grad: 0.0267 
Train Epoch: 20 [252/816 (31%)] loss: 0.0283 L_si: 0.0054 L_grad: 0.0228 
Train Epoch: 20 [288/816 (35%)] loss: 0.0443 L_si: 0.0183 L_grad: 0.0260 
Train Epoch: 20 [324/816 (40%)] loss: 0.0374 L_si: 0.0077 L_grad: 0.0296 
Train Epoch: 20 [360/816 (44%)] loss: 0.0403 L_si: 0.0130 L_grad: 0.0273 
Train Epoch: 20 [396/816 (49%)] loss: 0.0563 L_si: 0.0200 L_grad: 0.0363 
Train Epoch: 20 [432/816 (53%)] loss: 0.0351 L_si: 0.0087 L_grad: 0.0264 
Train Epoch: 20 [468/816 (57%)] loss: 0.0382 L_si: 0.0067 L_grad: 0.0315 
Train Epoch: 20 [504/816 (62%)] loss: 0.0361 L_si: 0.0063 L_grad: 0.0298 
Train Epoch: 20 [540/816 (66%)] loss: 0.0382 L_si: 0.0096 L_grad: 0.0286 
Train Epoch: 20 [576/816 (71%)] loss: 0.0375 L_si: 0.0093 L_grad: 0.0282 
Train Epoch: 20 [612/816 (75%)] loss: 0.0313 L_si: 0.0049 L_grad: 0.0264 
Train Epoch: 20 [648/816 (79%)] loss: 0.0313 L_si: 0.0069 L_grad: 0.0245 
Train Epoch: 20 [684/816 (84%)] loss: 0.0322 L_si: 0.0050 L_grad: 0.0272 
Train Epoch: 20 [720/816 (88%)] loss: 0.0315 L_si: 0.0044 L_grad: 0.0271 
Train Epoch: 20 [756/816 (93%)] loss: 0.0440 L_si: 0.0144 L_grad: 0.0297 
Train Epoch: 20 [792/816 (97%)] loss: 0.0341 L_si: 0.0055 L_grad: 0.0286 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving checkpoint: /root/autodl-tmp/train_il_200e/train_s2d_SpikeTransformer/checkpoint-epoch020-loss-0.0351.pth.tar ...
all losses in batch in validation:  {'loss': [0.041762806475162506, 0.04492545127868652, 0.041685089468955994, 0.04361095279455185, 0.0419839471578598, 0.04131021350622177, 0.04399668425321579, 0.042350638657808304, 0.04161999374628067, 0.04043859243392944, 0.04328494891524315, 0.0418415442109108, 0.043757617473602295, 0.041312336921691895, 0.04429357126355171, 0.04240889102220535, 0.04255985841155052, 0.039150286465883255, 0.011017918586730957], 'L_si': [0.009000243619084358, 0.00981283001601696, 0.008235305547714233, 0.009193606674671173, 0.009322993457317352, 0.009057216346263885, 0.008445372804999352, 0.008946981281042099, 0.007887139916419983, 0.008233318105340004, 0.00853465124964714, 0.007095877081155777, 0.007248871028423309, 0.007681479677557945, 0.008315782994031906, 0.008853476494550705, 0.007956791669130325, 0.00814690999686718, 0.002937646582722664], 'L_grad': [0.0327625647187233, 0.03511262312531471, 0.03344978392124176, 0.034417346119880676, 0.03266095370054245, 0.032252997159957886, 0.03555130958557129, 0.033403657376766205, 0.03373285382986069, 0.03220527619123459, 0.03475029766559601, 0.03474566712975502, 0.036508746445178986, 0.0336308591067791, 0.035977788269519806, 0.03355541452765465, 0.0346030667424202, 0.031003376469016075, 0.008080272004008293]}
Train Epoch: 21 [0/816 (0%)] loss: 0.0296 L_si: 0.0054 L_grad: 0.0242 
Train Epoch: 21 [36/816 (4%)] loss: 0.0353 L_si: 0.0090 L_grad: 0.0263 
Train Epoch: 21 [72/816 (9%)] loss: 0.0313 L_si: 0.0076 L_grad: 0.0237 
Train Epoch: 21 [108/816 (13%)] loss: 0.0365 L_si: 0.0102 L_grad: 0.0263 
Train Epoch: 21 [144/816 (18%)] loss: 0.0307 L_si: 0.0049 L_grad: 0.0258 
Train Epoch: 21 [180/816 (22%)] loss: 0.0413 L_si: 0.0084 L_grad: 0.0330 
Train Epoch: 21 [216/816 (26%)] loss: 0.0337 L_si: 0.0052 L_grad: 0.0285 
Train Epoch: 21 [252/816 (31%)] loss: 0.0341 L_si: 0.0079 L_grad: 0.0262 
Train Epoch: 21 [288/816 (35%)] loss: 0.0317 L_si: 0.0045 L_grad: 0.0272 
Train Epoch: 21 [324/816 (40%)] loss: 0.0293 L_si: 0.0040 L_grad: 0.0253 
Train Epoch: 21 [360/816 (44%)] loss: 0.0278 L_si: 0.0048 L_grad: 0.0230 
Train Epoch: 21 [396/816 (49%)] loss: 0.0393 L_si: 0.0119 L_grad: 0.0274 
Train Epoch: 21 [432/816 (53%)] loss: 0.0372 L_si: 0.0129 L_grad: 0.0242 
Train Epoch: 21 [468/816 (57%)] loss: 0.0469 L_si: 0.0157 L_grad: 0.0313 
Train Epoch: 21 [504/816 (62%)] loss: 0.0316 L_si: 0.0065 L_grad: 0.0251 
Train Epoch: 21 [540/816 (66%)] loss: 0.0304 L_si: 0.0041 L_grad: 0.0262 
Train Epoch: 21 [576/816 (71%)] loss: 0.0303 L_si: 0.0046 L_grad: 0.0258 
Train Epoch: 21 [612/816 (75%)] loss: 0.0334 L_si: 0.0058 L_grad: 0.0276 
Train Epoch: 21 [648/816 (79%)] loss: 0.0359 L_si: 0.0076 L_grad: 0.0283 
Train Epoch: 21 [684/816 (84%)] loss: 0.0338 L_si: 0.0068 L_grad: 0.0270 
Train Epoch: 21 [720/816 (88%)] loss: 0.0304 L_si: 0.0047 L_grad: 0.0257 
Train Epoch: 21 [756/816 (93%)] loss: 0.0413 L_si: 0.0136 L_grad: 0.0277 
Train Epoch: 21 [792/816 (97%)] loss: 0.0449 L_si: 0.0149 L_grad: 0.0300 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.03999517858028412, 0.042686138302087784, 0.038111746311187744, 0.04135213792324066, 0.04134911671280861, 0.04218457266688347, 0.04129026085138321, 0.03849107399582863, 0.0418098159134388, 0.04160873591899872, 0.04034224525094032, 0.038224995136260986, 0.041750699281692505, 0.04050081968307495, 0.04146897792816162, 0.04165986180305481, 0.04163346439599991, 0.04002506285905838, 0.016313105821609497], 'L_si': [0.007053561508655548, 0.007956881076097488, 0.007791060954332352, 0.00816684402525425, 0.008062664419412613, 0.00740940123796463, 0.008287658914923668, 0.006867699325084686, 0.008204910904169083, 0.007711879909038544, 0.006091557443141937, 0.007453812286257744, 0.008505439385771751, 0.00716538168489933, 0.008252730593085289, 0.008132347837090492, 0.008477000519633293, 0.007253749296069145, 0.006907528266310692], 'L_grad': [0.03294161707162857, 0.034729257225990295, 0.03032068721950054, 0.03318529576063156, 0.033286452293395996, 0.03477517142891884, 0.03300260007381439, 0.03162337467074394, 0.033604905009269714, 0.033896856009960175, 0.034250687807798386, 0.030771180987358093, 0.033245258033275604, 0.03333543613553047, 0.03321624547243118, 0.03352751210331917, 0.033156462013721466, 0.032771311700344086, 0.009405577555298805]}
Train Epoch: 22 [0/816 (0%)] loss: 0.0374 L_si: 0.0090 L_grad: 0.0284 
Train Epoch: 22 [36/816 (4%)] loss: 0.0351 L_si: 0.0074 L_grad: 0.0277 
Train Epoch: 22 [72/816 (9%)] loss: 0.0307 L_si: 0.0068 L_grad: 0.0239 
Train Epoch: 22 [108/816 (13%)] loss: 0.0296 L_si: 0.0053 L_grad: 0.0243 
Train Epoch: 22 [144/816 (18%)] loss: 0.0448 L_si: 0.0140 L_grad: 0.0307 
Train Epoch: 22 [180/816 (22%)] loss: 0.0337 L_si: 0.0076 L_grad: 0.0261 
Train Epoch: 22 [216/816 (26%)] loss: 0.0299 L_si: 0.0048 L_grad: 0.0252 
Train Epoch: 22 [252/816 (31%)] loss: 0.0280 L_si: 0.0036 L_grad: 0.0245 
Train Epoch: 22 [288/816 (35%)] loss: 0.0308 L_si: 0.0050 L_grad: 0.0258 
Train Epoch: 22 [324/816 (40%)] loss: 0.0475 L_si: 0.0144 L_grad: 0.0331 
Train Epoch: 22 [360/816 (44%)] loss: 0.0367 L_si: 0.0088 L_grad: 0.0278 
Train Epoch: 22 [396/816 (49%)] loss: 0.0378 L_si: 0.0089 L_grad: 0.0289 
Train Epoch: 22 [432/816 (53%)] loss: 0.0312 L_si: 0.0066 L_grad: 0.0246 
Train Epoch: 22 [468/816 (57%)] loss: 0.0348 L_si: 0.0079 L_grad: 0.0269 
Train Epoch: 22 [504/816 (62%)] loss: 0.0323 L_si: 0.0071 L_grad: 0.0252 
Train Epoch: 22 [540/816 (66%)] loss: 0.0441 L_si: 0.0107 L_grad: 0.0334 
Train Epoch: 22 [576/816 (71%)] loss: 0.0294 L_si: 0.0051 L_grad: 0.0243 
Train Epoch: 22 [612/816 (75%)] loss: 0.0380 L_si: 0.0066 L_grad: 0.0313 
Train Epoch: 22 [648/816 (79%)] loss: 0.0311 L_si: 0.0072 L_grad: 0.0239 
Train Epoch: 22 [684/816 (84%)] loss: 0.0253 L_si: 0.0037 L_grad: 0.0216 
Train Epoch: 22 [720/816 (88%)] loss: 0.0327 L_si: 0.0088 L_grad: 0.0239 
Train Epoch: 22 [756/816 (93%)] loss: 0.0346 L_si: 0.0078 L_grad: 0.0267 
Train Epoch: 22 [792/816 (97%)] loss: 0.0333 L_si: 0.0058 L_grad: 0.0275 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.0386601984500885, 0.042943648993968964, 0.043618425726890564, 0.04057186096906662, 0.03975851088762283, 0.04196855425834656, 0.04259512573480606, 0.04214605689048767, 0.04013372212648392, 0.04210670292377472, 0.04144616425037384, 0.04044084623456001, 0.041513942182064056, 0.043073635548353195, 0.044455982744693756, 0.042037300765514374, 0.042095981538295746, 0.03811829537153244, 0.01693263277411461], 'L_si': [0.00669078528881073, 0.007742404937744141, 0.008864222094416618, 0.008431009948253632, 0.007904378697276115, 0.007690064609050751, 0.008071187883615494, 0.007106304168701172, 0.007996318861842155, 0.008961230516433716, 0.006894713267683983, 0.007539588958024979, 0.008577819913625717, 0.008688364177942276, 0.007073801010847092, 0.007669417187571526, 0.007469724863767624, 0.005769155919551849, 0.008753117173910141], 'L_grad': [0.03196941316127777, 0.03520124405622482, 0.034754201769828796, 0.03214085102081299, 0.03185413032770157, 0.03427848964929581, 0.03452393785119057, 0.0350397527217865, 0.03213740140199661, 0.033145472407341, 0.03455144912004471, 0.032901257276535034, 0.03293612226843834, 0.03438527137041092, 0.037382181733846664, 0.034367885440588, 0.03462625667452812, 0.03234913945198059, 0.008179515600204468]}
Train Epoch: 23 [0/816 (0%)] loss: 0.0353 L_si: 0.0077 L_grad: 0.0276 
Train Epoch: 23 [36/816 (4%)] loss: 0.0476 L_si: 0.0188 L_grad: 0.0288 
Train Epoch: 23 [72/816 (9%)] loss: 0.0371 L_si: 0.0110 L_grad: 0.0260 
Train Epoch: 23 [108/816 (13%)] loss: 0.0356 L_si: 0.0067 L_grad: 0.0289 
Train Epoch: 23 [144/816 (18%)] loss: 0.0302 L_si: 0.0039 L_grad: 0.0263 
Train Epoch: 23 [180/816 (22%)] loss: 0.0235 L_si: 0.0032 L_grad: 0.0203 
Train Epoch: 23 [216/816 (26%)] loss: 0.0309 L_si: 0.0049 L_grad: 0.0260 
Train Epoch: 23 [252/816 (31%)] loss: 0.0334 L_si: 0.0059 L_grad: 0.0275 
Train Epoch: 23 [288/816 (35%)] loss: 0.0331 L_si: 0.0061 L_grad: 0.0270 
Train Epoch: 23 [324/816 (40%)] loss: 0.0329 L_si: 0.0053 L_grad: 0.0276 
Train Epoch: 23 [360/816 (44%)] loss: 0.0271 L_si: 0.0031 L_grad: 0.0241 
Train Epoch: 23 [396/816 (49%)] loss: 0.0285 L_si: 0.0052 L_grad: 0.0232 
Train Epoch: 23 [432/816 (53%)] loss: 0.0384 L_si: 0.0124 L_grad: 0.0260 
Train Epoch: 23 [468/816 (57%)] loss: 0.0324 L_si: 0.0061 L_grad: 0.0264 
Train Epoch: 23 [504/816 (62%)] loss: 0.0384 L_si: 0.0066 L_grad: 0.0317 
Train Epoch: 23 [540/816 (66%)] loss: 0.0291 L_si: 0.0051 L_grad: 0.0239 
Train Epoch: 23 [576/816 (71%)] loss: 0.0320 L_si: 0.0085 L_grad: 0.0235 
Train Epoch: 23 [612/816 (75%)] loss: 0.0281 L_si: 0.0037 L_grad: 0.0244 
Train Epoch: 23 [648/816 (79%)] loss: 0.0349 L_si: 0.0066 L_grad: 0.0284 
Train Epoch: 23 [684/816 (84%)] loss: 0.0473 L_si: 0.0152 L_grad: 0.0320 
Train Epoch: 23 [720/816 (88%)] loss: 0.0295 L_si: 0.0040 L_grad: 0.0255 
Train Epoch: 23 [756/816 (93%)] loss: 0.0352 L_si: 0.0063 L_grad: 0.0290 
Train Epoch: 23 [792/816 (97%)] loss: 0.0348 L_si: 0.0094 L_grad: 0.0254 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04166070744395256, 0.040840033441782, 0.04318486899137497, 0.0435040108859539, 0.03969089686870575, 0.0415935143828392, 0.043077245354652405, 0.04336056485772133, 0.04377477988600731, 0.0407649502158165, 0.043136756867170334, 0.04040932282805443, 0.04248786345124245, 0.04218409210443497, 0.04330998659133911, 0.04176289588212967, 0.04277423396706581, 0.04281821846961975, 0.015064729377627373], 'L_si': [0.009555835276842117, 0.009145401418209076, 0.00972021371126175, 0.008863940834999084, 0.009328199550509453, 0.009762272238731384, 0.01034313440322876, 0.009141653776168823, 0.009537320584058762, 0.008525552228093147, 0.00938338041305542, 0.008680056780576706, 0.009295377880334854, 0.009602073580026627, 0.00943499431014061, 0.009574970230460167, 0.008633878082036972, 0.009144609794020653, 0.007188841700553894], 'L_grad': [0.03210487216711044, 0.03169463202357292, 0.03346465528011322, 0.03464007005095482, 0.030362695455551147, 0.03183124214410782, 0.032734110951423645, 0.034218911081552505, 0.03423745930194855, 0.0322393998503685, 0.033753376454114914, 0.03172926604747772, 0.03319248557090759, 0.03258201852440834, 0.0338749922811985, 0.03218792378902435, 0.03414035588502884, 0.03367361053824425, 0.007875887677073479]}
Train Epoch: 24 [0/816 (0%)] loss: 0.0337 L_si: 0.0091 L_grad: 0.0246 
Train Epoch: 24 [36/816 (4%)] loss: 0.0295 L_si: 0.0045 L_grad: 0.0250 
Train Epoch: 24 [72/816 (9%)] loss: 0.0261 L_si: 0.0024 L_grad: 0.0237 
Train Epoch: 24 [108/816 (13%)] loss: 0.0403 L_si: 0.0114 L_grad: 0.0290 
Train Epoch: 24 [144/816 (18%)] loss: 0.0273 L_si: 0.0036 L_grad: 0.0237 
Train Epoch: 24 [180/816 (22%)] loss: 0.0419 L_si: 0.0137 L_grad: 0.0281 
Train Epoch: 24 [216/816 (26%)] loss: 0.0266 L_si: 0.0033 L_grad: 0.0233 
Train Epoch: 24 [252/816 (31%)] loss: 0.0412 L_si: 0.0091 L_grad: 0.0322 
Train Epoch: 24 [288/816 (35%)] loss: 0.0283 L_si: 0.0041 L_grad: 0.0242 
Train Epoch: 24 [324/816 (40%)] loss: 0.0316 L_si: 0.0044 L_grad: 0.0272 
Train Epoch: 24 [360/816 (44%)] loss: 0.0548 L_si: 0.0176 L_grad: 0.0372 
Train Epoch: 24 [396/816 (49%)] loss: 0.0289 L_si: 0.0054 L_grad: 0.0236 
Train Epoch: 24 [432/816 (53%)] loss: 0.0282 L_si: 0.0028 L_grad: 0.0254 
Train Epoch: 24 [468/816 (57%)] loss: 0.0376 L_si: 0.0126 L_grad: 0.0251 
Train Epoch: 24 [504/816 (62%)] loss: 0.0299 L_si: 0.0040 L_grad: 0.0259 
Train Epoch: 24 [540/816 (66%)] loss: 0.0305 L_si: 0.0043 L_grad: 0.0262 
Train Epoch: 24 [576/816 (71%)] loss: 0.0386 L_si: 0.0110 L_grad: 0.0276 
Train Epoch: 24 [612/816 (75%)] loss: 0.0334 L_si: 0.0065 L_grad: 0.0269 
Train Epoch: 24 [648/816 (79%)] loss: 0.0362 L_si: 0.0110 L_grad: 0.0252 
Train Epoch: 24 [684/816 (84%)] loss: 0.0422 L_si: 0.0139 L_grad: 0.0283 
Train Epoch: 24 [720/816 (88%)] loss: 0.0409 L_si: 0.0085 L_grad: 0.0324 
Train Epoch: 24 [756/816 (93%)] loss: 0.0389 L_si: 0.0082 L_grad: 0.0307 
Train Epoch: 24 [792/816 (97%)] loss: 0.0334 L_si: 0.0063 L_grad: 0.0271 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04288173094391823, 0.04335879907011986, 0.04362161457538605, 0.03984752669930458, 0.04172724857926369, 0.04519062116742134, 0.04527125135064125, 0.042780812829732895, 0.043261297047138214, 0.04150461405515671, 0.03976884484291077, 0.04203540459275246, 0.04115714505314827, 0.04250013455748558, 0.04126400500535965, 0.0430716909468174, 0.04210225120186806, 0.042511917650699615, 0.015552995726466179], 'L_si': [0.008498217910528183, 0.009888414293527603, 0.00948382169008255, 0.007347576320171356, 0.0084972083568573, 0.00975416973233223, 0.008638907223939896, 0.008690733462572098, 0.009574122726917267, 0.008638503029942513, 0.007117889821529388, 0.007413808256387711, 0.00832577794790268, 0.009042289108037949, 0.008376209065318108, 0.009256068617105484, 0.008943665772676468, 0.009010985493659973, 0.007405159994959831], 'L_grad': [0.034383513033390045, 0.033470384776592255, 0.0341377928853035, 0.032499950379133224, 0.03323004022240639, 0.03543645143508911, 0.036632344126701355, 0.0340900793671608, 0.03368717432022095, 0.032866112887859344, 0.03265095502138138, 0.034621596336364746, 0.03283136710524559, 0.03345784544944763, 0.03288779407739639, 0.033815622329711914, 0.03315858542919159, 0.03350093215703964, 0.008147835731506348]}
Train Epoch: 25 [0/816 (0%)] loss: 0.0308 L_si: 0.0076 L_grad: 0.0232 
Train Epoch: 25 [36/816 (4%)] loss: 0.0325 L_si: 0.0065 L_grad: 0.0260 
Train Epoch: 25 [72/816 (9%)] loss: 0.0381 L_si: 0.0091 L_grad: 0.0290 
Train Epoch: 25 [108/816 (13%)] loss: 0.0337 L_si: 0.0062 L_grad: 0.0276 
Train Epoch: 25 [144/816 (18%)] loss: 0.0321 L_si: 0.0068 L_grad: 0.0254 
Train Epoch: 25 [180/816 (22%)] loss: 0.0364 L_si: 0.0076 L_grad: 0.0288 
Train Epoch: 25 [216/816 (26%)] loss: 0.0362 L_si: 0.0084 L_grad: 0.0278 
Train Epoch: 25 [252/816 (31%)] loss: 0.0367 L_si: 0.0092 L_grad: 0.0275 
Train Epoch: 25 [288/816 (35%)] loss: 0.0352 L_si: 0.0068 L_grad: 0.0285 
Train Epoch: 25 [324/816 (40%)] loss: 0.0364 L_si: 0.0101 L_grad: 0.0263 
Train Epoch: 25 [360/816 (44%)] loss: 0.0325 L_si: 0.0047 L_grad: 0.0279 
Train Epoch: 25 [396/816 (49%)] loss: 0.0466 L_si: 0.0132 L_grad: 0.0334 
Train Epoch: 25 [432/816 (53%)] loss: 0.0487 L_si: 0.0158 L_grad: 0.0329 
Train Epoch: 25 [468/816 (57%)] loss: 0.0353 L_si: 0.0082 L_grad: 0.0271 
Train Epoch: 25 [504/816 (62%)] loss: 0.0296 L_si: 0.0056 L_grad: 0.0240 
Train Epoch: 25 [540/816 (66%)] loss: 0.0426 L_si: 0.0138 L_grad: 0.0288 
Train Epoch: 25 [576/816 (71%)] loss: 0.0323 L_si: 0.0068 L_grad: 0.0255 
Train Epoch: 25 [612/816 (75%)] loss: 0.0277 L_si: 0.0040 L_grad: 0.0237 
Train Epoch: 25 [648/816 (79%)] loss: 0.0279 L_si: 0.0037 L_grad: 0.0242 
Train Epoch: 25 [684/816 (84%)] loss: 0.0296 L_si: 0.0053 L_grad: 0.0243 
Train Epoch: 25 [720/816 (88%)] loss: 0.0264 L_si: 0.0037 L_grad: 0.0227 
Train Epoch: 25 [756/816 (93%)] loss: 0.0380 L_si: 0.0065 L_grad: 0.0316 
Train Epoch: 25 [792/816 (97%)] loss: 0.0375 L_si: 0.0079 L_grad: 0.0297 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.0410684198141098, 0.04156089946627617, 0.04231111332774162, 0.043921854346990585, 0.040040645748376846, 0.041380446404218674, 0.03870305418968201, 0.03882209211587906, 0.04029531031847, 0.039851393550634384, 0.040303222835063934, 0.043046679347753525, 0.043114908039569855, 0.03878214210271835, 0.03779903054237366, 0.04071764647960663, 0.04019363224506378, 0.04137960076332092, 0.014386550523340702], 'L_si': [0.006870059296488762, 0.007421538233757019, 0.0069345347583293915, 0.008087869733572006, 0.0077113136649131775, 0.007004458457231522, 0.0064204055815935135, 0.006297541782259941, 0.0074857138097286224, 0.007511522620916367, 0.008109832182526588, 0.008484039455652237, 0.008500779047608376, 0.007250241935253143, 0.0068953987210989, 0.007434343919157982, 0.006174016743898392, 0.00774228572845459, 0.006653308868408203], 'L_grad': [0.03419835865497589, 0.03413936123251915, 0.03537657856941223, 0.03583398461341858, 0.03232933208346367, 0.03437598794698715, 0.032282646745443344, 0.03252454847097397, 0.03280959650874138, 0.03233987092971802, 0.032193392515182495, 0.03456263989210129, 0.03461412712931633, 0.03153190016746521, 0.030903631821274757, 0.033283304423093796, 0.03401961550116539, 0.03363731503486633, 0.007733241654932499]}
Train Epoch: 26 [0/816 (0%)] loss: 0.0267 L_si: 0.0040 L_grad: 0.0226 
Train Epoch: 26 [36/816 (4%)] loss: 0.0397 L_si: 0.0148 L_grad: 0.0249 
Train Epoch: 26 [72/816 (9%)] loss: 0.0364 L_si: 0.0072 L_grad: 0.0293 
Train Epoch: 26 [108/816 (13%)] loss: 0.0375 L_si: 0.0104 L_grad: 0.0271 
Train Epoch: 26 [144/816 (18%)] loss: 0.0322 L_si: 0.0056 L_grad: 0.0265 
Train Epoch: 26 [180/816 (22%)] loss: 0.0327 L_si: 0.0059 L_grad: 0.0268 
Train Epoch: 26 [216/816 (26%)] loss: 0.0360 L_si: 0.0062 L_grad: 0.0298 
Train Epoch: 26 [252/816 (31%)] loss: 0.0475 L_si: 0.0185 L_grad: 0.0290 
Train Epoch: 26 [288/816 (35%)] loss: 0.0313 L_si: 0.0049 L_grad: 0.0264 
Train Epoch: 26 [324/816 (40%)] loss: 0.0291 L_si: 0.0046 L_grad: 0.0245 
Train Epoch: 26 [360/816 (44%)] loss: 0.0264 L_si: 0.0042 L_grad: 0.0221 
Train Epoch: 26 [396/816 (49%)] loss: 0.0381 L_si: 0.0063 L_grad: 0.0318 
Train Epoch: 26 [432/816 (53%)] loss: 0.0294 L_si: 0.0047 L_grad: 0.0248 
Train Epoch: 26 [468/816 (57%)] loss: 0.0248 L_si: 0.0026 L_grad: 0.0223 
Train Epoch: 26 [504/816 (62%)] loss: 0.0298 L_si: 0.0044 L_grad: 0.0255 
Train Epoch: 26 [540/816 (66%)] loss: 0.0371 L_si: 0.0122 L_grad: 0.0249 
Train Epoch: 26 [576/816 (71%)] loss: 0.0397 L_si: 0.0089 L_grad: 0.0308 
Train Epoch: 26 [612/816 (75%)] loss: 0.0388 L_si: 0.0057 L_grad: 0.0331 
Train Epoch: 26 [648/816 (79%)] loss: 0.0343 L_si: 0.0076 L_grad: 0.0267 
Train Epoch: 26 [684/816 (84%)] loss: 0.0412 L_si: 0.0143 L_grad: 0.0269 
Train Epoch: 26 [720/816 (88%)] loss: 0.0326 L_si: 0.0085 L_grad: 0.0240 
Train Epoch: 26 [756/816 (93%)] loss: 0.0302 L_si: 0.0048 L_grad: 0.0254 
Train Epoch: 26 [792/816 (97%)] loss: 0.0357 L_si: 0.0061 L_grad: 0.0295 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04167787730693817, 0.04064719006419182, 0.039890892803668976, 0.0388794019818306, 0.04187305271625519, 0.0423000231385231, 0.04414435476064682, 0.042148344218730927, 0.04114349186420441, 0.04055986553430557, 0.04306451603770256, 0.03956785053014755, 0.042761288583278656, 0.042818792164325714, 0.040311455726623535, 0.04228817671537399, 0.039349816739559174, 0.040879037231206894, 0.012480130419135094], 'L_si': [0.00841590203344822, 0.008464403450489044, 0.007350172847509384, 0.007264094427227974, 0.008584775030612946, 0.008813070133328438, 0.00931783951818943, 0.008867377415299416, 0.00917493924498558, 0.007947530597448349, 0.00962582603096962, 0.007887408137321472, 0.009750735014677048, 0.00921509601175785, 0.008742300793528557, 0.009129691869020462, 0.008043723180890083, 0.006889898329973221, 0.004281081259250641], 'L_grad': [0.0332619771361351, 0.032182786613702774, 0.03254071995615959, 0.03161530941724777, 0.03328827768564224, 0.033486951142549515, 0.03482651710510254, 0.03328096866607666, 0.031968552619218826, 0.032612334936857224, 0.03343869000673294, 0.03168044239282608, 0.03301055356860161, 0.03360369801521301, 0.03156915307044983, 0.03315848484635353, 0.03130609542131424, 0.03398913890123367, 0.008199049159884453]}
Train Epoch: 27 [0/816 (0%)] loss: 0.0354 L_si: 0.0083 L_grad: 0.0271 
Train Epoch: 27 [36/816 (4%)] loss: 0.0330 L_si: 0.0057 L_grad: 0.0273 
Train Epoch: 27 [72/816 (9%)] loss: 0.0247 L_si: 0.0034 L_grad: 0.0214 
Train Epoch: 27 [108/816 (13%)] loss: 0.0356 L_si: 0.0092 L_grad: 0.0263 
Train Epoch: 27 [144/816 (18%)] loss: 0.0351 L_si: 0.0088 L_grad: 0.0264 
Train Epoch: 27 [180/816 (22%)] loss: 0.0371 L_si: 0.0075 L_grad: 0.0296 
Train Epoch: 27 [216/816 (26%)] loss: 0.0341 L_si: 0.0070 L_grad: 0.0271 
Train Epoch: 27 [252/816 (31%)] loss: 0.0355 L_si: 0.0078 L_grad: 0.0276 
Train Epoch: 27 [288/816 (35%)] loss: 0.0380 L_si: 0.0073 L_grad: 0.0307 
Train Epoch: 27 [324/816 (40%)] loss: 0.0376 L_si: 0.0079 L_grad: 0.0297 
Train Epoch: 27 [360/816 (44%)] loss: 0.0365 L_si: 0.0073 L_grad: 0.0292 
Train Epoch: 27 [396/816 (49%)] loss: 0.0262 L_si: 0.0028 L_grad: 0.0234 
Train Epoch: 27 [432/816 (53%)] loss: 0.0287 L_si: 0.0036 L_grad: 0.0251 
Train Epoch: 27 [468/816 (57%)] loss: 0.0400 L_si: 0.0086 L_grad: 0.0315 
Train Epoch: 27 [504/816 (62%)] loss: 0.0292 L_si: 0.0053 L_grad: 0.0239 
Train Epoch: 27 [540/816 (66%)] loss: 0.0248 L_si: 0.0033 L_grad: 0.0215 
Train Epoch: 27 [576/816 (71%)] loss: 0.0277 L_si: 0.0030 L_grad: 0.0246 
Train Epoch: 27 [612/816 (75%)] loss: 0.0254 L_si: 0.0032 L_grad: 0.0223 
Train Epoch: 27 [648/816 (79%)] loss: 0.0370 L_si: 0.0097 L_grad: 0.0273 
Train Epoch: 27 [684/816 (84%)] loss: 0.0436 L_si: 0.0152 L_grad: 0.0284 
Train Epoch: 27 [720/816 (88%)] loss: 0.0328 L_si: 0.0063 L_grad: 0.0265 
Train Epoch: 27 [756/816 (93%)] loss: 0.0260 L_si: 0.0032 L_grad: 0.0228 
Train Epoch: 27 [792/816 (97%)] loss: 0.0288 L_si: 0.0052 L_grad: 0.0236 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04514181241393089, 0.04158112779259682, 0.04454493522644043, 0.044599197804927826, 0.043762050569057465, 0.04394186660647392, 0.04030837118625641, 0.04171895980834961, 0.04178617149591446, 0.042293645441532135, 0.04526694491505623, 0.04140344634652138, 0.04406282305717468, 0.042586617171764374, 0.044868405908346176, 0.04320889338850975, 0.0407627671957016, 0.04222235083580017, 0.016758974641561508], 'L_si': [0.010257977992296219, 0.007381521165370941, 0.009247131645679474, 0.01031263917684555, 0.010678078979253769, 0.009919863194227219, 0.009124776348471642, 0.008955838158726692, 0.008636433631181717, 0.009388955309987068, 0.009630545973777771, 0.008588481694459915, 0.009997747838497162, 0.008051075041294098, 0.009455729275941849, 0.009749680757522583, 0.00930563174188137, 0.008518539369106293, 0.008088648319244385], 'L_grad': [0.034883834421634674, 0.034199606627225876, 0.035297803580760956, 0.034286558628082275, 0.033083971589803696, 0.034022003412246704, 0.031183592975139618, 0.032763123512268066, 0.03314973786473274, 0.032904691994190216, 0.03563639894127846, 0.03281496465206146, 0.03406507521867752, 0.034535542130470276, 0.03541267663240433, 0.03345921263098717, 0.03145713359117508, 0.03370381146669388, 0.008670326322317123]}
Train Epoch: 28 [0/816 (0%)] loss: 0.0359 L_si: 0.0125 L_grad: 0.0234 
Train Epoch: 28 [36/816 (4%)] loss: 0.0388 L_si: 0.0125 L_grad: 0.0263 
Train Epoch: 28 [72/816 (9%)] loss: 0.0268 L_si: 0.0032 L_grad: 0.0235 
Train Epoch: 28 [108/816 (13%)] loss: 0.0287 L_si: 0.0050 L_grad: 0.0237 
Train Epoch: 28 [144/816 (18%)] loss: 0.0321 L_si: 0.0044 L_grad: 0.0277 
Train Epoch: 28 [180/816 (22%)] loss: 0.0378 L_si: 0.0083 L_grad: 0.0294 
Train Epoch: 28 [216/816 (26%)] loss: 0.0317 L_si: 0.0071 L_grad: 0.0246 
Train Epoch: 28 [252/816 (31%)] loss: 0.0244 L_si: 0.0029 L_grad: 0.0214 
Train Epoch: 28 [288/816 (35%)] loss: 0.0397 L_si: 0.0120 L_grad: 0.0277 
Train Epoch: 28 [324/816 (40%)] loss: 0.0325 L_si: 0.0052 L_grad: 0.0273 
Train Epoch: 28 [360/816 (44%)] loss: 0.0292 L_si: 0.0035 L_grad: 0.0257 
Train Epoch: 28 [396/816 (49%)] loss: 0.0320 L_si: 0.0059 L_grad: 0.0261 
Train Epoch: 28 [432/816 (53%)] loss: 0.0250 L_si: 0.0028 L_grad: 0.0222 
Train Epoch: 28 [468/816 (57%)] loss: 0.0316 L_si: 0.0043 L_grad: 0.0272 
Train Epoch: 28 [504/816 (62%)] loss: 0.0409 L_si: 0.0132 L_grad: 0.0277 
Train Epoch: 28 [540/816 (66%)] loss: 0.0246 L_si: 0.0032 L_grad: 0.0214 
Train Epoch: 28 [576/816 (71%)] loss: 0.0245 L_si: 0.0028 L_grad: 0.0216 
Train Epoch: 28 [612/816 (75%)] loss: 0.0346 L_si: 0.0062 L_grad: 0.0284 
Train Epoch: 28 [648/816 (79%)] loss: 0.0416 L_si: 0.0113 L_grad: 0.0303 
Train Epoch: 28 [684/816 (84%)] loss: 0.0301 L_si: 0.0033 L_grad: 0.0267 
Train Epoch: 28 [720/816 (88%)] loss: 0.0433 L_si: 0.0128 L_grad: 0.0305 
Train Epoch: 28 [756/816 (93%)] loss: 0.0348 L_si: 0.0086 L_grad: 0.0262 
Train Epoch: 28 [792/816 (97%)] loss: 0.0310 L_si: 0.0053 L_grad: 0.0256 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.042825277894735336, 0.04247620701789856, 0.044978611171245575, 0.039618298411369324, 0.04263529181480408, 0.0433574840426445, 0.0411062017083168, 0.03908505290746689, 0.041679129004478455, 0.04275466501712799, 0.04038696736097336, 0.040797535330057144, 0.04488898441195488, 0.04093121364712715, 0.041408635675907135, 0.03914899006485939, 0.04164866358041763, 0.04251885041594505, 0.016100846230983734], 'L_si': [0.008141320198774338, 0.007133997976779938, 0.007952321320772171, 0.007412480190396309, 0.007882781326770782, 0.00813215970993042, 0.007623568177223206, 0.0066833291202783585, 0.008263818919658661, 0.006591983139514923, 0.007410919293761253, 0.008146066218614578, 0.008417297154664993, 0.006917931139469147, 0.0072114914655685425, 0.007142484188079834, 0.007590033113956451, 0.007136814296245575, 0.007931675761938095], 'L_grad': [0.034683957695961, 0.03534220904111862, 0.037026289850473404, 0.032205820083618164, 0.034752510488033295, 0.03522532433271408, 0.0334826335310936, 0.03240172564983368, 0.033415310084819794, 0.03616268187761307, 0.032976046204566956, 0.032651469111442566, 0.036471687257289886, 0.034013282507658005, 0.03419714421033859, 0.032006505876779556, 0.03405863046646118, 0.03538203611969948, 0.008169170469045639]}
Train Epoch: 29 [0/816 (0%)] loss: 0.0258 L_si: 0.0038 L_grad: 0.0220 
Train Epoch: 29 [36/816 (4%)] loss: 0.0340 L_si: 0.0063 L_grad: 0.0278 
Train Epoch: 29 [72/816 (9%)] loss: 0.0270 L_si: 0.0036 L_grad: 0.0234 
Train Epoch: 29 [108/816 (13%)] loss: 0.0307 L_si: 0.0041 L_grad: 0.0266 
Train Epoch: 29 [144/816 (18%)] loss: 0.0337 L_si: 0.0073 L_grad: 0.0264 
Train Epoch: 29 [180/816 (22%)] loss: 0.0330 L_si: 0.0074 L_grad: 0.0256 
Train Epoch: 29 [216/816 (26%)] loss: 0.0369 L_si: 0.0072 L_grad: 0.0297 
Train Epoch: 29 [252/816 (31%)] loss: 0.0323 L_si: 0.0071 L_grad: 0.0252 
Train Epoch: 29 [288/816 (35%)] loss: 0.0291 L_si: 0.0058 L_grad: 0.0233 
Train Epoch: 29 [324/816 (40%)] loss: 0.0293 L_si: 0.0070 L_grad: 0.0224 
Train Epoch: 29 [360/816 (44%)] loss: 0.0280 L_si: 0.0039 L_grad: 0.0241 
Train Epoch: 29 [396/816 (49%)] loss: 0.0349 L_si: 0.0076 L_grad: 0.0274 
Train Epoch: 29 [432/816 (53%)] loss: 0.0285 L_si: 0.0053 L_grad: 0.0233 
Train Epoch: 29 [468/816 (57%)] loss: 0.0290 L_si: 0.0041 L_grad: 0.0249 
Train Epoch: 29 [504/816 (62%)] loss: 0.0304 L_si: 0.0054 L_grad: 0.0250 
Train Epoch: 29 [540/816 (66%)] loss: 0.0296 L_si: 0.0047 L_grad: 0.0248 
Train Epoch: 29 [576/816 (71%)] loss: 0.0307 L_si: 0.0048 L_grad: 0.0260 
Train Epoch: 29 [612/816 (75%)] loss: 0.0322 L_si: 0.0075 L_grad: 0.0248 
Train Epoch: 29 [648/816 (79%)] loss: 0.0354 L_si: 0.0102 L_grad: 0.0252 
Train Epoch: 29 [684/816 (84%)] loss: 0.0341 L_si: 0.0084 L_grad: 0.0257 
Train Epoch: 29 [720/816 (88%)] loss: 0.0395 L_si: 0.0098 L_grad: 0.0297 
Train Epoch: 29 [756/816 (93%)] loss: 0.0327 L_si: 0.0090 L_grad: 0.0237 
Train Epoch: 29 [792/816 (97%)] loss: 0.0336 L_si: 0.0066 L_grad: 0.0270 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04309960827231407, 0.041089773178100586, 0.04065883159637451, 0.03963891416788101, 0.04302298650145531, 0.04164482280611992, 0.0431646965444088, 0.038262300193309784, 0.04193264991044998, 0.041636303067207336, 0.04287664219737053, 0.040491990745067596, 0.041427113115787506, 0.040984995663166046, 0.04258229583501816, 0.04333193600177765, 0.04294484108686447, 0.04203334450721741, 0.015324817039072514], 'L_si': [0.008409462869167328, 0.007384099066257477, 0.007797645404934883, 0.008143087849020958, 0.007755190134048462, 0.008919823914766312, 0.008351828902959824, 0.007869057357311249, 0.008466431871056557, 0.008093258365988731, 0.008401919156312943, 0.008208602666854858, 0.008932573720812798, 0.0075914617627859116, 0.008801808580756187, 0.008394718170166016, 0.00856771320104599, 0.008618533611297607, 0.0073945000767707825], 'L_grad': [0.034690145403146744, 0.03370567411184311, 0.03286118805408478, 0.031495824456214905, 0.035267796367406845, 0.03272499889135361, 0.034812867641448975, 0.030393240973353386, 0.033466216176748276, 0.033543042838573456, 0.03447472304105759, 0.03228338807821274, 0.03249453753232956, 0.033393532037734985, 0.03378048539161682, 0.03493721783161163, 0.03437712788581848, 0.0334148108959198, 0.007930316962301731]}
Train Epoch: 30 [0/816 (0%)] loss: 0.0254 L_si: 0.0034 L_grad: 0.0219 
Train Epoch: 30 [36/816 (4%)] loss: 0.0257 L_si: 0.0044 L_grad: 0.0213 
Train Epoch: 30 [72/816 (9%)] loss: 0.0441 L_si: 0.0169 L_grad: 0.0272 
Train Epoch: 30 [108/816 (13%)] loss: 0.0237 L_si: 0.0021 L_grad: 0.0215 
Train Epoch: 30 [144/816 (18%)] loss: 0.0326 L_si: 0.0077 L_grad: 0.0250 
Train Epoch: 30 [180/816 (22%)] loss: 0.0264 L_si: 0.0039 L_grad: 0.0225 
Train Epoch: 30 [216/816 (26%)] loss: 0.0322 L_si: 0.0078 L_grad: 0.0244 
Train Epoch: 30 [252/816 (31%)] loss: 0.0330 L_si: 0.0086 L_grad: 0.0244 
Train Epoch: 30 [288/816 (35%)] loss: 0.0272 L_si: 0.0042 L_grad: 0.0230 
Train Epoch: 30 [324/816 (40%)] loss: 0.0330 L_si: 0.0063 L_grad: 0.0266 
Train Epoch: 30 [360/816 (44%)] loss: 0.0319 L_si: 0.0057 L_grad: 0.0262 
Train Epoch: 30 [396/816 (49%)] loss: 0.0387 L_si: 0.0136 L_grad: 0.0251 
Train Epoch: 30 [432/816 (53%)] loss: 0.0287 L_si: 0.0043 L_grad: 0.0244 
Train Epoch: 30 [468/816 (57%)] loss: 0.0287 L_si: 0.0040 L_grad: 0.0247 
Train Epoch: 30 [504/816 (62%)] loss: 0.0255 L_si: 0.0047 L_grad: 0.0209 
Train Epoch: 30 [540/816 (66%)] loss: 0.0271 L_si: 0.0035 L_grad: 0.0236 
Train Epoch: 30 [576/816 (71%)] loss: 0.0258 L_si: 0.0034 L_grad: 0.0224 
Train Epoch: 30 [612/816 (75%)] loss: 0.0268 L_si: 0.0050 L_grad: 0.0218 
Train Epoch: 30 [648/816 (79%)] loss: 0.0328 L_si: 0.0063 L_grad: 0.0265 
Train Epoch: 30 [684/816 (84%)] loss: 0.0267 L_si: 0.0039 L_grad: 0.0229 
Train Epoch: 30 [720/816 (88%)] loss: 0.0276 L_si: 0.0048 L_grad: 0.0228 
Train Epoch: 30 [756/816 (93%)] loss: 0.0228 L_si: 0.0020 L_grad: 0.0208 
Train Epoch: 30 [792/816 (97%)] loss: 0.0377 L_si: 0.0075 L_grad: 0.0303 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving checkpoint: /root/autodl-tmp/train_il_200e/train_s2d_SpikeTransformer/checkpoint-epoch030-loss-0.0324.pth.tar ...
all losses in batch in validation:  {'loss': [0.03950890153646469, 0.04290817305445671, 0.042066190391778946, 0.040763940662145615, 0.04147018492221832, 0.04023335129022598, 0.04010813683271408, 0.04105740040540695, 0.042433205991983414, 0.04153122752904892, 0.04047156125307083, 0.041931651532649994, 0.041699547320604324, 0.04193313792347908, 0.04158312827348709, 0.04315107315778732, 0.04216792434453964, 0.039888475090265274, 0.017988305538892746], 'L_si': [0.006452493369579315, 0.007866673171520233, 0.0071533359587192535, 0.00778580829501152, 0.00793202593922615, 0.006067192181944847, 0.007268235087394714, 0.007857255637645721, 0.008833196014165878, 0.008556246757507324, 0.007471619173884392, 0.007881633937358856, 0.00733000785112381, 0.008278761059045792, 0.008195288479328156, 0.00810915045440197, 0.00833643227815628, 0.00679359957575798, 0.009017795324325562], 'L_grad': [0.033056408166885376, 0.03504149988293648, 0.03491285443305969, 0.032978132367134094, 0.03353815898299217, 0.034166157245635986, 0.032839901745319366, 0.03320014476776123, 0.033600009977817535, 0.032974980771541595, 0.03299994021654129, 0.03405001759529114, 0.034369539469480515, 0.03365437686443329, 0.033387839794158936, 0.035041920840740204, 0.03383149206638336, 0.033094875514507294, 0.008970510214567184]}
Train Epoch: 31 [0/816 (0%)] loss: 0.0239 L_si: 0.0026 L_grad: 0.0213 
Train Epoch: 31 [36/816 (4%)] loss: 0.0338 L_si: 0.0058 L_grad: 0.0280 
Train Epoch: 31 [72/816 (9%)] loss: 0.0297 L_si: 0.0039 L_grad: 0.0258 
Train Epoch: 31 [108/816 (13%)] loss: 0.0267 L_si: 0.0039 L_grad: 0.0228 
Train Epoch: 31 [144/816 (18%)] loss: 0.0320 L_si: 0.0061 L_grad: 0.0259 
Train Epoch: 31 [180/816 (22%)] loss: 0.0313 L_si: 0.0038 L_grad: 0.0275 
Train Epoch: 31 [216/816 (26%)] loss: 0.0344 L_si: 0.0068 L_grad: 0.0276 
Train Epoch: 31 [252/816 (31%)] loss: 0.0275 L_si: 0.0036 L_grad: 0.0239 
Train Epoch: 31 [288/816 (35%)] loss: 0.0426 L_si: 0.0118 L_grad: 0.0308 
Train Epoch: 31 [324/816 (40%)] loss: 0.0425 L_si: 0.0124 L_grad: 0.0301 
Train Epoch: 31 [360/816 (44%)] loss: 0.0330 L_si: 0.0073 L_grad: 0.0257 
Train Epoch: 31 [396/816 (49%)] loss: 0.0351 L_si: 0.0093 L_grad: 0.0258 
Train Epoch: 31 [432/816 (53%)] loss: 0.0272 L_si: 0.0056 L_grad: 0.0217 
Train Epoch: 31 [468/816 (57%)] loss: 0.0321 L_si: 0.0059 L_grad: 0.0263 
Train Epoch: 31 [504/816 (62%)] loss: 0.0314 L_si: 0.0057 L_grad: 0.0256 
Train Epoch: 31 [540/816 (66%)] loss: 0.0307 L_si: 0.0058 L_grad: 0.0249 
Train Epoch: 31 [576/816 (71%)] loss: 0.0316 L_si: 0.0059 L_grad: 0.0258 
Train Epoch: 31 [612/816 (75%)] loss: 0.0261 L_si: 0.0040 L_grad: 0.0221 
Train Epoch: 31 [648/816 (79%)] loss: 0.0350 L_si: 0.0084 L_grad: 0.0266 
Train Epoch: 31 [684/816 (84%)] loss: 0.0253 L_si: 0.0033 L_grad: 0.0219 
Train Epoch: 31 [720/816 (88%)] loss: 0.0456 L_si: 0.0095 L_grad: 0.0361 
Train Epoch: 31 [756/816 (93%)] loss: 0.0322 L_si: 0.0071 L_grad: 0.0251 
Train Epoch: 31 [792/816 (97%)] loss: 0.0334 L_si: 0.0063 L_grad: 0.0271 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.045687466859817505, 0.04064008593559265, 0.04825415834784508, 0.043502822518348694, 0.039813440293073654, 0.04346087574958801, 0.04614301025867462, 0.043976109474897385, 0.04255268722772598, 0.040956661105155945, 0.041703637689352036, 0.0457923598587513, 0.04313956946134567, 0.04123283922672272, 0.04481599107384682, 0.047865692526102066, 0.04082385450601578, 0.04215697944164276, 0.013455955311655998], 'L_si': [0.009901504963636398, 0.007531600072979927, 0.009262677282094955, 0.00925075076520443, 0.007742639631032944, 0.009431153535842896, 0.009284665808081627, 0.009096894413232803, 0.008573673665523529, 0.008292732760310173, 0.00898471474647522, 0.008947234600782394, 0.009086262434720993, 0.00863429345190525, 0.009344752877950668, 0.011211995035409927, 0.008551552891731262, 0.007681161165237427, 0.005792144685983658], 'L_grad': [0.03578596189618111, 0.033108483999967575, 0.03899148106575012, 0.034252069890499115, 0.03207080066204071, 0.03402972221374512, 0.036858346313238144, 0.03487921506166458, 0.033979013562202454, 0.03266392648220062, 0.032718922942876816, 0.0368451252579689, 0.03405330702662468, 0.032598547637462616, 0.03547123819589615, 0.03665369749069214, 0.032272301614284515, 0.034475818276405334, 0.00766381062567234]}
Train Epoch: 32 [0/816 (0%)] loss: 0.0354 L_si: 0.0056 L_grad: 0.0298 
Train Epoch: 32 [36/816 (4%)] loss: 0.0340 L_si: 0.0068 L_grad: 0.0272 
Train Epoch: 32 [72/816 (9%)] loss: 0.0271 L_si: 0.0048 L_grad: 0.0223 
Train Epoch: 32 [108/816 (13%)] loss: 0.0275 L_si: 0.0039 L_grad: 0.0236 
Train Epoch: 32 [144/816 (18%)] loss: 0.0395 L_si: 0.0085 L_grad: 0.0309 
Train Epoch: 32 [180/816 (22%)] loss: 0.0317 L_si: 0.0079 L_grad: 0.0238 
Train Epoch: 32 [216/816 (26%)] loss: 0.0371 L_si: 0.0107 L_grad: 0.0264 
Train Epoch: 32 [252/816 (31%)] loss: 0.0352 L_si: 0.0061 L_grad: 0.0291 
Train Epoch: 32 [288/816 (35%)] loss: 0.0323 L_si: 0.0055 L_grad: 0.0268 
Train Epoch: 32 [324/816 (40%)] loss: 0.0267 L_si: 0.0040 L_grad: 0.0227 
Train Epoch: 32 [360/816 (44%)] loss: 0.0341 L_si: 0.0066 L_grad: 0.0275 
Train Epoch: 32 [396/816 (49%)] loss: 0.0284 L_si: 0.0039 L_grad: 0.0245 
Train Epoch: 32 [432/816 (53%)] loss: 0.0307 L_si: 0.0062 L_grad: 0.0245 
Train Epoch: 32 [468/816 (57%)] loss: 0.0352 L_si: 0.0061 L_grad: 0.0292 
Train Epoch: 32 [504/816 (62%)] loss: 0.0285 L_si: 0.0040 L_grad: 0.0245 
Train Epoch: 32 [540/816 (66%)] loss: 0.0397 L_si: 0.0096 L_grad: 0.0301 
Train Epoch: 32 [576/816 (71%)] loss: 0.0376 L_si: 0.0098 L_grad: 0.0278 
Train Epoch: 32 [612/816 (75%)] loss: 0.0258 L_si: 0.0029 L_grad: 0.0229 
Train Epoch: 32 [648/816 (79%)] loss: 0.0326 L_si: 0.0040 L_grad: 0.0287 
Train Epoch: 32 [684/816 (84%)] loss: 0.0370 L_si: 0.0128 L_grad: 0.0242 
Train Epoch: 32 [720/816 (88%)] loss: 0.0268 L_si: 0.0031 L_grad: 0.0237 
Train Epoch: 32 [756/816 (93%)] loss: 0.0448 L_si: 0.0170 L_grad: 0.0278 
Train Epoch: 32 [792/816 (97%)] loss: 0.0355 L_si: 0.0083 L_grad: 0.0272 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04268890619277954, 0.044724419713020325, 0.043414101004600525, 0.04356160759925842, 0.04298188537359238, 0.04072176665067673, 0.04367663338780403, 0.039868079125881195, 0.04720015078783035, 0.0402606762945652, 0.042922891676425934, 0.04333295300602913, 0.04371599480509758, 0.04314364492893219, 0.04372298717498779, 0.04181179776787758, 0.04588979110121727, 0.042369723320007324, 0.014302033931016922], 'L_si': [0.008819347247481346, 0.009613769128918648, 0.00796433538198471, 0.009121453389525414, 0.008941808715462685, 0.0067548323422670364, 0.008398432284593582, 0.007266877219080925, 0.009714445099234581, 0.007152911275625229, 0.008396292105317116, 0.008394178003072739, 0.008861154317855835, 0.008861346170306206, 0.009424781426787376, 0.006998289376497269, 0.009341560304164886, 0.008065711706876755, 0.006365498527884483], 'L_grad': [0.033869560807943344, 0.035110652446746826, 0.035449765622615814, 0.03444015234708786, 0.03404007852077484, 0.03396693244576454, 0.03527820110321045, 0.03260120376944542, 0.03748570382595062, 0.03310776501893997, 0.03452659770846367, 0.03493877500295639, 0.034854840487241745, 0.034282296895980835, 0.034298207610845566, 0.03481350839138031, 0.03654823079705238, 0.03430401161313057, 0.007936535403132439]}
Train Epoch: 33 [0/816 (0%)] loss: 0.0295 L_si: 0.0059 L_grad: 0.0236 
Train Epoch: 33 [36/816 (4%)] loss: 0.0236 L_si: 0.0021 L_grad: 0.0215 
Train Epoch: 33 [72/816 (9%)] loss: 0.0298 L_si: 0.0040 L_grad: 0.0258 
Train Epoch: 33 [108/816 (13%)] loss: 0.0313 L_si: 0.0054 L_grad: 0.0259 
Train Epoch: 33 [144/816 (18%)] loss: 0.0248 L_si: 0.0033 L_grad: 0.0215 
Train Epoch: 33 [180/816 (22%)] loss: 0.0335 L_si: 0.0068 L_grad: 0.0267 
Train Epoch: 33 [216/816 (26%)] loss: 0.0331 L_si: 0.0057 L_grad: 0.0273 
Train Epoch: 33 [252/816 (31%)] loss: 0.0341 L_si: 0.0075 L_grad: 0.0266 
Train Epoch: 33 [288/816 (35%)] loss: 0.0299 L_si: 0.0045 L_grad: 0.0254 
Train Epoch: 33 [324/816 (40%)] loss: 0.0434 L_si: 0.0105 L_grad: 0.0329 
Train Epoch: 33 [360/816 (44%)] loss: 0.0436 L_si: 0.0136 L_grad: 0.0300 
Train Epoch: 33 [396/816 (49%)] loss: 0.0287 L_si: 0.0059 L_grad: 0.0229 
Train Epoch: 33 [432/816 (53%)] loss: 0.0373 L_si: 0.0086 L_grad: 0.0288 
Train Epoch: 33 [468/816 (57%)] loss: 0.0344 L_si: 0.0076 L_grad: 0.0268 
Train Epoch: 33 [504/816 (62%)] loss: 0.0310 L_si: 0.0062 L_grad: 0.0248 
Train Epoch: 33 [540/816 (66%)] loss: 0.0361 L_si: 0.0095 L_grad: 0.0266 
Train Epoch: 33 [576/816 (71%)] loss: 0.0339 L_si: 0.0065 L_grad: 0.0274 
Train Epoch: 33 [612/816 (75%)] loss: 0.0380 L_si: 0.0122 L_grad: 0.0258 
Train Epoch: 33 [648/816 (79%)] loss: 0.0316 L_si: 0.0053 L_grad: 0.0263 
Train Epoch: 33 [684/816 (84%)] loss: 0.0281 L_si: 0.0034 L_grad: 0.0247 
Train Epoch: 33 [720/816 (88%)] loss: 0.0246 L_si: 0.0029 L_grad: 0.0216 
Train Epoch: 33 [756/816 (93%)] loss: 0.0385 L_si: 0.0097 L_grad: 0.0288 
Train Epoch: 33 [792/816 (97%)] loss: 0.0347 L_si: 0.0065 L_grad: 0.0282 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.0406707301735878, 0.03992690518498421, 0.0414259098470211, 0.039771389216184616, 0.04194227606058121, 0.03864086791872978, 0.043061476200819016, 0.040234386920928955, 0.04162899777293205, 0.03676813840866089, 0.03646332025527954, 0.03824217617511749, 0.04160242900252342, 0.04147786647081375, 0.042507972568273544, 0.03996068984270096, 0.03680974245071411, 0.04109378159046173, 0.021830540150403976], 'L_si': [0.008159443736076355, 0.007073063403367996, 0.008272971957921982, 0.008206803351640701, 0.00822458602488041, 0.005970638245344162, 0.008580777794122696, 0.0075608715415000916, 0.008106749504804611, 0.007261006161570549, 0.006752051413059235, 0.007089544087648392, 0.007712487131357193, 0.008662914857268333, 0.0076739974319934845, 0.007345877587795258, 0.005691120401024818, 0.007714349776506424, 0.011667042970657349], 'L_grad': [0.032511286437511444, 0.03285384178161621, 0.03315293788909912, 0.031564585864543915, 0.03371768817305565, 0.03267022967338562, 0.03448069840669632, 0.032673515379428864, 0.03352224826812744, 0.02950713410973549, 0.029711266979575157, 0.0311526320874691, 0.03388994187116623, 0.03281494975090027, 0.03483397513628006, 0.0326148122549057, 0.031118620187044144, 0.03337943181395531, 0.010163497179746628]}
Train Epoch: 34 [0/816 (0%)] loss: 0.0361 L_si: 0.0082 L_grad: 0.0279 
Train Epoch: 34 [36/816 (4%)] loss: 0.0244 L_si: 0.0039 L_grad: 0.0205 
Train Epoch: 34 [72/816 (9%)] loss: 0.0256 L_si: 0.0039 L_grad: 0.0217 
Train Epoch: 34 [108/816 (13%)] loss: 0.0284 L_si: 0.0059 L_grad: 0.0225 
Train Epoch: 34 [144/816 (18%)] loss: 0.0353 L_si: 0.0062 L_grad: 0.0291 
Train Epoch: 34 [180/816 (22%)] loss: 0.0274 L_si: 0.0041 L_grad: 0.0233 
Train Epoch: 34 [216/816 (26%)] loss: 0.0282 L_si: 0.0040 L_grad: 0.0241 
Train Epoch: 34 [252/816 (31%)] loss: 0.0308 L_si: 0.0041 L_grad: 0.0267 
Train Epoch: 34 [288/816 (35%)] loss: 0.0435 L_si: 0.0130 L_grad: 0.0306 
Train Epoch: 34 [324/816 (40%)] loss: 0.0345 L_si: 0.0087 L_grad: 0.0258 
Train Epoch: 34 [360/816 (44%)] loss: 0.0309 L_si: 0.0061 L_grad: 0.0248 
Train Epoch: 34 [396/816 (49%)] loss: 0.0303 L_si: 0.0046 L_grad: 0.0257 
Train Epoch: 34 [432/816 (53%)] loss: 0.0388 L_si: 0.0114 L_grad: 0.0273 
Train Epoch: 34 [468/816 (57%)] loss: 0.0277 L_si: 0.0053 L_grad: 0.0224 
Train Epoch: 34 [504/816 (62%)] loss: 0.0342 L_si: 0.0069 L_grad: 0.0274 
Train Epoch: 34 [540/816 (66%)] loss: 0.0282 L_si: 0.0042 L_grad: 0.0240 
Train Epoch: 34 [576/816 (71%)] loss: 0.0329 L_si: 0.0096 L_grad: 0.0234 
Train Epoch: 34 [612/816 (75%)] loss: 0.0315 L_si: 0.0073 L_grad: 0.0242 
Train Epoch: 34 [648/816 (79%)] loss: 0.0343 L_si: 0.0082 L_grad: 0.0260 
Train Epoch: 34 [684/816 (84%)] loss: 0.0344 L_si: 0.0086 L_grad: 0.0258 
Train Epoch: 34 [720/816 (88%)] loss: 0.0376 L_si: 0.0099 L_grad: 0.0277 
Train Epoch: 34 [756/816 (93%)] loss: 0.0385 L_si: 0.0116 L_grad: 0.0269 
Train Epoch: 34 [792/816 (97%)] loss: 0.0334 L_si: 0.0054 L_grad: 0.0280 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.044783271849155426, 0.04167559742927551, 0.04227742180228233, 0.04160503298044205, 0.04357633367180824, 0.04496634006500244, 0.0421423614025116, 0.04247887060046196, 0.03978145495057106, 0.044557731598615646, 0.04172033816576004, 0.044282540678977966, 0.04312776029109955, 0.03954491764307022, 0.03986850380897522, 0.04412084072828293, 0.04307270050048828, 0.04281237721443176, 0.00890553928911686], 'L_si': [0.009258078411221504, 0.007680991664528847, 0.00801040232181549, 0.008405076339840889, 0.007894579321146011, 0.010369116440415382, 0.008129829540848732, 0.007080603390932083, 0.006747763603925705, 0.007449936121702194, 0.00777292437851429, 0.009411275386810303, 0.009008942171931267, 0.006883908063173294, 0.008399873971939087, 0.008493831381201744, 0.007383627817034721, 0.008997179567813873, 0.0018370077013969421], 'L_grad': [0.03552519530057907, 0.033994607627391815, 0.03426701948046684, 0.03319995850324631, 0.03568175435066223, 0.03459722548723221, 0.034012529999017715, 0.03539826720952988, 0.033033691346645355, 0.03710779547691345, 0.0339474156498909, 0.034871265292167664, 0.03411881625652313, 0.03266100957989693, 0.03146862983703613, 0.035627007484436035, 0.03568907082080841, 0.03381519764661789, 0.007068531587719917]}
Train Epoch: 35 [0/816 (0%)] loss: 0.0319 L_si: 0.0045 L_grad: 0.0274 
Train Epoch: 35 [36/816 (4%)] loss: 0.0379 L_si: 0.0080 L_grad: 0.0299 
Train Epoch: 35 [72/816 (9%)] loss: 0.0383 L_si: 0.0111 L_grad: 0.0272 
Train Epoch: 35 [108/816 (13%)] loss: 0.0272 L_si: 0.0042 L_grad: 0.0231 
Train Epoch: 35 [144/816 (18%)] loss: 0.0379 L_si: 0.0063 L_grad: 0.0316 
Train Epoch: 35 [180/816 (22%)] loss: 0.0262 L_si: 0.0028 L_grad: 0.0233 
Train Epoch: 35 [216/816 (26%)] loss: 0.0492 L_si: 0.0150 L_grad: 0.0341 
Train Epoch: 35 [252/816 (31%)] loss: 0.0339 L_si: 0.0085 L_grad: 0.0254 
Train Epoch: 35 [288/816 (35%)] loss: 0.0227 L_si: 0.0026 L_grad: 0.0200 
Train Epoch: 35 [324/816 (40%)] loss: 0.0383 L_si: 0.0081 L_grad: 0.0302 
Train Epoch: 35 [360/816 (44%)] loss: 0.0335 L_si: 0.0058 L_grad: 0.0278 
Train Epoch: 35 [396/816 (49%)] loss: 0.0341 L_si: 0.0053 L_grad: 0.0288 
Train Epoch: 35 [432/816 (53%)] loss: 0.0319 L_si: 0.0049 L_grad: 0.0270 
Train Epoch: 35 [468/816 (57%)] loss: 0.0347 L_si: 0.0071 L_grad: 0.0276 
Train Epoch: 35 [504/816 (62%)] loss: 0.0401 L_si: 0.0129 L_grad: 0.0272 
Train Epoch: 35 [540/816 (66%)] loss: 0.0262 L_si: 0.0027 L_grad: 0.0235 
Train Epoch: 35 [576/816 (71%)] loss: 0.0270 L_si: 0.0047 L_grad: 0.0222 
Train Epoch: 35 [612/816 (75%)] loss: 0.0259 L_si: 0.0031 L_grad: 0.0228 
Train Epoch: 35 [648/816 (79%)] loss: 0.0404 L_si: 0.0139 L_grad: 0.0265 
Train Epoch: 35 [684/816 (84%)] loss: 0.0354 L_si: 0.0069 L_grad: 0.0285 
Train Epoch: 35 [720/816 (88%)] loss: 0.0238 L_si: 0.0025 L_grad: 0.0213 
Train Epoch: 35 [756/816 (93%)] loss: 0.0254 L_si: 0.0025 L_grad: 0.0229 
Train Epoch: 35 [792/816 (97%)] loss: 0.0335 L_si: 0.0070 L_grad: 0.0265 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04174373298883438, 0.04199033975601196, 0.041810572147369385, 0.04069635272026062, 0.04066713899374008, 0.04237394779920578, 0.04430050775408745, 0.041284624487161636, 0.04095727205276489, 0.03989821672439575, 0.04556490108370781, 0.041921623051166534, 0.04356468468904495, 0.04086924344301224, 0.04169493913650513, 0.040672749280929565, 0.042969293892383575, 0.04571447893977165, 0.015356086194515228], 'L_si': [0.008169787004590034, 0.008781664073467255, 0.009711196646094322, 0.006992161273956299, 0.009001029655337334, 0.009114827960729599, 0.009281255304813385, 0.009079162031412125, 0.007841324433684349, 0.008153347298502922, 0.009860936552286148, 0.008957035839557648, 0.009623108431696892, 0.008886454626917839, 0.008907955139875412, 0.00815591961145401, 0.008670687675476074, 0.00982646644115448, 0.007826557382941246], 'L_grad': [0.033573947846889496, 0.03320867568254471, 0.03209937363862991, 0.03370419144630432, 0.0316661074757576, 0.03325911983847618, 0.03501925244927406, 0.03220546245574951, 0.03311594948172569, 0.03174487128853798, 0.03570396453142166, 0.03296458721160889, 0.03394157439470291, 0.03198278695344925, 0.032786983996629715, 0.032516829669475555, 0.0342986062169075, 0.03588801249861717, 0.007529528811573982]}
Train Epoch: 36 [0/816 (0%)] loss: 0.0256 L_si: 0.0026 L_grad: 0.0229 
Train Epoch: 36 [36/816 (4%)] loss: 0.0323 L_si: 0.0061 L_grad: 0.0263 
Train Epoch: 36 [72/816 (9%)] loss: 0.0233 L_si: 0.0026 L_grad: 0.0207 
Train Epoch: 36 [108/816 (13%)] loss: 0.0278 L_si: 0.0036 L_grad: 0.0242 
Train Epoch: 36 [144/816 (18%)] loss: 0.0292 L_si: 0.0047 L_grad: 0.0245 
Train Epoch: 36 [180/816 (22%)] loss: 0.0283 L_si: 0.0043 L_grad: 0.0240 
Train Epoch: 36 [216/816 (26%)] loss: 0.0362 L_si: 0.0119 L_grad: 0.0243 
Train Epoch: 36 [252/816 (31%)] loss: 0.0231 L_si: 0.0029 L_grad: 0.0201 
Train Epoch: 36 [288/816 (35%)] loss: 0.0269 L_si: 0.0037 L_grad: 0.0232 
Train Epoch: 36 [324/816 (40%)] loss: 0.0248 L_si: 0.0035 L_grad: 0.0213 
Train Epoch: 36 [360/816 (44%)] loss: 0.0319 L_si: 0.0068 L_grad: 0.0252 
Train Epoch: 36 [396/816 (49%)] loss: 0.0195 L_si: 0.0014 L_grad: 0.0182 
Train Epoch: 36 [432/816 (53%)] loss: 0.0367 L_si: 0.0062 L_grad: 0.0305 
Train Epoch: 36 [468/816 (57%)] loss: 0.0433 L_si: 0.0131 L_grad: 0.0301 
Train Epoch: 36 [504/816 (62%)] loss: 0.0353 L_si: 0.0081 L_grad: 0.0273 
Train Epoch: 36 [540/816 (66%)] loss: 0.0300 L_si: 0.0058 L_grad: 0.0241 
Train Epoch: 36 [576/816 (71%)] loss: 0.0263 L_si: 0.0037 L_grad: 0.0226 
Train Epoch: 36 [612/816 (75%)] loss: 0.0294 L_si: 0.0058 L_grad: 0.0236 
Train Epoch: 36 [648/816 (79%)] loss: 0.0345 L_si: 0.0088 L_grad: 0.0257 
Train Epoch: 36 [684/816 (84%)] loss: 0.0394 L_si: 0.0111 L_grad: 0.0283 
Train Epoch: 36 [720/816 (88%)] loss: 0.0325 L_si: 0.0055 L_grad: 0.0270 
Train Epoch: 36 [756/816 (93%)] loss: 0.0302 L_si: 0.0045 L_grad: 0.0257 
Train Epoch: 36 [792/816 (97%)] loss: 0.0305 L_si: 0.0074 L_grad: 0.0231 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.0404348224401474, 0.03876581788063049, 0.03726406395435333, 0.03958600014448166, 0.04184624180197716, 0.04413844645023346, 0.04022667184472084, 0.040708839893341064, 0.04128735139966011, 0.04559142887592316, 0.04174612835049629, 0.040611229836940765, 0.03910447657108307, 0.039012543857097626, 0.03950777277350426, 0.04144853353500366, 0.03911057859659195, 0.038072530180215836, 0.015108587220311165], 'L_si': [0.007993597537279129, 0.007283082231879234, 0.006687352433800697, 0.007273012772202492, 0.006862521171569824, 0.008134894073009491, 0.0057948194444179535, 0.007403139024972916, 0.00679861381649971, 0.009606540203094482, 0.008992623537778854, 0.007412498816847801, 0.0072189923375844955, 0.007061075419187546, 0.007012460380792618, 0.007127039134502411, 0.0070702265948057175, 0.0061888620257377625, 0.005903646349906921], 'L_grad': [0.03244122490286827, 0.03148273378610611, 0.030576709657907486, 0.032312989234924316, 0.03498372063040733, 0.03600355237722397, 0.03443185240030289, 0.03330570086836815, 0.0344887375831604, 0.035984888672828674, 0.03275350481271744, 0.033198729157447815, 0.031885482370853424, 0.03195146843791008, 0.03249531239271164, 0.03432149440050125, 0.03204035386443138, 0.03188366815447807, 0.009204940870404243]}
Train Epoch: 37 [0/816 (0%)] loss: 0.0244 L_si: 0.0021 L_grad: 0.0223 
Train Epoch: 37 [36/816 (4%)] loss: 0.0321 L_si: 0.0047 L_grad: 0.0274 
Train Epoch: 37 [72/816 (9%)] loss: 0.0245 L_si: 0.0032 L_grad: 0.0213 
Train Epoch: 37 [108/816 (13%)] loss: 0.0309 L_si: 0.0061 L_grad: 0.0248 
Train Epoch: 37 [144/816 (18%)] loss: 0.0246 L_si: 0.0031 L_grad: 0.0216 
Train Epoch: 37 [180/816 (22%)] loss: 0.0257 L_si: 0.0026 L_grad: 0.0232 
Train Epoch: 37 [216/816 (26%)] loss: 0.0297 L_si: 0.0053 L_grad: 0.0245 
Train Epoch: 37 [252/816 (31%)] loss: 0.0445 L_si: 0.0102 L_grad: 0.0343 
Train Epoch: 37 [288/816 (35%)] loss: 0.0380 L_si: 0.0102 L_grad: 0.0278 
Train Epoch: 37 [324/816 (40%)] loss: 0.0324 L_si: 0.0072 L_grad: 0.0252 
Train Epoch: 37 [360/816 (44%)] loss: 0.0271 L_si: 0.0032 L_grad: 0.0239 
Train Epoch: 37 [396/816 (49%)] loss: 0.0313 L_si: 0.0038 L_grad: 0.0275 
Train Epoch: 37 [432/816 (53%)] loss: 0.0315 L_si: 0.0046 L_grad: 0.0269 
Train Epoch: 37 [468/816 (57%)] loss: 0.0372 L_si: 0.0093 L_grad: 0.0279 
Train Epoch: 37 [504/816 (62%)] loss: 0.0292 L_si: 0.0063 L_grad: 0.0229 
Train Epoch: 37 [540/816 (66%)] loss: 0.0310 L_si: 0.0073 L_grad: 0.0237 
Train Epoch: 37 [576/816 (71%)] loss: 0.0382 L_si: 0.0096 L_grad: 0.0286 
Train Epoch: 37 [612/816 (75%)] loss: 0.0323 L_si: 0.0050 L_grad: 0.0272 
Train Epoch: 37 [648/816 (79%)] loss: 0.0401 L_si: 0.0107 L_grad: 0.0294 
Train Epoch: 37 [684/816 (84%)] loss: 0.0275 L_si: 0.0045 L_grad: 0.0230 
Train Epoch: 37 [720/816 (88%)] loss: 0.0254 L_si: 0.0034 L_grad: 0.0220 
Train Epoch: 37 [756/816 (93%)] loss: 0.0317 L_si: 0.0061 L_grad: 0.0257 
Train Epoch: 37 [792/816 (97%)] loss: 0.0395 L_si: 0.0092 L_grad: 0.0303 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.047447606921195984, 0.04187895357608795, 0.04126880317926407, 0.04200007766485214, 0.04196292161941528, 0.043140992522239685, 0.041187647730112076, 0.043558269739151, 0.04364614561200142, 0.0415126197040081, 0.0426398366689682, 0.0437077172100544, 0.035899702459573746, 0.040325552225112915, 0.04568490758538246, 0.041775234043598175, 0.04240424931049347, 0.042566969990730286, 0.01799149438738823], 'L_si': [0.010436568409204483, 0.008962748572230339, 0.008122473955154419, 0.007959270849823952, 0.007612423971295357, 0.00915519893169403, 0.0075506605207920074, 0.007929250597953796, 0.00886143371462822, 0.008089415729045868, 0.007917886599898338, 0.009351290762424469, 0.005593227222561836, 0.008394300937652588, 0.009057190269231796, 0.008216165006160736, 0.009588442742824554, 0.008876128122210503, 0.010223282501101494], 'L_grad': [0.0370110385119915, 0.03291620686650276, 0.03314632922410965, 0.03404080867767334, 0.034350499510765076, 0.033985793590545654, 0.03363698720932007, 0.035629019141197205, 0.0347847118973732, 0.033423203974962234, 0.03472194820642471, 0.03435642644762993, 0.03030647523701191, 0.03193125128746033, 0.036627717316150665, 0.03355906903743744, 0.032815806567668915, 0.033690840005874634, 0.0077682118862867355]}
Train Epoch: 38 [0/816 (0%)] loss: 0.0293 L_si: 0.0048 L_grad: 0.0245 
Train Epoch: 38 [36/816 (4%)] loss: 0.0368 L_si: 0.0075 L_grad: 0.0293 
Train Epoch: 38 [72/816 (9%)] loss: 0.0362 L_si: 0.0104 L_grad: 0.0258 
Train Epoch: 38 [108/816 (13%)] loss: 0.0295 L_si: 0.0057 L_grad: 0.0238 
Train Epoch: 38 [144/816 (18%)] loss: 0.0241 L_si: 0.0026 L_grad: 0.0215 
Train Epoch: 38 [180/816 (22%)] loss: 0.0305 L_si: 0.0038 L_grad: 0.0267 
Train Epoch: 38 [216/816 (26%)] loss: 0.0396 L_si: 0.0094 L_grad: 0.0302 
Train Epoch: 38 [252/816 (31%)] loss: 0.0249 L_si: 0.0037 L_grad: 0.0211 
Train Epoch: 38 [288/816 (35%)] loss: 0.0284 L_si: 0.0039 L_grad: 0.0246 
Train Epoch: 38 [324/816 (40%)] loss: 0.0281 L_si: 0.0043 L_grad: 0.0238 
Train Epoch: 38 [360/816 (44%)] loss: 0.0359 L_si: 0.0076 L_grad: 0.0284 
Train Epoch: 38 [396/816 (49%)] loss: 0.0339 L_si: 0.0078 L_grad: 0.0261 
Train Epoch: 38 [432/816 (53%)] loss: 0.0328 L_si: 0.0044 L_grad: 0.0284 
Train Epoch: 38 [468/816 (57%)] loss: 0.0256 L_si: 0.0032 L_grad: 0.0224 
Train Epoch: 38 [504/816 (62%)] loss: 0.0316 L_si: 0.0053 L_grad: 0.0263 
Train Epoch: 38 [540/816 (66%)] loss: 0.0316 L_si: 0.0044 L_grad: 0.0272 
Train Epoch: 38 [576/816 (71%)] loss: 0.0418 L_si: 0.0151 L_grad: 0.0267 
Train Epoch: 38 [612/816 (75%)] loss: 0.0392 L_si: 0.0106 L_grad: 0.0285 
Train Epoch: 38 [648/816 (79%)] loss: 0.0295 L_si: 0.0064 L_grad: 0.0231 
Train Epoch: 38 [684/816 (84%)] loss: 0.0269 L_si: 0.0035 L_grad: 0.0233 
Train Epoch: 38 [720/816 (88%)] loss: 0.0266 L_si: 0.0036 L_grad: 0.0230 
Train Epoch: 38 [756/816 (93%)] loss: 0.0293 L_si: 0.0050 L_grad: 0.0243 
Train Epoch: 38 [792/816 (97%)] loss: 0.0347 L_si: 0.0073 L_grad: 0.0274 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.03981689363718033, 0.042763855308294296, 0.04087890312075615, 0.04113229736685753, 0.0410994291305542, 0.040794968605041504, 0.038271624594926834, 0.04050598666071892, 0.039110444486141205, 0.045913226902484894, 0.040361516177654266, 0.042931534349918365, 0.04427501931786537, 0.0401713103055954, 0.04273780435323715, 0.043848998844623566, 0.04306250810623169, 0.04147076606750488, 0.014680678024888039], 'L_si': [0.008113343268632889, 0.008354853838682175, 0.008461564779281616, 0.008909646421670914, 0.008111817762255669, 0.008880183100700378, 0.0075586941093206406, 0.00829300656914711, 0.008338987827301025, 0.009543448686599731, 0.008184943348169327, 0.009594682604074478, 0.009548310190439224, 0.009218145161867142, 0.008684271946549416, 0.008888095617294312, 0.009259745478630066, 0.00839434377849102, 0.005645319819450378], 'L_grad': [0.03170355036854744, 0.03440900146961212, 0.03241733834147453, 0.032222650945186615, 0.03298761323094368, 0.031914785504341125, 0.030712930485606194, 0.03221298009157181, 0.03077145852148533, 0.03636977821588516, 0.03217657282948494, 0.03333685174584389, 0.03472670912742615, 0.030953167006373405, 0.03405353054404259, 0.034960903227329254, 0.033802762627601624, 0.03307642415165901, 0.00903535820543766]}
Train Epoch: 39 [0/816 (0%)] loss: 0.0476 L_si: 0.0182 L_grad: 0.0293 
Train Epoch: 39 [36/816 (4%)] loss: 0.0336 L_si: 0.0061 L_grad: 0.0275 
Train Epoch: 39 [72/816 (9%)] loss: 0.0290 L_si: 0.0049 L_grad: 0.0241 
Train Epoch: 39 [108/816 (13%)] loss: 0.0382 L_si: 0.0100 L_grad: 0.0282 
Train Epoch: 39 [144/816 (18%)] loss: 0.0291 L_si: 0.0046 L_grad: 0.0245 
Train Epoch: 39 [180/816 (22%)] loss: 0.0258 L_si: 0.0030 L_grad: 0.0227 
Train Epoch: 39 [216/816 (26%)] loss: 0.0314 L_si: 0.0049 L_grad: 0.0266 
Train Epoch: 39 [252/816 (31%)] loss: 0.0318 L_si: 0.0055 L_grad: 0.0262 
Train Epoch: 39 [288/816 (35%)] loss: 0.0387 L_si: 0.0084 L_grad: 0.0303 
Train Epoch: 39 [324/816 (40%)] loss: 0.0280 L_si: 0.0032 L_grad: 0.0248 
Train Epoch: 39 [360/816 (44%)] loss: 0.0267 L_si: 0.0024 L_grad: 0.0243 
Train Epoch: 39 [396/816 (49%)] loss: 0.0352 L_si: 0.0073 L_grad: 0.0278 
Train Epoch: 39 [432/816 (53%)] loss: 0.0305 L_si: 0.0038 L_grad: 0.0267 
Train Epoch: 39 [468/816 (57%)] loss: 0.0338 L_si: 0.0106 L_grad: 0.0232 
Train Epoch: 39 [504/816 (62%)] loss: 0.0206 L_si: 0.0018 L_grad: 0.0188 
Train Epoch: 39 [540/816 (66%)] loss: 0.0431 L_si: 0.0118 L_grad: 0.0313 
Train Epoch: 39 [576/816 (71%)] loss: 0.0275 L_si: 0.0045 L_grad: 0.0231 
Train Epoch: 39 [612/816 (75%)] loss: 0.0241 L_si: 0.0026 L_grad: 0.0215 
Train Epoch: 39 [648/816 (79%)] loss: 0.0242 L_si: 0.0032 L_grad: 0.0210 
Train Epoch: 39 [684/816 (84%)] loss: 0.0299 L_si: 0.0055 L_grad: 0.0244 
Train Epoch: 39 [720/816 (88%)] loss: 0.0371 L_si: 0.0086 L_grad: 0.0285 
Train Epoch: 39 [756/816 (93%)] loss: 0.0297 L_si: 0.0081 L_grad: 0.0217 
Train Epoch: 39 [792/816 (97%)] loss: 0.0265 L_si: 0.0030 L_grad: 0.0234 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.03610784560441971, 0.041221983730793, 0.04793218523263931, 0.044223811477422714, 0.04038833826780319, 0.035696130245923996, 0.04566774144768715, 0.04282192140817642, 0.04133337363600731, 0.040157027542591095, 0.04473227635025978, 0.04126877337694168, 0.04253191873431206, 0.04072636365890503, 0.04088468849658966, 0.04458446800708771, 0.03810863941907883, 0.04362083971500397, 0.015293462201952934], 'L_si': [0.006973849609494209, 0.008754797279834747, 0.01064436137676239, 0.009059876203536987, 0.008055714890360832, 0.005878802388906479, 0.008279971778392792, 0.008613036945462227, 0.00782056525349617, 0.008415436372160912, 0.009843826293945312, 0.00867382436990738, 0.008019331842660904, 0.009254561737179756, 0.008463751524686813, 0.009315036237239838, 0.007303040474653244, 0.008893191814422607, 0.005249351263046265], 'L_grad': [0.0291339959949255, 0.03246718645095825, 0.03728782385587692, 0.03516393527388573, 0.03233262151479721, 0.029817327857017517, 0.03738776966929436, 0.034208886325359344, 0.03351280838251114, 0.031741589307785034, 0.03488845005631447, 0.0325949490070343, 0.034512586891651154, 0.03147180378437042, 0.03242093697190285, 0.03526943176984787, 0.030805598944425583, 0.03472764790058136, 0.01004411093890667]}
Train Epoch: 40 [0/816 (0%)] loss: 0.0343 L_si: 0.0080 L_grad: 0.0262 
Train Epoch: 40 [36/816 (4%)] loss: 0.0312 L_si: 0.0066 L_grad: 0.0245 
Train Epoch: 40 [72/816 (9%)] loss: 0.0331 L_si: 0.0059 L_grad: 0.0272 
Train Epoch: 40 [108/816 (13%)] loss: 0.0275 L_si: 0.0039 L_grad: 0.0236 
Train Epoch: 40 [144/816 (18%)] loss: 0.0422 L_si: 0.0093 L_grad: 0.0329 
Train Epoch: 40 [180/816 (22%)] loss: 0.0257 L_si: 0.0040 L_grad: 0.0218 
Train Epoch: 40 [216/816 (26%)] loss: 0.0300 L_si: 0.0042 L_grad: 0.0258 
Train Epoch: 40 [252/816 (31%)] loss: 0.0229 L_si: 0.0021 L_grad: 0.0208 
Train Epoch: 40 [288/816 (35%)] loss: 0.0327 L_si: 0.0061 L_grad: 0.0266 
Train Epoch: 40 [324/816 (40%)] loss: 0.0358 L_si: 0.0066 L_grad: 0.0292 
Train Epoch: 40 [360/816 (44%)] loss: 0.0353 L_si: 0.0066 L_grad: 0.0288 
Train Epoch: 40 [396/816 (49%)] loss: 0.0237 L_si: 0.0030 L_grad: 0.0207 
Train Epoch: 40 [432/816 (53%)] loss: 0.0329 L_si: 0.0060 L_grad: 0.0269 
Train Epoch: 40 [468/816 (57%)] loss: 0.0371 L_si: 0.0111 L_grad: 0.0260 
Train Epoch: 40 [504/816 (62%)] loss: 0.0363 L_si: 0.0064 L_grad: 0.0299 
Train Epoch: 40 [540/816 (66%)] loss: 0.0396 L_si: 0.0155 L_grad: 0.0241 
Train Epoch: 40 [576/816 (71%)] loss: 0.0270 L_si: 0.0045 L_grad: 0.0226 
Train Epoch: 40 [612/816 (75%)] loss: 0.0254 L_si: 0.0028 L_grad: 0.0226 
Train Epoch: 40 [648/816 (79%)] loss: 0.0324 L_si: 0.0060 L_grad: 0.0264 
Train Epoch: 40 [684/816 (84%)] loss: 0.0404 L_si: 0.0086 L_grad: 0.0317 
Train Epoch: 40 [720/816 (88%)] loss: 0.0304 L_si: 0.0080 L_grad: 0.0224 
Train Epoch: 40 [756/816 (93%)] loss: 0.0287 L_si: 0.0037 L_grad: 0.0250 
Train Epoch: 40 [792/816 (97%)] loss: 0.0287 L_si: 0.0044 L_grad: 0.0243 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving checkpoint: /root/autodl-tmp/train_il_200e/train_s2d_SpikeTransformer/checkpoint-epoch040-loss-0.0308.pth.tar ...
all losses in batch in validation:  {'loss': [0.041796546429395676, 0.042712099850177765, 0.03929797559976578, 0.03938223049044609, 0.04005187749862671, 0.03972979635000229, 0.042430806905031204, 0.04001636058092117, 0.042421456426382065, 0.04234859719872475, 0.043151043355464935, 0.044315069913864136, 0.03958752006292343, 0.04077360779047012, 0.03986026346683502, 0.04245816916227341, 0.04218187555670738, 0.039292871952056885, 0.017321620136499405], 'L_si': [0.010147981345653534, 0.010198744013905525, 0.008593803271651268, 0.007812581956386566, 0.009136222302913666, 0.008426250889897346, 0.008943837136030197, 0.00819472037255764, 0.00932154431939125, 0.009048108011484146, 0.009597636759281158, 0.009210892021656036, 0.008476799353957176, 0.008952932432293892, 0.00834243930876255, 0.009167077019810677, 0.00766749307513237, 0.007731972262263298, 0.00938289612531662], 'L_grad': [0.03164856508374214, 0.03251335769891739, 0.03070417046546936, 0.031569648534059525, 0.030915655195713043, 0.03130354732275009, 0.03348696976900101, 0.031821638345718384, 0.033099912106990814, 0.0333004891872406, 0.03355340659618378, 0.0351041778922081, 0.031110722571611404, 0.03182067722082138, 0.03151782602071762, 0.03329109400510788, 0.03451438248157501, 0.03156089782714844, 0.007938724011182785]}
Train Epoch: 41 [0/816 (0%)] loss: 0.0337 L_si: 0.0090 L_grad: 0.0248 
Train Epoch: 41 [36/816 (4%)] loss: 0.0357 L_si: 0.0080 L_grad: 0.0277 
Train Epoch: 41 [72/816 (9%)] loss: 0.0379 L_si: 0.0081 L_grad: 0.0298 
Train Epoch: 41 [108/816 (13%)] loss: 0.0332 L_si: 0.0073 L_grad: 0.0259 
Train Epoch: 41 [144/816 (18%)] loss: 0.0404 L_si: 0.0108 L_grad: 0.0296 
Train Epoch: 41 [180/816 (22%)] loss: 0.0215 L_si: 0.0022 L_grad: 0.0193 
Train Epoch: 41 [216/816 (26%)] loss: 0.0303 L_si: 0.0057 L_grad: 0.0247 
Train Epoch: 41 [252/816 (31%)] loss: 0.0340 L_si: 0.0072 L_grad: 0.0269 
Train Epoch: 41 [288/816 (35%)] loss: 0.0337 L_si: 0.0063 L_grad: 0.0274 
Train Epoch: 41 [324/816 (40%)] loss: 0.0268 L_si: 0.0038 L_grad: 0.0230 
Train Epoch: 41 [360/816 (44%)] loss: 0.0351 L_si: 0.0104 L_grad: 0.0247 
Train Epoch: 41 [396/816 (49%)] loss: 0.0257 L_si: 0.0030 L_grad: 0.0227 
Train Epoch: 41 [432/816 (53%)] loss: 0.0331 L_si: 0.0059 L_grad: 0.0271 
Train Epoch: 41 [468/816 (57%)] loss: 0.0329 L_si: 0.0044 L_grad: 0.0285 
Train Epoch: 41 [504/816 (62%)] loss: 0.0263 L_si: 0.0031 L_grad: 0.0232 
Train Epoch: 41 [540/816 (66%)] loss: 0.0312 L_si: 0.0057 L_grad: 0.0255 
Train Epoch: 41 [576/816 (71%)] loss: 0.0317 L_si: 0.0060 L_grad: 0.0257 
Train Epoch: 41 [612/816 (75%)] loss: 0.0435 L_si: 0.0129 L_grad: 0.0305 
Train Epoch: 41 [648/816 (79%)] loss: 0.0308 L_si: 0.0084 L_grad: 0.0224 
Train Epoch: 41 [684/816 (84%)] loss: 0.0329 L_si: 0.0062 L_grad: 0.0267 
Train Epoch: 41 [720/816 (88%)] loss: 0.0257 L_si: 0.0039 L_grad: 0.0218 
Train Epoch: 41 [756/816 (93%)] loss: 0.0318 L_si: 0.0088 L_grad: 0.0230 
Train Epoch: 41 [792/816 (97%)] loss: 0.0291 L_si: 0.0043 L_grad: 0.0249 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.041588056832551956, 0.03905551880598068, 0.03747245669364929, 0.038980431854724884, 0.03859815374016762, 0.037938233464956284, 0.03874458745121956, 0.04228362441062927, 0.04278564453125, 0.040482617914676666, 0.0398855060338974, 0.04105198383331299, 0.0418182872235775, 0.04019429534673691, 0.04055625945329666, 0.04016273468732834, 0.03830086439847946, 0.04045065492391586, 0.019350696355104446], 'L_si': [0.00877972412854433, 0.00691978819668293, 0.005351364612579346, 0.007134502753615379, 0.00684872642159462, 0.006715256720781326, 0.007121920585632324, 0.008810406550765038, 0.0072680022567510605, 0.005936518311500549, 0.007064001634716988, 0.008433440700173378, 0.007015664130449295, 0.006472671404480934, 0.007550299167633057, 0.00713094137609005, 0.006634212099015713, 0.007201848551630974, 0.00980880856513977], 'L_grad': [0.0328083336353302, 0.0321357324719429, 0.032121092081069946, 0.031845927238464355, 0.031749427318573, 0.031222976744174957, 0.031622666865587234, 0.033473215997219086, 0.03551764041185379, 0.03454609960317612, 0.03282150253653526, 0.03261854127049446, 0.034802623093128204, 0.03372162580490112, 0.033005960285663605, 0.03303179144859314, 0.031666651368141174, 0.03324880450963974, 0.009541886858642101]}
Train Epoch: 42 [0/816 (0%)] loss: 0.0404 L_si: 0.0107 L_grad: 0.0297 
Train Epoch: 42 [36/816 (4%)] loss: 0.0333 L_si: 0.0090 L_grad: 0.0243 
Train Epoch: 42 [72/816 (9%)] loss: 0.0286 L_si: 0.0032 L_grad: 0.0254 
Train Epoch: 42 [108/816 (13%)] loss: 0.0483 L_si: 0.0180 L_grad: 0.0303 
Train Epoch: 42 [144/816 (18%)] loss: 0.0312 L_si: 0.0048 L_grad: 0.0264 
Train Epoch: 42 [180/816 (22%)] loss: 0.0221 L_si: 0.0023 L_grad: 0.0197 
Train Epoch: 42 [216/816 (26%)] loss: 0.0459 L_si: 0.0123 L_grad: 0.0336 
Train Epoch: 42 [252/816 (31%)] loss: 0.0348 L_si: 0.0064 L_grad: 0.0284 
Train Epoch: 42 [288/816 (35%)] loss: 0.0386 L_si: 0.0096 L_grad: 0.0290 
Train Epoch: 42 [324/816 (40%)] loss: 0.0228 L_si: 0.0029 L_grad: 0.0198 
Train Epoch: 42 [360/816 (44%)] loss: 0.0403 L_si: 0.0135 L_grad: 0.0268 
Train Epoch: 42 [396/816 (49%)] loss: 0.0315 L_si: 0.0055 L_grad: 0.0260 
Train Epoch: 42 [432/816 (53%)] loss: 0.0349 L_si: 0.0092 L_grad: 0.0257 
Train Epoch: 42 [468/816 (57%)] loss: 0.0268 L_si: 0.0044 L_grad: 0.0225 
Train Epoch: 42 [504/816 (62%)] loss: 0.0507 L_si: 0.0162 L_grad: 0.0345 
Train Epoch: 42 [540/816 (66%)] loss: 0.0304 L_si: 0.0046 L_grad: 0.0257 
Train Epoch: 42 [576/816 (71%)] loss: 0.0310 L_si: 0.0052 L_grad: 0.0258 
Train Epoch: 42 [612/816 (75%)] loss: 0.0391 L_si: 0.0061 L_grad: 0.0330 
Train Epoch: 42 [648/816 (79%)] loss: 0.0286 L_si: 0.0050 L_grad: 0.0236 
Train Epoch: 42 [684/816 (84%)] loss: 0.0367 L_si: 0.0109 L_grad: 0.0257 
Train Epoch: 42 [720/816 (88%)] loss: 0.0361 L_si: 0.0077 L_grad: 0.0284 
Train Epoch: 42 [756/816 (93%)] loss: 0.0264 L_si: 0.0050 L_grad: 0.0214 
Train Epoch: 42 [792/816 (97%)] loss: 0.0230 L_si: 0.0032 L_grad: 0.0198 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.044786758720874786, 0.040617797523736954, 0.03779112175107002, 0.0382378026843071, 0.039560094475746155, 0.03743807226419449, 0.037802066653966904, 0.038522206246852875, 0.04220901429653168, 0.03925241529941559, 0.03945956751704216, 0.04012402147054672, 0.04125495254993439, 0.03930249065160751, 0.04013488441705704, 0.03768052160739899, 0.04148305207490921, 0.03950667381286621, 0.012394068762660027], 'L_si': [0.009057698771357536, 0.007774844765663147, 0.006454426795244217, 0.006683504208922386, 0.007735731080174446, 0.006246065720915794, 0.006698139011859894, 0.006968289613723755, 0.007397504523396492, 0.0070341844111680984, 0.007464524358510971, 0.006921315565705299, 0.00839541107416153, 0.007791575044393539, 0.008548539131879807, 0.005689740180969238, 0.007544331252574921, 0.00798911601305008, 0.00482039712369442], 'L_grad': [0.0357290580868721, 0.03284295275807381, 0.031336694955825806, 0.03155430033802986, 0.03182436153292656, 0.031192004680633545, 0.03110392764210701, 0.03155391663312912, 0.034811511635780334, 0.03221822902560234, 0.03199504315853119, 0.033202704042196274, 0.03285954147577286, 0.031510915607213974, 0.03158634528517723, 0.03199078142642975, 0.03393872082233429, 0.03151755779981613, 0.007573672104626894]}
Train Epoch: 43 [0/816 (0%)] loss: 0.0296 L_si: 0.0046 L_grad: 0.0250 
Train Epoch: 43 [36/816 (4%)] loss: 0.0271 L_si: 0.0036 L_grad: 0.0234 
Train Epoch: 43 [72/816 (9%)] loss: 0.0291 L_si: 0.0036 L_grad: 0.0255 
Train Epoch: 43 [108/816 (13%)] loss: 0.0268 L_si: 0.0034 L_grad: 0.0234 
Train Epoch: 43 [144/816 (18%)] loss: 0.0255 L_si: 0.0030 L_grad: 0.0225 
Train Epoch: 43 [180/816 (22%)] loss: 0.0348 L_si: 0.0074 L_grad: 0.0274 
Train Epoch: 43 [216/816 (26%)] loss: 0.0408 L_si: 0.0089 L_grad: 0.0319 
Train Epoch: 43 [252/816 (31%)] loss: 0.0376 L_si: 0.0090 L_grad: 0.0286 
Train Epoch: 43 [288/816 (35%)] loss: 0.0253 L_si: 0.0041 L_grad: 0.0212 
Train Epoch: 43 [324/816 (40%)] loss: 0.0308 L_si: 0.0050 L_grad: 0.0258 
Train Epoch: 43 [360/816 (44%)] loss: 0.0312 L_si: 0.0057 L_grad: 0.0255 
Train Epoch: 43 [396/816 (49%)] loss: 0.0265 L_si: 0.0031 L_grad: 0.0234 
Train Epoch: 43 [432/816 (53%)] loss: 0.0272 L_si: 0.0044 L_grad: 0.0229 
Train Epoch: 43 [468/816 (57%)] loss: 0.0372 L_si: 0.0097 L_grad: 0.0276 
Train Epoch: 43 [504/816 (62%)] loss: 0.0408 L_si: 0.0111 L_grad: 0.0297 
Train Epoch: 43 [540/816 (66%)] loss: 0.0369 L_si: 0.0073 L_grad: 0.0296 
Train Epoch: 43 [576/816 (71%)] loss: 0.0326 L_si: 0.0069 L_grad: 0.0258 
Train Epoch: 43 [612/816 (75%)] loss: 0.0369 L_si: 0.0091 L_grad: 0.0278 
Train Epoch: 43 [648/816 (79%)] loss: 0.0351 L_si: 0.0064 L_grad: 0.0288 
Train Epoch: 43 [684/816 (84%)] loss: 0.0348 L_si: 0.0076 L_grad: 0.0272 
Train Epoch: 43 [720/816 (88%)] loss: 0.0226 L_si: 0.0022 L_grad: 0.0204 
Train Epoch: 43 [756/816 (93%)] loss: 0.0316 L_si: 0.0076 L_grad: 0.0240 
Train Epoch: 43 [792/816 (97%)] loss: 0.0282 L_si: 0.0053 L_grad: 0.0229 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04209011793136597, 0.04288749396800995, 0.043357569724321365, 0.03923901170492172, 0.04148981347680092, 0.04285593330860138, 0.045726388692855835, 0.0434083454310894, 0.04253663867712021, 0.04453478008508682, 0.0402529239654541, 0.0432458333671093, 0.04171423614025116, 0.03570578992366791, 0.042883872985839844, 0.042849451303482056, 0.04012478515505791, 0.043786417692899704, 0.015579583123326302], 'L_si': [0.008228622376918793, 0.009470343589782715, 0.008542891591787338, 0.006680704653263092, 0.008317817002534866, 0.007899489253759384, 0.008672535419464111, 0.009158708155155182, 0.008802676573395729, 0.005752377212047577, 0.007155081257224083, 0.007755827158689499, 0.00842256285250187, 0.00549689307808876, 0.007942384108901024, 0.00841306522488594, 0.007935144007205963, 0.008996319025754929, 0.007154842838644981], 'L_grad': [0.033861495554447174, 0.033417150378227234, 0.03481467813253403, 0.03255830705165863, 0.03317199647426605, 0.034956444054841995, 0.037053853273391724, 0.03424963727593422, 0.03373396396636963, 0.038782402873039246, 0.03309784084558487, 0.0354900062084198, 0.03329167142510414, 0.030208896845579147, 0.03494149073958397, 0.034436386078596115, 0.032189641147851944, 0.034790098667144775, 0.00842474028468132]}
Train Epoch: 44 [0/816 (0%)] loss: 0.0306 L_si: 0.0045 L_grad: 0.0261 
Train Epoch: 44 [36/816 (4%)] loss: 0.0262 L_si: 0.0051 L_grad: 0.0210 
Train Epoch: 44 [72/816 (9%)] loss: 0.0310 L_si: 0.0051 L_grad: 0.0259 
Train Epoch: 44 [108/816 (13%)] loss: 0.0319 L_si: 0.0060 L_grad: 0.0259 
Train Epoch: 44 [144/816 (18%)] loss: 0.0247 L_si: 0.0022 L_grad: 0.0226 
Train Epoch: 44 [180/816 (22%)] loss: 0.0250 L_si: 0.0026 L_grad: 0.0224 
Train Epoch: 44 [216/816 (26%)] loss: 0.0311 L_si: 0.0052 L_grad: 0.0259 
Train Epoch: 44 [252/816 (31%)] loss: 0.0376 L_si: 0.0076 L_grad: 0.0300 
Train Epoch: 44 [288/816 (35%)] loss: 0.0323 L_si: 0.0064 L_grad: 0.0259 
Train Epoch: 44 [324/816 (40%)] loss: 0.0327 L_si: 0.0056 L_grad: 0.0271 
Train Epoch: 44 [360/816 (44%)] loss: 0.0347 L_si: 0.0073 L_grad: 0.0273 
Train Epoch: 44 [396/816 (49%)] loss: 0.0298 L_si: 0.0073 L_grad: 0.0225 
Train Epoch: 44 [432/816 (53%)] loss: 0.0260 L_si: 0.0026 L_grad: 0.0233 
Train Epoch: 44 [468/816 (57%)] loss: 0.0268 L_si: 0.0037 L_grad: 0.0231 
Train Epoch: 44 [504/816 (62%)] loss: 0.0241 L_si: 0.0021 L_grad: 0.0220 
Train Epoch: 44 [540/816 (66%)] loss: 0.0371 L_si: 0.0084 L_grad: 0.0287 
Train Epoch: 44 [576/816 (71%)] loss: 0.0281 L_si: 0.0038 L_grad: 0.0243 
Train Epoch: 44 [612/816 (75%)] loss: 0.0323 L_si: 0.0045 L_grad: 0.0278 
Train Epoch: 44 [648/816 (79%)] loss: 0.0340 L_si: 0.0063 L_grad: 0.0277 
Train Epoch: 44 [684/816 (84%)] loss: 0.0348 L_si: 0.0087 L_grad: 0.0261 
Train Epoch: 44 [720/816 (88%)] loss: 0.0318 L_si: 0.0055 L_grad: 0.0263 
Train Epoch: 44 [756/816 (93%)] loss: 0.0337 L_si: 0.0072 L_grad: 0.0265 
Train Epoch: 44 [792/816 (97%)] loss: 0.0310 L_si: 0.0041 L_grad: 0.0269 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.03827778995037079, 0.03680715709924698, 0.03609752655029297, 0.041039228439331055, 0.03802821785211563, 0.038916945457458496, 0.03769885376095772, 0.038468942046165466, 0.04159540683031082, 0.038956694304943085, 0.04130931943655014, 0.0404357947409153, 0.03943329304456711, 0.038943737745285034, 0.038624681532382965, 0.04112238809466362, 0.03883428871631622, 0.04301806166768074, 0.012314826250076294], 'L_si': [0.006552513688802719, 0.005654200911521912, 0.006078565493226051, 0.007491881027817726, 0.006148310378193855, 0.006042102351784706, 0.006339319050312042, 0.006409099325537682, 0.007499190047383308, 0.006315262988209724, 0.00746522843837738, 0.007748570293188095, 0.0056058187037706375, 0.005737535655498505, 0.005560822784900665, 0.007764715701341629, 0.006120482459664345, 0.006243016570806503, 0.0041390396654605865], 'L_grad': [0.03172527626156807, 0.031152956187725067, 0.030018959194421768, 0.03354734927415848, 0.031879909336566925, 0.03287484496831894, 0.031359534710645676, 0.032059840857982635, 0.03409621864557266, 0.03264143317937851, 0.03384409099817276, 0.0326872244477272, 0.03382747620344162, 0.03320620208978653, 0.0330638587474823, 0.03335767239332199, 0.03271380811929703, 0.03677504509687424, 0.008175786584615707]}
Train Epoch: 45 [0/816 (0%)] loss: 0.0351 L_si: 0.0081 L_grad: 0.0270 
Train Epoch: 45 [36/816 (4%)] loss: 0.0291 L_si: 0.0046 L_grad: 0.0245 
Train Epoch: 45 [72/816 (9%)] loss: 0.0289 L_si: 0.0039 L_grad: 0.0250 
Train Epoch: 45 [108/816 (13%)] loss: 0.0393 L_si: 0.0073 L_grad: 0.0320 
Train Epoch: 45 [144/816 (18%)] loss: 0.0367 L_si: 0.0057 L_grad: 0.0310 
Train Epoch: 45 [180/816 (22%)] loss: 0.0423 L_si: 0.0117 L_grad: 0.0306 
Train Epoch: 45 [216/816 (26%)] loss: 0.0343 L_si: 0.0069 L_grad: 0.0274 
Train Epoch: 45 [252/816 (31%)] loss: 0.0295 L_si: 0.0072 L_grad: 0.0222 
Train Epoch: 45 [288/816 (35%)] loss: 0.0345 L_si: 0.0090 L_grad: 0.0255 
Train Epoch: 45 [324/816 (40%)] loss: 0.0292 L_si: 0.0054 L_grad: 0.0238 
Train Epoch: 45 [360/816 (44%)] loss: 0.0362 L_si: 0.0076 L_grad: 0.0286 
Train Epoch: 45 [396/816 (49%)] loss: 0.0277 L_si: 0.0034 L_grad: 0.0243 
Train Epoch: 45 [432/816 (53%)] loss: 0.0319 L_si: 0.0079 L_grad: 0.0239 
Train Epoch: 45 [468/816 (57%)] loss: 0.0243 L_si: 0.0037 L_grad: 0.0205 
Train Epoch: 45 [504/816 (62%)] loss: 0.0282 L_si: 0.0041 L_grad: 0.0242 
Train Epoch: 45 [540/816 (66%)] loss: 0.0311 L_si: 0.0044 L_grad: 0.0266 
Train Epoch: 45 [576/816 (71%)] loss: 0.0329 L_si: 0.0074 L_grad: 0.0256 
Train Epoch: 45 [612/816 (75%)] loss: 0.0252 L_si: 0.0027 L_grad: 0.0226 
Train Epoch: 45 [648/816 (79%)] loss: 0.0341 L_si: 0.0071 L_grad: 0.0270 
Train Epoch: 45 [684/816 (84%)] loss: 0.0311 L_si: 0.0097 L_grad: 0.0215 
Train Epoch: 45 [720/816 (88%)] loss: 0.0328 L_si: 0.0056 L_grad: 0.0272 
Train Epoch: 45 [756/816 (93%)] loss: 0.0320 L_si: 0.0055 L_grad: 0.0265 
Train Epoch: 45 [792/816 (97%)] loss: 0.0288 L_si: 0.0035 L_grad: 0.0253 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.043081071227788925, 0.040942706167697906, 0.036759160459041595, 0.043175630271434784, 0.041053496301174164, 0.04140283167362213, 0.03982781246304512, 0.041499316692352295, 0.04101749137043953, 0.03813523054122925, 0.043507590889930725, 0.04154684394598007, 0.041962169110774994, 0.037961266934871674, 0.04514319449663162, 0.040042128413915634, 0.041866712272167206, 0.04069748520851135, 0.015707600861787796], 'L_si': [0.007932223379611969, 0.00791880488395691, 0.006841477006673813, 0.007931409403681755, 0.007674491032958031, 0.007878115400671959, 0.007617861032485962, 0.008248066529631615, 0.0053427331149578094, 0.006014090031385422, 0.008045796304941177, 0.007997101172804832, 0.007747381925582886, 0.007329046726226807, 0.008647417649626732, 0.007432539016008377, 0.007431581616401672, 0.008068282157182693, 0.008093979209661484], 'L_grad': [0.035148847848176956, 0.033023901283741, 0.029917685315012932, 0.03524422273039818, 0.033379003405570984, 0.03352471441030502, 0.03220995143055916, 0.03325124830007553, 0.03567475825548172, 0.032121140509843826, 0.03546179458498955, 0.03354974463582039, 0.03421478718519211, 0.030632220208644867, 0.03649577498435974, 0.03260958939790726, 0.03443513065576553, 0.03262920305132866, 0.0076136221177875996]}
Train Epoch: 46 [0/816 (0%)] loss: 0.0265 L_si: 0.0029 L_grad: 0.0236 
Train Epoch: 46 [36/816 (4%)] loss: 0.0318 L_si: 0.0073 L_grad: 0.0245 
Train Epoch: 46 [72/816 (9%)] loss: 0.0332 L_si: 0.0054 L_grad: 0.0277 
Train Epoch: 46 [108/816 (13%)] loss: 0.0289 L_si: 0.0049 L_grad: 0.0240 
Train Epoch: 46 [144/816 (18%)] loss: 0.0241 L_si: 0.0028 L_grad: 0.0212 
Train Epoch: 46 [180/816 (22%)] loss: 0.0271 L_si: 0.0046 L_grad: 0.0225 
Train Epoch: 46 [216/816 (26%)] loss: 0.0280 L_si: 0.0069 L_grad: 0.0211 
Train Epoch: 46 [252/816 (31%)] loss: 0.0262 L_si: 0.0040 L_grad: 0.0222 
Train Epoch: 46 [288/816 (35%)] loss: 0.0320 L_si: 0.0054 L_grad: 0.0266 
Train Epoch: 46 [324/816 (40%)] loss: 0.0298 L_si: 0.0039 L_grad: 0.0259 
Train Epoch: 46 [360/816 (44%)] loss: 0.0384 L_si: 0.0077 L_grad: 0.0307 
Train Epoch: 46 [396/816 (49%)] loss: 0.0256 L_si: 0.0033 L_grad: 0.0223 
Train Epoch: 46 [432/816 (53%)] loss: 0.0309 L_si: 0.0060 L_grad: 0.0249 
Train Epoch: 46 [468/816 (57%)] loss: 0.0296 L_si: 0.0043 L_grad: 0.0253 
Train Epoch: 46 [504/816 (62%)] loss: 0.0313 L_si: 0.0062 L_grad: 0.0251 
Train Epoch: 46 [540/816 (66%)] loss: 0.0291 L_si: 0.0072 L_grad: 0.0219 
Train Epoch: 46 [576/816 (71%)] loss: 0.0284 L_si: 0.0046 L_grad: 0.0238 
Train Epoch: 46 [612/816 (75%)] loss: 0.0390 L_si: 0.0103 L_grad: 0.0287 
Train Epoch: 46 [648/816 (79%)] loss: 0.0335 L_si: 0.0081 L_grad: 0.0254 
Train Epoch: 46 [684/816 (84%)] loss: 0.0340 L_si: 0.0083 L_grad: 0.0256 
Train Epoch: 46 [720/816 (88%)] loss: 0.0239 L_si: 0.0029 L_grad: 0.0211 
Train Epoch: 46 [756/816 (93%)] loss: 0.0285 L_si: 0.0037 L_grad: 0.0248 
Train Epoch: 46 [792/816 (97%)] loss: 0.0362 L_si: 0.0091 L_grad: 0.0271 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.040175240486860275, 0.03725424408912659, 0.04205191880464554, 0.038380563259124756, 0.04152902960777283, 0.037586815655231476, 0.04149838164448738, 0.03698831424117088, 0.0446387380361557, 0.035954199731349945, 0.037720952183008194, 0.04136672616004944, 0.039164938032627106, 0.036643169820308685, 0.041124358773231506, 0.038956232368946075, 0.04387693479657173, 0.04159341752529144, 0.014856727793812752], 'L_si': [0.008620060980319977, 0.005788445472717285, 0.007354456931352615, 0.006474921479821205, 0.008015288040041924, 0.005882741883397102, 0.007278081029653549, 0.005791421979665756, 0.008131148293614388, 0.005471447482705116, 0.006166230887174606, 0.007307380437850952, 0.007220551371574402, 0.006162269040942192, 0.007678890600800514, 0.00613892637193203, 0.008828919380903244, 0.007562519982457161, 0.006125936284661293], 'L_grad': [0.0315551795065403, 0.0314657986164093, 0.03469746187329292, 0.0319056399166584, 0.03351374343037605, 0.031704071909189224, 0.03422030061483383, 0.031196892261505127, 0.036507587879896164, 0.03048275038599968, 0.03155472129583359, 0.034059345722198486, 0.031944386661052704, 0.030480900779366493, 0.03344547003507614, 0.032817304134368896, 0.03504801541566849, 0.03403089568018913, 0.008730791509151459]}
Train Epoch: 47 [0/816 (0%)] loss: 0.0428 L_si: 0.0115 L_grad: 0.0313 
Train Epoch: 47 [36/816 (4%)] loss: 0.0340 L_si: 0.0087 L_grad: 0.0253 
Train Epoch: 47 [72/816 (9%)] loss: 0.0274 L_si: 0.0045 L_grad: 0.0229 
Train Epoch: 47 [108/816 (13%)] loss: 0.0272 L_si: 0.0042 L_grad: 0.0230 
Train Epoch: 47 [144/816 (18%)] loss: 0.0283 L_si: 0.0038 L_grad: 0.0244 
Train Epoch: 47 [180/816 (22%)] loss: 0.0260 L_si: 0.0026 L_grad: 0.0233 
Train Epoch: 47 [216/816 (26%)] loss: 0.0322 L_si: 0.0068 L_grad: 0.0254 
Train Epoch: 47 [252/816 (31%)] loss: 0.0346 L_si: 0.0077 L_grad: 0.0269 
Train Epoch: 47 [288/816 (35%)] loss: 0.0367 L_si: 0.0096 L_grad: 0.0271 
Train Epoch: 47 [324/816 (40%)] loss: 0.0307 L_si: 0.0059 L_grad: 0.0248 
Train Epoch: 47 [360/816 (44%)] loss: 0.0264 L_si: 0.0030 L_grad: 0.0235 
Train Epoch: 47 [396/816 (49%)] loss: 0.0236 L_si: 0.0028 L_grad: 0.0208 
Train Epoch: 47 [432/816 (53%)] loss: 0.0284 L_si: 0.0046 L_grad: 0.0238 
Train Epoch: 47 [468/816 (57%)] loss: 0.0294 L_si: 0.0051 L_grad: 0.0244 
Train Epoch: 47 [504/816 (62%)] loss: 0.0213 L_si: 0.0019 L_grad: 0.0194 
Train Epoch: 47 [540/816 (66%)] loss: 0.0216 L_si: 0.0018 L_grad: 0.0199 
Train Epoch: 47 [576/816 (71%)] loss: 0.0409 L_si: 0.0124 L_grad: 0.0285 
Train Epoch: 47 [612/816 (75%)] loss: 0.0281 L_si: 0.0042 L_grad: 0.0239 
Train Epoch: 47 [648/816 (79%)] loss: 0.0323 L_si: 0.0067 L_grad: 0.0257 
Train Epoch: 47 [684/816 (84%)] loss: 0.0266 L_si: 0.0046 L_grad: 0.0220 
Train Epoch: 47 [720/816 (88%)] loss: 0.0309 L_si: 0.0061 L_grad: 0.0248 
Train Epoch: 47 [756/816 (93%)] loss: 0.0337 L_si: 0.0081 L_grad: 0.0256 
Train Epoch: 47 [792/816 (97%)] loss: 0.0304 L_si: 0.0044 L_grad: 0.0259 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.0383988618850708, 0.039411406964063644, 0.04181545972824097, 0.04252350702881813, 0.03773142024874687, 0.038760263472795486, 0.037197988480329514, 0.03845253586769104, 0.0418250747025013, 0.037449441850185394, 0.03702055290341377, 0.04322715476155281, 0.03549344092607498, 0.03878249228000641, 0.04225727915763855, 0.04066794365644455, 0.03431539610028267, 0.04001103341579437, 0.016479356214404106], 'L_si': [0.005910925567150116, 0.007280092686414719, 0.007744083181023598, 0.007904108613729477, 0.00571640208363533, 0.00561017170548439, 0.006439372897148132, 0.005750695243477821, 0.007597059011459351, 0.0060169268399477005, 0.006150051020085812, 0.00819474458694458, 0.005334869027137756, 0.006260516121983528, 0.007116755470633507, 0.007172629237174988, 0.004143878817558289, 0.007517112419009209, 0.008525276556611061], 'L_grad': [0.032487936317920685, 0.032131314277648926, 0.03407137840986252, 0.034619398415088654, 0.03201501816511154, 0.033150091767311096, 0.03075861558318138, 0.03270184248685837, 0.034228015691041946, 0.03143251687288284, 0.030870502814650536, 0.03503241017460823, 0.030158573761582375, 0.03252197802066803, 0.035140521824359894, 0.03349531441926956, 0.03017151728272438, 0.032493919134140015, 0.007954079657793045]}
Train Epoch: 48 [0/816 (0%)] loss: 0.0290 L_si: 0.0036 L_grad: 0.0254 
Train Epoch: 48 [36/816 (4%)] loss: 0.0279 L_si: 0.0047 L_grad: 0.0232 
Train Epoch: 48 [72/816 (9%)] loss: 0.0267 L_si: 0.0036 L_grad: 0.0231 
Train Epoch: 48 [108/816 (13%)] loss: 0.0243 L_si: 0.0028 L_grad: 0.0215 
Train Epoch: 48 [144/816 (18%)] loss: 0.0260 L_si: 0.0042 L_grad: 0.0217 
Train Epoch: 48 [180/816 (22%)] loss: 0.0216 L_si: 0.0020 L_grad: 0.0196 
Train Epoch: 48 [216/816 (26%)] loss: 0.0219 L_si: 0.0024 L_grad: 0.0195 
Train Epoch: 48 [252/816 (31%)] loss: 0.0300 L_si: 0.0048 L_grad: 0.0252 
Train Epoch: 48 [288/816 (35%)] loss: 0.0304 L_si: 0.0049 L_grad: 0.0255 
Train Epoch: 48 [324/816 (40%)] loss: 0.0272 L_si: 0.0033 L_grad: 0.0239 
Train Epoch: 48 [360/816 (44%)] loss: 0.0356 L_si: 0.0066 L_grad: 0.0290 
Train Epoch: 48 [396/816 (49%)] loss: 0.0347 L_si: 0.0055 L_grad: 0.0291 
Train Epoch: 48 [432/816 (53%)] loss: 0.0305 L_si: 0.0072 L_grad: 0.0234 
Train Epoch: 48 [468/816 (57%)] loss: 0.0367 L_si: 0.0090 L_grad: 0.0278 
Train Epoch: 48 [504/816 (62%)] loss: 0.0364 L_si: 0.0077 L_grad: 0.0287 
Train Epoch: 48 [540/816 (66%)] loss: 0.0245 L_si: 0.0029 L_grad: 0.0217 
Train Epoch: 48 [576/816 (71%)] loss: 0.0276 L_si: 0.0050 L_grad: 0.0226 
Train Epoch: 48 [612/816 (75%)] loss: 0.0435 L_si: 0.0113 L_grad: 0.0322 
Train Epoch: 48 [648/816 (79%)] loss: 0.0248 L_si: 0.0036 L_grad: 0.0212 
Train Epoch: 48 [684/816 (84%)] loss: 0.0313 L_si: 0.0049 L_grad: 0.0264 
Train Epoch: 48 [720/816 (88%)] loss: 0.0293 L_si: 0.0042 L_grad: 0.0252 
Train Epoch: 48 [756/816 (93%)] loss: 0.0340 L_si: 0.0079 L_grad: 0.0261 
Train Epoch: 48 [792/816 (97%)] loss: 0.0342 L_si: 0.0068 L_grad: 0.0274 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving current best: model_best.pth.tar ...
all losses in batch in validation:  {'loss': [0.03767762333154678, 0.0403299555182457, 0.039908766746520996, 0.0395740382373333, 0.038199394941329956, 0.042495790868997574, 0.04182644933462143, 0.043981097638607025, 0.03882787004113197, 0.040978334844112396, 0.0370543971657753, 0.04092679172754288, 0.03636639937758446, 0.03589610382914543, 0.038207683712244034, 0.03809630125761032, 0.03802673518657684, 0.03140722215175629, 0.01184549555182457], 'L_si': [0.00639486126601696, 0.007070425897836685, 0.006917158141732216, 0.0067832618951797485, 0.006952578201889992, 0.008745897561311722, 0.007586883381009102, 0.008108822628855705, 0.006245691329240799, 0.006303304806351662, 0.004371650516986847, 0.006995752453804016, 0.005263259634375572, 0.005394573323428631, 0.006439518183469772, 0.00623532198369503, 0.005671186372637749, 0.003314887173473835, 0.0031627267599105835], 'L_grad': [0.031282760202884674, 0.03325952962040901, 0.03299160674214363, 0.03279077634215355, 0.031246814876794815, 0.03374989330768585, 0.03423956781625748, 0.03587227314710617, 0.032582178711891174, 0.034675031900405884, 0.03268274664878845, 0.03393103927373886, 0.031103139743208885, 0.030501529574394226, 0.03176816552877426, 0.03186097741127014, 0.032355550676584244, 0.028092335909605026, 0.008682768791913986]}
Train Epoch: 49 [0/816 (0%)] loss: 0.0263 L_si: 0.0034 L_grad: 0.0230 
Train Epoch: 49 [36/816 (4%)] loss: 0.0280 L_si: 0.0048 L_grad: 0.0231 
Train Epoch: 49 [72/816 (9%)] loss: 0.0382 L_si: 0.0125 L_grad: 0.0257 
Train Epoch: 49 [108/816 (13%)] loss: 0.0293 L_si: 0.0058 L_grad: 0.0235 
Train Epoch: 49 [144/816 (18%)] loss: 0.0302 L_si: 0.0062 L_grad: 0.0239 
Train Epoch: 49 [180/816 (22%)] loss: 0.0294 L_si: 0.0038 L_grad: 0.0256 
Train Epoch: 49 [216/816 (26%)] loss: 0.0295 L_si: 0.0044 L_grad: 0.0251 
Train Epoch: 49 [252/816 (31%)] loss: 0.0245 L_si: 0.0062 L_grad: 0.0182 
Train Epoch: 49 [288/816 (35%)] loss: 0.0369 L_si: 0.0088 L_grad: 0.0281 
Train Epoch: 49 [324/816 (40%)] loss: 0.0362 L_si: 0.0110 L_grad: 0.0252 
Train Epoch: 49 [360/816 (44%)] loss: 0.0327 L_si: 0.0072 L_grad: 0.0255 
Train Epoch: 49 [396/816 (49%)] loss: 0.0321 L_si: 0.0045 L_grad: 0.0276 
Train Epoch: 49 [432/816 (53%)] loss: 0.0349 L_si: 0.0061 L_grad: 0.0288 
Train Epoch: 49 [468/816 (57%)] loss: 0.0315 L_si: 0.0039 L_grad: 0.0276 
Train Epoch: 49 [504/816 (62%)] loss: 0.0246 L_si: 0.0035 L_grad: 0.0210 
Train Epoch: 49 [540/816 (66%)] loss: 0.0429 L_si: 0.0118 L_grad: 0.0311 
Train Epoch: 49 [576/816 (71%)] loss: 0.0342 L_si: 0.0078 L_grad: 0.0264 
Train Epoch: 49 [612/816 (75%)] loss: 0.0271 L_si: 0.0042 L_grad: 0.0229 
Train Epoch: 49 [648/816 (79%)] loss: 0.0280 L_si: 0.0033 L_grad: 0.0247 
Train Epoch: 49 [684/816 (84%)] loss: 0.0368 L_si: 0.0100 L_grad: 0.0269 
Train Epoch: 49 [720/816 (88%)] loss: 0.0297 L_si: 0.0043 L_grad: 0.0254 
Train Epoch: 49 [756/816 (93%)] loss: 0.0278 L_si: 0.0052 L_grad: 0.0226 
Train Epoch: 49 [792/816 (97%)] loss: 0.0333 L_si: 0.0055 L_grad: 0.0278 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04099993780255318, 0.043632395565509796, 0.041377171874046326, 0.04217575490474701, 0.04049815610051155, 0.04006486386060715, 0.04052214324474335, 0.04115039110183716, 0.041401710361242294, 0.04336366057395935, 0.04169812798500061, 0.04628942906856537, 0.04215768352150917, 0.03723403066396713, 0.045114029198884964, 0.044682372361421585, 0.0420171320438385, 0.04512675106525421, 0.01862151548266411], 'L_si': [0.007457658648490906, 0.009156260639429092, 0.0079222172498703, 0.009270275011658669, 0.008555438369512558, 0.007895683869719505, 0.008581200614571571, 0.007768241688609123, 0.009540930390357971, 0.008297059684991837, 0.008489316329360008, 0.010985879227519035, 0.009059417992830276, 0.006310317665338516, 0.009297940880060196, 0.008263133466243744, 0.007881570607423782, 0.009300407022237778, 0.009916257113218307], 'L_grad': [0.03354227915406227, 0.034476134926080704, 0.033454954624176025, 0.03290548175573349, 0.03194271773099899, 0.03216918185353279, 0.031940944492816925, 0.033382147550582886, 0.03186077997088432, 0.035066600888967514, 0.03320880979299545, 0.035303547978401184, 0.033098265528678894, 0.030923712998628616, 0.03581608831882477, 0.03641923889517784, 0.03413556143641472, 0.035826344043016434, 0.0087052583694458]}
Train Epoch: 50 [0/816 (0%)] loss: 0.0366 L_si: 0.0081 L_grad: 0.0285 
Train Epoch: 50 [36/816 (4%)] loss: 0.0402 L_si: 0.0107 L_grad: 0.0295 
Train Epoch: 50 [72/816 (9%)] loss: 0.0343 L_si: 0.0061 L_grad: 0.0282 
Train Epoch: 50 [108/816 (13%)] loss: 0.0286 L_si: 0.0049 L_grad: 0.0237 
Train Epoch: 50 [144/816 (18%)] loss: 0.0466 L_si: 0.0144 L_grad: 0.0323 
Train Epoch: 50 [180/816 (22%)] loss: 0.0296 L_si: 0.0039 L_grad: 0.0256 
Train Epoch: 50 [216/816 (26%)] loss: 0.0332 L_si: 0.0058 L_grad: 0.0274 
Train Epoch: 50 [252/816 (31%)] loss: 0.0309 L_si: 0.0042 L_grad: 0.0267 
Train Epoch: 50 [288/816 (35%)] loss: 0.0359 L_si: 0.0063 L_grad: 0.0296 
Train Epoch: 50 [324/816 (40%)] loss: 0.0307 L_si: 0.0050 L_grad: 0.0257 
Train Epoch: 50 [360/816 (44%)] loss: 0.0289 L_si: 0.0054 L_grad: 0.0235 
Train Epoch: 50 [396/816 (49%)] loss: 0.0326 L_si: 0.0078 L_grad: 0.0248 
Train Epoch: 50 [432/816 (53%)] loss: 0.0239 L_si: 0.0028 L_grad: 0.0211 
Train Epoch: 50 [468/816 (57%)] loss: 0.0280 L_si: 0.0050 L_grad: 0.0230 
Train Epoch: 50 [504/816 (62%)] loss: 0.0282 L_si: 0.0037 L_grad: 0.0245 
Train Epoch: 50 [540/816 (66%)] loss: 0.0347 L_si: 0.0064 L_grad: 0.0283 
Train Epoch: 50 [576/816 (71%)] loss: 0.0240 L_si: 0.0028 L_grad: 0.0213 
Train Epoch: 50 [612/816 (75%)] loss: 0.0278 L_si: 0.0047 L_grad: 0.0231 
Train Epoch: 50 [648/816 (79%)] loss: 0.0217 L_si: 0.0020 L_grad: 0.0197 
Train Epoch: 50 [684/816 (84%)] loss: 0.0309 L_si: 0.0047 L_grad: 0.0262 
Train Epoch: 50 [720/816 (88%)] loss: 0.0279 L_si: 0.0043 L_grad: 0.0236 
Train Epoch: 50 [756/816 (93%)] loss: 0.0246 L_si: 0.0028 L_grad: 0.0218 
Train Epoch: 50 [792/816 (97%)] loss: 0.0196 L_si: 0.0014 L_grad: 0.0182 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving checkpoint: /root/autodl-tmp/train_il_200e/train_s2d_SpikeTransformer/checkpoint-epoch050-loss-0.0307.pth.tar ...
New Learning Rate: 0.000150
all losses in batch in validation:  {'loss': [0.03725893050432205, 0.03786157816648483, 0.03695587068796158, 0.042830970138311386, 0.04139552637934685, 0.04133899509906769, 0.04018902778625488, 0.03876535966992378, 0.04101402312517166, 0.04252711683511734, 0.038712598383426666, 0.04053990915417671, 0.037742942571640015, 0.04036295413970947, 0.0439411923289299, 0.04278501495718956, 0.04079582169651985, 0.04395001381635666, 0.013313578441739082], 'L_si': [0.007998881861567497, 0.007447080686688423, 0.007557237520813942, 0.00814574584364891, 0.006791211664676666, 0.0078000836074352264, 0.008213920518755913, 0.007668189704418182, 0.009005000814795494, 0.008512748405337334, 0.007971489802002907, 0.00794050469994545, 0.007060067728161812, 0.008472355082631111, 0.008818518370389938, 0.0080692358314991, 0.00814555212855339, 0.008717909455299377, 0.0040304996073246], 'L_grad': [0.029260046780109406, 0.03041449561715126, 0.029398631304502487, 0.034685224294662476, 0.03460431471467018, 0.03353891149163246, 0.03197510913014412, 0.0310971699655056, 0.03200902044773102, 0.034014370292425156, 0.03074111044406891, 0.03259940445423126, 0.030682874843478203, 0.03189060091972351, 0.03512267395853996, 0.03471577912569046, 0.03265026956796646, 0.03523210436105728, 0.009283078834414482]}
Train Epoch: 51 [0/816 (0%)] loss: 0.0268 L_si: 0.0038 L_grad: 0.0230 
Train Epoch: 51 [36/816 (4%)] loss: 0.0359 L_si: 0.0084 L_grad: 0.0275 
Train Epoch: 51 [72/816 (9%)] loss: 0.0249 L_si: 0.0031 L_grad: 0.0218 
Train Epoch: 51 [108/816 (13%)] loss: 0.0309 L_si: 0.0056 L_grad: 0.0253 
Train Epoch: 51 [144/816 (18%)] loss: 0.0364 L_si: 0.0087 L_grad: 0.0278 
Train Epoch: 51 [180/816 (22%)] loss: 0.0310 L_si: 0.0065 L_grad: 0.0244 
Train Epoch: 51 [216/816 (26%)] loss: 0.0201 L_si: 0.0022 L_grad: 0.0179 
Train Epoch: 51 [252/816 (31%)] loss: 0.0314 L_si: 0.0068 L_grad: 0.0246 
Train Epoch: 51 [288/816 (35%)] loss: 0.0310 L_si: 0.0086 L_grad: 0.0223 
Train Epoch: 51 [324/816 (40%)] loss: 0.0347 L_si: 0.0051 L_grad: 0.0296 
Train Epoch: 51 [360/816 (44%)] loss: 0.0368 L_si: 0.0116 L_grad: 0.0253 
Train Epoch: 51 [396/816 (49%)] loss: 0.0309 L_si: 0.0060 L_grad: 0.0250 
Train Epoch: 51 [432/816 (53%)] loss: 0.0235 L_si: 0.0026 L_grad: 0.0209 
Train Epoch: 51 [468/816 (57%)] loss: 0.0245 L_si: 0.0026 L_grad: 0.0218 
Train Epoch: 51 [504/816 (62%)] loss: 0.0252 L_si: 0.0041 L_grad: 0.0211 
Train Epoch: 51 [540/816 (66%)] loss: 0.0324 L_si: 0.0090 L_grad: 0.0233 
Train Epoch: 51 [576/816 (71%)] loss: 0.0245 L_si: 0.0033 L_grad: 0.0212 
Train Epoch: 51 [612/816 (75%)] loss: 0.0325 L_si: 0.0062 L_grad: 0.0263 
Train Epoch: 51 [648/816 (79%)] loss: 0.0425 L_si: 0.0127 L_grad: 0.0298 
Train Epoch: 51 [684/816 (84%)] loss: 0.0317 L_si: 0.0062 L_grad: 0.0254 
Train Epoch: 51 [720/816 (88%)] loss: 0.0354 L_si: 0.0092 L_grad: 0.0262 
Train Epoch: 51 [756/816 (93%)] loss: 0.0261 L_si: 0.0026 L_grad: 0.0235 
Train Epoch: 51 [792/816 (97%)] loss: 0.0296 L_si: 0.0057 L_grad: 0.0239 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.0392603874206543, 0.0409921333193779, 0.04527871310710907, 0.03937000036239624, 0.041413817554712296, 0.04262647032737732, 0.04007679596543312, 0.04082879051566124, 0.042110685259103775, 0.04020388051867485, 0.041844770312309265, 0.038741253316402435, 0.04033312574028969, 0.042879074811935425, 0.041514236479997635, 0.03938843682408333, 0.039490096271038055, 0.04240758717060089, 0.018900565803050995], 'L_si': [0.008137337863445282, 0.009296458214521408, 0.009968124330043793, 0.007295330986380577, 0.009288627654314041, 0.008273974061012268, 0.008627418428659439, 0.009384024888277054, 0.00921536609530449, 0.009142082184553146, 0.009426139295101166, 0.00760483555495739, 0.008030377328395844, 0.010624270886182785, 0.009324800223112106, 0.008861221373081207, 0.008417239412665367, 0.00836990773677826, 0.010489063337445259], 'L_grad': [0.031123047694563866, 0.03169567510485649, 0.03531058877706528, 0.032074667513370514, 0.032125189900398254, 0.03435249626636505, 0.03144937753677368, 0.031444765627384186, 0.032895319163799286, 0.031061798334121704, 0.0324186310172081, 0.031136415898799896, 0.032302748411893845, 0.03225480392575264, 0.03218943625688553, 0.03052721545100212, 0.031072858721017838, 0.03403767943382263, 0.008411501534283161]}
Train Epoch: 52 [0/816 (0%)] loss: 0.0277 L_si: 0.0077 L_grad: 0.0200 
Train Epoch: 52 [36/816 (4%)] loss: 0.0254 L_si: 0.0030 L_grad: 0.0224 
Train Epoch: 52 [72/816 (9%)] loss: 0.0409 L_si: 0.0079 L_grad: 0.0330 
Train Epoch: 52 [108/816 (13%)] loss: 0.0346 L_si: 0.0080 L_grad: 0.0266 
Train Epoch: 52 [144/816 (18%)] loss: 0.0288 L_si: 0.0038 L_grad: 0.0250 
Train Epoch: 52 [180/816 (22%)] loss: 0.0232 L_si: 0.0027 L_grad: 0.0206 
Train Epoch: 52 [216/816 (26%)] loss: 0.0348 L_si: 0.0069 L_grad: 0.0279 
Train Epoch: 52 [252/816 (31%)] loss: 0.0290 L_si: 0.0055 L_grad: 0.0235 
Train Epoch: 52 [288/816 (35%)] loss: 0.0295 L_si: 0.0053 L_grad: 0.0242 
Train Epoch: 52 [324/816 (40%)] loss: 0.0321 L_si: 0.0053 L_grad: 0.0268 
Train Epoch: 52 [360/816 (44%)] loss: 0.0259 L_si: 0.0033 L_grad: 0.0226 
Train Epoch: 52 [396/816 (49%)] loss: 0.0282 L_si: 0.0075 L_grad: 0.0207 
Train Epoch: 52 [432/816 (53%)] loss: 0.0273 L_si: 0.0039 L_grad: 0.0234 
Train Epoch: 52 [468/816 (57%)] loss: 0.0281 L_si: 0.0046 L_grad: 0.0235 
Train Epoch: 52 [504/816 (62%)] loss: 0.0285 L_si: 0.0039 L_grad: 0.0246 
Train Epoch: 52 [540/816 (66%)] loss: 0.0231 L_si: 0.0024 L_grad: 0.0208 
Train Epoch: 52 [576/816 (71%)] loss: 0.0350 L_si: 0.0088 L_grad: 0.0262 
Train Epoch: 52 [612/816 (75%)] loss: 0.0269 L_si: 0.0048 L_grad: 0.0220 
Train Epoch: 52 [648/816 (79%)] loss: 0.0359 L_si: 0.0087 L_grad: 0.0271 
Train Epoch: 52 [684/816 (84%)] loss: 0.0280 L_si: 0.0043 L_grad: 0.0238 
Train Epoch: 52 [720/816 (88%)] loss: 0.0241 L_si: 0.0026 L_grad: 0.0215 
Train Epoch: 52 [756/816 (93%)] loss: 0.0242 L_si: 0.0033 L_grad: 0.0209 
Train Epoch: 52 [792/816 (97%)] loss: 0.0294 L_si: 0.0056 L_grad: 0.0238 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.042823825031518936, 0.0432291105389595, 0.039648305624723434, 0.04278157278895378, 0.04202685505151749, 0.04134232550859451, 0.03900860249996185, 0.04261034354567528, 0.041695378720760345, 0.03904952108860016, 0.04159065708518028, 0.04087323322892189, 0.04054832085967064, 0.044279590249061584, 0.04288025572896004, 0.039575979113578796, 0.042642440646886826, 0.043591927736997604, 0.014666998758912086], 'L_si': [0.008389964699745178, 0.009780291467905045, 0.008196406066417694, 0.008817929774522781, 0.008601529523730278, 0.006507173180580139, 0.008885152637958527, 0.008616607636213303, 0.008090415969491005, 0.0079283956438303, 0.008156705647706985, 0.008157927542924881, 0.007742725312709808, 0.010500244796276093, 0.007753036916255951, 0.007968902587890625, 0.0090714730322361, 0.009274430572986603, 0.006558947265148163], 'L_grad': [0.03443386033177376, 0.03344881907105446, 0.03145189955830574, 0.033963643014431, 0.03342532739043236, 0.034835152328014374, 0.030123447999358177, 0.033993735909461975, 0.03360496461391449, 0.03112112730741501, 0.0334339514374733, 0.03271530568599701, 0.03280559554696083, 0.03377934545278549, 0.035127218812704086, 0.03160707652568817, 0.033570967614650726, 0.034317497164011, 0.008108051493763924]}
Train Epoch: 53 [0/816 (0%)] loss: 0.0284 L_si: 0.0053 L_grad: 0.0231 
Train Epoch: 53 [36/816 (4%)] loss: 0.0217 L_si: 0.0022 L_grad: 0.0195 
Train Epoch: 53 [72/816 (9%)] loss: 0.0292 L_si: 0.0041 L_grad: 0.0252 
Train Epoch: 53 [108/816 (13%)] loss: 0.0275 L_si: 0.0075 L_grad: 0.0200 
Train Epoch: 53 [144/816 (18%)] loss: 0.0397 L_si: 0.0115 L_grad: 0.0282 
Train Epoch: 53 [180/816 (22%)] loss: 0.0287 L_si: 0.0043 L_grad: 0.0244 
Train Epoch: 53 [216/816 (26%)] loss: 0.0308 L_si: 0.0044 L_grad: 0.0264 
Train Epoch: 53 [252/816 (31%)] loss: 0.0305 L_si: 0.0068 L_grad: 0.0238 
Train Epoch: 53 [288/816 (35%)] loss: 0.0224 L_si: 0.0028 L_grad: 0.0197 
Train Epoch: 53 [324/816 (40%)] loss: 0.0278 L_si: 0.0028 L_grad: 0.0251 
Train Epoch: 53 [360/816 (44%)] loss: 0.0290 L_si: 0.0037 L_grad: 0.0253 
Train Epoch: 53 [396/816 (49%)] loss: 0.0272 L_si: 0.0041 L_grad: 0.0230 
Train Epoch: 53 [432/816 (53%)] loss: 0.0331 L_si: 0.0062 L_grad: 0.0269 
Train Epoch: 53 [468/816 (57%)] loss: 0.0235 L_si: 0.0025 L_grad: 0.0210 
Train Epoch: 53 [504/816 (62%)] loss: 0.0195 L_si: 0.0013 L_grad: 0.0182 
Train Epoch: 53 [540/816 (66%)] loss: 0.0326 L_si: 0.0063 L_grad: 0.0263 
Train Epoch: 53 [576/816 (71%)] loss: 0.0310 L_si: 0.0036 L_grad: 0.0274 
Train Epoch: 53 [612/816 (75%)] loss: 0.0233 L_si: 0.0040 L_grad: 0.0193 
Train Epoch: 53 [648/816 (79%)] loss: 0.0267 L_si: 0.0034 L_grad: 0.0233 
Train Epoch: 53 [684/816 (84%)] loss: 0.0416 L_si: 0.0079 L_grad: 0.0337 
Train Epoch: 53 [720/816 (88%)] loss: 0.0333 L_si: 0.0087 L_grad: 0.0246 
Train Epoch: 53 [756/816 (93%)] loss: 0.0261 L_si: 0.0035 L_grad: 0.0226 
Train Epoch: 53 [792/816 (97%)] loss: 0.0254 L_si: 0.0029 L_grad: 0.0225 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04143301770091057, 0.040230974555015564, 0.03919942304491997, 0.04142744094133377, 0.0374557301402092, 0.04036791995167732, 0.035146985203027725, 0.04232129827141762, 0.040278512984514236, 0.040432363748550415, 0.0437924787402153, 0.03915414959192276, 0.04048331826925278, 0.04195360094308853, 0.040249742567539215, 0.03617795556783676, 0.04065905138850212, 0.03871332108974457, 0.014472979120910168], 'L_si': [0.009085923433303833, 0.007116135209798813, 0.007952716201543808, 0.007200703024864197, 0.0065656378865242004, 0.007240679115056992, 0.005364127457141876, 0.007478531450033188, 0.00888724997639656, 0.008240267634391785, 0.008374180644750595, 0.007296482101082802, 0.007583338767290115, 0.008496824651956558, 0.00830802135169506, 0.006600614637136459, 0.008196953684091568, 0.006553845480084419, 0.005930867046117783], 'L_grad': [0.032347094267606735, 0.03311483934521675, 0.03124670684337616, 0.034226737916469574, 0.03089009039103985, 0.03312724083662033, 0.02978285774588585, 0.03484276682138443, 0.031391263008117676, 0.03219209611415863, 0.035418298095464706, 0.03185766935348511, 0.03289997950196266, 0.03345677629113197, 0.031941719353199005, 0.029577339068055153, 0.03246209770441055, 0.0321594774723053, 0.008542112074792385]}
Train Epoch: 54 [0/816 (0%)] loss: 0.0285 L_si: 0.0080 L_grad: 0.0204 
Train Epoch: 54 [36/816 (4%)] loss: 0.0309 L_si: 0.0079 L_grad: 0.0229 
Train Epoch: 54 [72/816 (9%)] loss: 0.0355 L_si: 0.0063 L_grad: 0.0292 
Train Epoch: 54 [108/816 (13%)] loss: 0.0255 L_si: 0.0030 L_grad: 0.0225 
Train Epoch: 54 [144/816 (18%)] loss: 0.0339 L_si: 0.0057 L_grad: 0.0283 
Train Epoch: 54 [180/816 (22%)] loss: 0.0392 L_si: 0.0075 L_grad: 0.0316 
Train Epoch: 54 [216/816 (26%)] loss: 0.0237 L_si: 0.0022 L_grad: 0.0215 
Train Epoch: 54 [252/816 (31%)] loss: 0.0268 L_si: 0.0060 L_grad: 0.0209 
Train Epoch: 54 [288/816 (35%)] loss: 0.0217 L_si: 0.0027 L_grad: 0.0191 
Train Epoch: 54 [324/816 (40%)] loss: 0.0209 L_si: 0.0019 L_grad: 0.0191 
Train Epoch: 54 [360/816 (44%)] loss: 0.0263 L_si: 0.0044 L_grad: 0.0220 
Train Epoch: 54 [396/816 (49%)] loss: 0.0259 L_si: 0.0033 L_grad: 0.0226 
Train Epoch: 54 [432/816 (53%)] loss: 0.0331 L_si: 0.0042 L_grad: 0.0289 
Train Epoch: 54 [468/816 (57%)] loss: 0.0279 L_si: 0.0042 L_grad: 0.0237 
Train Epoch: 54 [504/816 (62%)] loss: 0.0241 L_si: 0.0029 L_grad: 0.0212 
Train Epoch: 54 [540/816 (66%)] loss: 0.0242 L_si: 0.0039 L_grad: 0.0202 
Train Epoch: 54 [576/816 (71%)] loss: 0.0330 L_si: 0.0055 L_grad: 0.0275 
Train Epoch: 54 [612/816 (75%)] loss: 0.0288 L_si: 0.0052 L_grad: 0.0236 
Train Epoch: 54 [648/816 (79%)] loss: 0.0236 L_si: 0.0022 L_grad: 0.0214 
Train Epoch: 54 [684/816 (84%)] loss: 0.0307 L_si: 0.0070 L_grad: 0.0237 
Train Epoch: 54 [720/816 (88%)] loss: 0.0299 L_si: 0.0072 L_grad: 0.0227 
Train Epoch: 54 [756/816 (93%)] loss: 0.0426 L_si: 0.0102 L_grad: 0.0324 
Train Epoch: 54 [792/816 (97%)] loss: 0.0270 L_si: 0.0035 L_grad: 0.0234 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.03788620978593826, 0.042008452117443085, 0.04463572800159454, 0.03930078446865082, 0.040892813354730606, 0.04109737277030945, 0.04243096709251404, 0.03881653770804405, 0.04217780381441116, 0.04148188233375549, 0.04017246142029762, 0.03722170367836952, 0.038941483944654465, 0.03736083209514618, 0.0393960177898407, 0.040230412036180496, 0.03784193843603134, 0.04269848018884659, 0.017864122986793518], 'L_si': [0.006605574861168861, 0.007544230669736862, 0.008946463465690613, 0.0072664786130189896, 0.006264224648475647, 0.007356537505984306, 0.007831200957298279, 0.0073297880589962006, 0.00822489894926548, 0.007959358394145966, 0.008258383721113205, 0.006692033261060715, 0.007134374231100082, 0.006428420543670654, 0.007084699347615242, 0.0071783773601055145, 0.006806524470448494, 0.00876314751803875, 0.00908726267516613], 'L_grad': [0.03128063306212425, 0.03446422144770622, 0.03568926453590393, 0.03203430771827698, 0.03462858870625496, 0.03374083340167999, 0.03459976613521576, 0.03148674964904785, 0.03395290672779083, 0.03352252393960953, 0.03191407769918442, 0.030529670417308807, 0.03180710971355438, 0.030932409688830376, 0.032311320304870605, 0.03305203467607498, 0.031035415828227997, 0.03393533453345299, 0.008776861242949963]}
Train Epoch: 55 [0/816 (0%)] loss: 0.0384 L_si: 0.0070 L_grad: 0.0314 
Train Epoch: 55 [36/816 (4%)] loss: 0.0218 L_si: 0.0021 L_grad: 0.0197 
Train Epoch: 55 [72/816 (9%)] loss: 0.0270 L_si: 0.0039 L_grad: 0.0231 
Train Epoch: 55 [108/816 (13%)] loss: 0.0233 L_si: 0.0025 L_grad: 0.0208 
Train Epoch: 55 [144/816 (18%)] loss: 0.0243 L_si: 0.0035 L_grad: 0.0208 
Train Epoch: 55 [180/816 (22%)] loss: 0.0309 L_si: 0.0061 L_grad: 0.0248 
Train Epoch: 55 [216/816 (26%)] loss: 0.0275 L_si: 0.0040 L_grad: 0.0236 
Train Epoch: 55 [252/816 (31%)] loss: 0.0262 L_si: 0.0037 L_grad: 0.0226 
Train Epoch: 55 [288/816 (35%)] loss: 0.0280 L_si: 0.0063 L_grad: 0.0217 
Train Epoch: 55 [324/816 (40%)] loss: 0.0257 L_si: 0.0037 L_grad: 0.0220 
Train Epoch: 55 [360/816 (44%)] loss: 0.0339 L_si: 0.0106 L_grad: 0.0232 
Train Epoch: 55 [396/816 (49%)] loss: 0.0319 L_si: 0.0076 L_grad: 0.0243 
Train Epoch: 55 [432/816 (53%)] loss: 0.0244 L_si: 0.0033 L_grad: 0.0211 
Train Epoch: 55 [468/816 (57%)] loss: 0.0309 L_si: 0.0053 L_grad: 0.0256 
Train Epoch: 55 [504/816 (62%)] loss: 0.0406 L_si: 0.0081 L_grad: 0.0324 
Train Epoch: 55 [540/816 (66%)] loss: 0.0236 L_si: 0.0028 L_grad: 0.0208 
Train Epoch: 55 [576/816 (71%)] loss: 0.0296 L_si: 0.0046 L_grad: 0.0250 
Train Epoch: 55 [612/816 (75%)] loss: 0.0353 L_si: 0.0059 L_grad: 0.0294 
Train Epoch: 55 [648/816 (79%)] loss: 0.0296 L_si: 0.0060 L_grad: 0.0236 
Train Epoch: 55 [684/816 (84%)] loss: 0.0418 L_si: 0.0133 L_grad: 0.0285 
Train Epoch: 55 [720/816 (88%)] loss: 0.0298 L_si: 0.0049 L_grad: 0.0249 
Train Epoch: 55 [756/816 (93%)] loss: 0.0299 L_si: 0.0054 L_grad: 0.0245 
Train Epoch: 55 [792/816 (97%)] loss: 0.0277 L_si: 0.0051 L_grad: 0.0226 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.042057622224092484, 0.039255399256944656, 0.040908679366111755, 0.038065336644649506, 0.039207760244607925, 0.04015970230102539, 0.042083337903022766, 0.036866895854473114, 0.03875306248664856, 0.03942696750164032, 0.04018164053559303, 0.042690351605415344, 0.03817807510495186, 0.04028360918164253, 0.04228069633245468, 0.04074668139219284, 0.041012778878211975, 0.03935797140002251, 0.018359242007136345], 'L_si': [0.007868114858865738, 0.006809670478105545, 0.006914572790265083, 0.006403714418411255, 0.006977051496505737, 0.006781848147511482, 0.007717447355389595, 0.006212951615452766, 0.006142947822809219, 0.00717204250395298, 0.007280893623828888, 0.0068255215883255005, 0.006603796035051346, 0.006846025586128235, 0.008181164041161537, 0.008276477456092834, 0.007985467091202736, 0.007790490984916687, 0.009761204943060875], 'L_grad': [0.034189507365226746, 0.03244572877883911, 0.03399410843849182, 0.03166162222623825, 0.03223070874810219, 0.03337785601615906, 0.03436589241027832, 0.030653946101665497, 0.03261011466383934, 0.03225492686033249, 0.032900746911764145, 0.035864830017089844, 0.03157427906990051, 0.0334375835955143, 0.034099530428647995, 0.032470203936100006, 0.03302731364965439, 0.03156748041510582, 0.00859803706407547]}
Train Epoch: 56 [0/816 (0%)] loss: 0.0344 L_si: 0.0068 L_grad: 0.0275 
Train Epoch: 56 [36/816 (4%)] loss: 0.0323 L_si: 0.0081 L_grad: 0.0242 
Train Epoch: 56 [72/816 (9%)] loss: 0.0244 L_si: 0.0031 L_grad: 0.0213 
Train Epoch: 56 [108/816 (13%)] loss: 0.0296 L_si: 0.0042 L_grad: 0.0253 
Train Epoch: 56 [144/816 (18%)] loss: 0.0283 L_si: 0.0049 L_grad: 0.0234 
Train Epoch: 56 [180/816 (22%)] loss: 0.0257 L_si: 0.0032 L_grad: 0.0225 
Train Epoch: 56 [216/816 (26%)] loss: 0.0243 L_si: 0.0028 L_grad: 0.0215 
Train Epoch: 56 [252/816 (31%)] loss: 0.0289 L_si: 0.0043 L_grad: 0.0246 
Train Epoch: 56 [288/816 (35%)] loss: 0.0231 L_si: 0.0026 L_grad: 0.0205 
Train Epoch: 56 [324/816 (40%)] loss: 0.0238 L_si: 0.0031 L_grad: 0.0208 
Train Epoch: 56 [360/816 (44%)] loss: 0.0235 L_si: 0.0019 L_grad: 0.0216 
Train Epoch: 56 [396/816 (49%)] loss: 0.0292 L_si: 0.0044 L_grad: 0.0248 
Train Epoch: 56 [432/816 (53%)] loss: 0.0239 L_si: 0.0036 L_grad: 0.0204 
Train Epoch: 56 [468/816 (57%)] loss: 0.0273 L_si: 0.0030 L_grad: 0.0243 
Train Epoch: 56 [504/816 (62%)] loss: 0.0312 L_si: 0.0050 L_grad: 0.0262 
Train Epoch: 56 [540/816 (66%)] loss: 0.0294 L_si: 0.0050 L_grad: 0.0244 
Train Epoch: 56 [576/816 (71%)] loss: 0.0318 L_si: 0.0071 L_grad: 0.0246 
Train Epoch: 56 [612/816 (75%)] loss: 0.0252 L_si: 0.0033 L_grad: 0.0219 
Train Epoch: 56 [648/816 (79%)] loss: 0.0177 L_si: 0.0015 L_grad: 0.0162 
Train Epoch: 56 [684/816 (84%)] loss: 0.0286 L_si: 0.0063 L_grad: 0.0223 
Train Epoch: 56 [720/816 (88%)] loss: 0.0235 L_si: 0.0029 L_grad: 0.0206 
Train Epoch: 56 [756/816 (93%)] loss: 0.0306 L_si: 0.0056 L_grad: 0.0250 
Train Epoch: 56 [792/816 (97%)] loss: 0.0231 L_si: 0.0046 L_grad: 0.0185 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04417255520820618, 0.04422350972890854, 0.045118194073438644, 0.039914604276418686, 0.03815801441669464, 0.04430834949016571, 0.03813277184963226, 0.04482785612344742, 0.03905811905860901, 0.04455418139696121, 0.0377596840262413, 0.04187723994255066, 0.040080830454826355, 0.04147573933005333, 0.04086832329630852, 0.04443744942545891, 0.04382502660155296, 0.043789032846689224, 0.01615125685930252], 'L_si': [0.009531818330287933, 0.00939943641424179, 0.01047586277127266, 0.008044883608818054, 0.006848173215985298, 0.01063726656138897, 0.007958609610795975, 0.009138993918895721, 0.007420502603054047, 0.01032019779086113, 0.006512301042675972, 0.009370297193527222, 0.008338931947946548, 0.008349165320396423, 0.00876222550868988, 0.00991109386086464, 0.00902823731303215, 0.010045338422060013, 0.00821664184331894], 'L_grad': [0.03464073687791824, 0.03482407331466675, 0.034642331302165985, 0.03186972066760063, 0.03130984306335449, 0.03367108106613159, 0.030174164101481438, 0.0356888622045517, 0.03163761645555496, 0.03423398360610008, 0.03124738112092018, 0.03250694274902344, 0.031741898506879807, 0.033126574009656906, 0.03210609778761864, 0.03452635556459427, 0.03479678928852081, 0.03374369442462921, 0.007934615015983582]}
Train Epoch: 57 [0/816 (0%)] loss: 0.0298 L_si: 0.0054 L_grad: 0.0244 
Train Epoch: 57 [36/816 (4%)] loss: 0.0305 L_si: 0.0094 L_grad: 0.0211 
Train Epoch: 57 [72/816 (9%)] loss: 0.0326 L_si: 0.0063 L_grad: 0.0263 
Train Epoch: 57 [108/816 (13%)] loss: 0.0258 L_si: 0.0036 L_grad: 0.0222 
Train Epoch: 57 [144/816 (18%)] loss: 0.0273 L_si: 0.0037 L_grad: 0.0236 
Train Epoch: 57 [180/816 (22%)] loss: 0.0376 L_si: 0.0111 L_grad: 0.0265 
Train Epoch: 57 [216/816 (26%)] loss: 0.0242 L_si: 0.0031 L_grad: 0.0211 
Train Epoch: 57 [252/816 (31%)] loss: 0.0240 L_si: 0.0030 L_grad: 0.0210 
Train Epoch: 57 [288/816 (35%)] loss: 0.0279 L_si: 0.0041 L_grad: 0.0238 
Train Epoch: 57 [324/816 (40%)] loss: 0.0287 L_si: 0.0043 L_grad: 0.0244 
Train Epoch: 57 [360/816 (44%)] loss: 0.0231 L_si: 0.0032 L_grad: 0.0199 
Train Epoch: 57 [396/816 (49%)] loss: 0.0294 L_si: 0.0041 L_grad: 0.0253 
Train Epoch: 57 [432/816 (53%)] loss: 0.0252 L_si: 0.0036 L_grad: 0.0216 
Train Epoch: 57 [468/816 (57%)] loss: 0.0405 L_si: 0.0083 L_grad: 0.0322 
Train Epoch: 57 [504/816 (62%)] loss: 0.0300 L_si: 0.0057 L_grad: 0.0243 
Train Epoch: 57 [540/816 (66%)] loss: 0.0195 L_si: 0.0019 L_grad: 0.0176 
Train Epoch: 57 [576/816 (71%)] loss: 0.0245 L_si: 0.0027 L_grad: 0.0218 
Train Epoch: 57 [612/816 (75%)] loss: 0.0365 L_si: 0.0089 L_grad: 0.0276 
Train Epoch: 57 [648/816 (79%)] loss: 0.0226 L_si: 0.0032 L_grad: 0.0194 
Train Epoch: 57 [684/816 (84%)] loss: 0.0280 L_si: 0.0041 L_grad: 0.0239 
Train Epoch: 57 [720/816 (88%)] loss: 0.0258 L_si: 0.0045 L_grad: 0.0213 
Train Epoch: 57 [756/816 (93%)] loss: 0.0303 L_si: 0.0047 L_grad: 0.0256 
Train Epoch: 57 [792/816 (97%)] loss: 0.0276 L_si: 0.0041 L_grad: 0.0235 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.037523843348026276, 0.043384354561567307, 0.0457075797021389, 0.03725779801607132, 0.04189252108335495, 0.03899792954325676, 0.04435649886727333, 0.03681227192282677, 0.04358842223882675, 0.03903820365667343, 0.04186565428972244, 0.040780652314424515, 0.04050947725772858, 0.03992651402950287, 0.040431492030620575, 0.041744112968444824, 0.03925377503037453, 0.038341447710990906, 0.015837404876947403], 'L_si': [0.0074915289878845215, 0.00859403982758522, 0.00923195481300354, 0.0077938176691532135, 0.00823850929737091, 0.007521606981754303, 0.009605105966329575, 0.0071314312517642975, 0.008728565648198128, 0.007299609482288361, 0.00807257741689682, 0.007182277739048004, 0.008106457069516182, 0.007782364264130592, 0.008760577067732811, 0.008597595617175102, 0.007752422243356705, 0.00791092962026596, 0.007789064198732376], 'L_grad': [0.030032314360141754, 0.034790314733982086, 0.03647562488913536, 0.029463980346918106, 0.03365401178598404, 0.03147632256150246, 0.034751392900943756, 0.02968084067106247, 0.03485985845327377, 0.03173859417438507, 0.03379307687282562, 0.03359837457537651, 0.032403022050857544, 0.032144151628017426, 0.03167091682553291, 0.03314651548862457, 0.03150135278701782, 0.030430516228079796, 0.008048340678215027]}
Train Epoch: 58 [0/816 (0%)] loss: 0.0236 L_si: 0.0029 L_grad: 0.0207 
Train Epoch: 58 [36/816 (4%)] loss: 0.0300 L_si: 0.0058 L_grad: 0.0242 
Train Epoch: 58 [72/816 (9%)] loss: 0.0268 L_si: 0.0042 L_grad: 0.0227 
Train Epoch: 58 [108/816 (13%)] loss: 0.0244 L_si: 0.0034 L_grad: 0.0209 
Train Epoch: 58 [144/816 (18%)] loss: 0.0254 L_si: 0.0047 L_grad: 0.0207 
Train Epoch: 58 [180/816 (22%)] loss: 0.0257 L_si: 0.0034 L_grad: 0.0224 
Train Epoch: 58 [216/816 (26%)] loss: 0.0255 L_si: 0.0027 L_grad: 0.0228 
Train Epoch: 58 [252/816 (31%)] loss: 0.0295 L_si: 0.0084 L_grad: 0.0212 
Train Epoch: 58 [288/816 (35%)] loss: 0.0204 L_si: 0.0016 L_grad: 0.0188 
Train Epoch: 58 [324/816 (40%)] loss: 0.0315 L_si: 0.0062 L_grad: 0.0253 
Train Epoch: 58 [360/816 (44%)] loss: 0.0312 L_si: 0.0053 L_grad: 0.0259 
Train Epoch: 58 [396/816 (49%)] loss: 0.0356 L_si: 0.0084 L_grad: 0.0272 
Train Epoch: 58 [432/816 (53%)] loss: 0.0296 L_si: 0.0062 L_grad: 0.0234 
Train Epoch: 58 [468/816 (57%)] loss: 0.0358 L_si: 0.0065 L_grad: 0.0293 
Train Epoch: 58 [504/816 (62%)] loss: 0.0267 L_si: 0.0053 L_grad: 0.0214 
Train Epoch: 58 [540/816 (66%)] loss: 0.0285 L_si: 0.0045 L_grad: 0.0240 
Train Epoch: 58 [576/816 (71%)] loss: 0.0260 L_si: 0.0034 L_grad: 0.0226 
Train Epoch: 58 [612/816 (75%)] loss: 0.0343 L_si: 0.0057 L_grad: 0.0286 
Train Epoch: 58 [648/816 (79%)] loss: 0.0192 L_si: 0.0014 L_grad: 0.0178 
Train Epoch: 58 [684/816 (84%)] loss: 0.0220 L_si: 0.0027 L_grad: 0.0193 
Train Epoch: 58 [720/816 (88%)] loss: 0.0266 L_si: 0.0027 L_grad: 0.0239 
Train Epoch: 58 [756/816 (93%)] loss: 0.0356 L_si: 0.0074 L_grad: 0.0281 
Train Epoch: 58 [792/816 (97%)] loss: 0.0222 L_si: 0.0018 L_grad: 0.0204 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.043103545904159546, 0.04152648523449898, 0.040152471512556076, 0.04060203582048416, 0.04235859960317612, 0.04060419648885727, 0.041123177856206894, 0.04072365537285805, 0.041107915341854095, 0.03684621304273605, 0.03825054317712784, 0.04271771013736725, 0.03985428437590599, 0.04242870956659317, 0.03992883861064911, 0.04159770533442497, 0.038612209260463715, 0.034403346478939056, 0.012882825918495655], 'L_si': [0.008255675435066223, 0.008457314223051071, 0.008654028177261353, 0.00738883763551712, 0.008393794298171997, 0.0069319382309913635, 0.007427576929330826, 0.006449811160564423, 0.006679939106106758, 0.0054539404809474945, 0.007134264335036278, 0.009593822062015533, 0.006674852222204208, 0.007883261889219284, 0.006740424782037735, 0.007862042635679245, 0.006804019212722778, 0.006018038839101791, 0.005512576550245285], 'L_grad': [0.03484787046909332, 0.033069171011447906, 0.031498443335294724, 0.03321319818496704, 0.03396480530500412, 0.033672258257865906, 0.03369560092687607, 0.034273844212293625, 0.03442797809839249, 0.03139227256178856, 0.03111627697944641, 0.033123888075351715, 0.03317943215370178, 0.034545447677373886, 0.033188413828611374, 0.03373566269874573, 0.031808190047740936, 0.028385305777192116, 0.00737024936825037]}
Train Epoch: 59 [0/816 (0%)] loss: 0.0299 L_si: 0.0058 L_grad: 0.0241 
Train Epoch: 59 [36/816 (4%)] loss: 0.0285 L_si: 0.0058 L_grad: 0.0228 
Train Epoch: 59 [72/816 (9%)] loss: 0.0234 L_si: 0.0025 L_grad: 0.0209 
Train Epoch: 59 [108/816 (13%)] loss: 0.0284 L_si: 0.0058 L_grad: 0.0226 
Train Epoch: 59 [144/816 (18%)] loss: 0.0319 L_si: 0.0061 L_grad: 0.0258 
Train Epoch: 59 [180/816 (22%)] loss: 0.0272 L_si: 0.0039 L_grad: 0.0233 
Train Epoch: 59 [216/816 (26%)] loss: 0.0335 L_si: 0.0072 L_grad: 0.0263 
Train Epoch: 59 [252/816 (31%)] loss: 0.0307 L_si: 0.0070 L_grad: 0.0238 
Train Epoch: 59 [288/816 (35%)] loss: 0.0329 L_si: 0.0050 L_grad: 0.0278 
Train Epoch: 59 [324/816 (40%)] loss: 0.0301 L_si: 0.0059 L_grad: 0.0241 
Train Epoch: 59 [360/816 (44%)] loss: 0.0270 L_si: 0.0040 L_grad: 0.0230 
Train Epoch: 59 [396/816 (49%)] loss: 0.0264 L_si: 0.0048 L_grad: 0.0216 
Train Epoch: 59 [432/816 (53%)] loss: 0.0245 L_si: 0.0033 L_grad: 0.0211 
Train Epoch: 59 [468/816 (57%)] loss: 0.0235 L_si: 0.0034 L_grad: 0.0201 
Train Epoch: 59 [504/816 (62%)] loss: 0.0311 L_si: 0.0052 L_grad: 0.0259 
Train Epoch: 59 [540/816 (66%)] loss: 0.0252 L_si: 0.0049 L_grad: 0.0203 
Train Epoch: 59 [576/816 (71%)] loss: 0.0251 L_si: 0.0033 L_grad: 0.0217 
Train Epoch: 59 [612/816 (75%)] loss: 0.0192 L_si: 0.0019 L_grad: 0.0173 
Train Epoch: 59 [648/816 (79%)] loss: 0.0331 L_si: 0.0089 L_grad: 0.0242 
Train Epoch: 59 [684/816 (84%)] loss: 0.0230 L_si: 0.0022 L_grad: 0.0208 
Train Epoch: 59 [720/816 (88%)] loss: 0.0297 L_si: 0.0039 L_grad: 0.0258 
Train Epoch: 59 [756/816 (93%)] loss: 0.0256 L_si: 0.0036 L_grad: 0.0221 
Train Epoch: 59 [792/816 (97%)] loss: 0.0419 L_si: 0.0126 L_grad: 0.0293 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04061125963926315, 0.03873422369360924, 0.03877083584666252, 0.03978097811341286, 0.039283789694309235, 0.03847993165254593, 0.03832036629319191, 0.038561850786209106, 0.040676966309547424, 0.03839227184653282, 0.04115995764732361, 0.039624448865652084, 0.04195370152592659, 0.04101189225912094, 0.037077855318784714, 0.040303993970155716, 0.040534161031246185, 0.03814096748828888, 0.011782325804233551], 'L_si': [0.006712786853313446, 0.0068992674350738525, 0.006532248109579086, 0.00621698796749115, 0.0064284708350896835, 0.00827222503721714, 0.005476091057062149, 0.007202873006463051, 0.006518082693219185, 0.006674226373434067, 0.007126703858375549, 0.006405625492334366, 0.006793469190597534, 0.007891109213232994, 0.006325177848339081, 0.007977385073900223, 0.007855556905269623, 0.006347300484776497, 0.004681714810431004], 'L_grad': [0.03389847278594971, 0.031834956258535385, 0.032238587737083435, 0.03356399014592171, 0.0328553169965744, 0.03020770475268364, 0.03284427523612976, 0.031358979642391205, 0.03415888547897339, 0.031718045473098755, 0.03403325378894806, 0.03321882337331772, 0.035160232335329056, 0.0331207811832428, 0.030752677470445633, 0.03232660889625549, 0.03267860412597656, 0.03179366514086723, 0.0071006109938025475]}
Train Epoch: 60 [0/816 (0%)] loss: 0.0248 L_si: 0.0029 L_grad: 0.0218 
Train Epoch: 60 [36/816 (4%)] loss: 0.0340 L_si: 0.0062 L_grad: 0.0278 
Train Epoch: 60 [72/816 (9%)] loss: 0.0325 L_si: 0.0059 L_grad: 0.0266 
Train Epoch: 60 [108/816 (13%)] loss: 0.0291 L_si: 0.0038 L_grad: 0.0253 
Train Epoch: 60 [144/816 (18%)] loss: 0.0346 L_si: 0.0067 L_grad: 0.0279 
Train Epoch: 60 [180/816 (22%)] loss: 0.0265 L_si: 0.0025 L_grad: 0.0241 
Train Epoch: 60 [216/816 (26%)] loss: 0.0260 L_si: 0.0030 L_grad: 0.0230 
Train Epoch: 60 [252/816 (31%)] loss: 0.0234 L_si: 0.0029 L_grad: 0.0205 
Train Epoch: 60 [288/816 (35%)] loss: 0.0336 L_si: 0.0083 L_grad: 0.0253 
Train Epoch: 60 [324/816 (40%)] loss: 0.0249 L_si: 0.0027 L_grad: 0.0222 
Train Epoch: 60 [360/816 (44%)] loss: 0.0257 L_si: 0.0041 L_grad: 0.0216 
Train Epoch: 60 [396/816 (49%)] loss: 0.0243 L_si: 0.0029 L_grad: 0.0214 
Train Epoch: 60 [432/816 (53%)] loss: 0.0253 L_si: 0.0030 L_grad: 0.0223 
Train Epoch: 60 [468/816 (57%)] loss: 0.0264 L_si: 0.0041 L_grad: 0.0223 
Train Epoch: 60 [504/816 (62%)] loss: 0.0251 L_si: 0.0029 L_grad: 0.0222 
Train Epoch: 60 [540/816 (66%)] loss: 0.0225 L_si: 0.0022 L_grad: 0.0204 
Train Epoch: 60 [576/816 (71%)] loss: 0.0248 L_si: 0.0037 L_grad: 0.0211 
Train Epoch: 60 [612/816 (75%)] loss: 0.0367 L_si: 0.0067 L_grad: 0.0301 
Train Epoch: 60 [648/816 (79%)] loss: 0.0387 L_si: 0.0108 L_grad: 0.0279 
Train Epoch: 60 [684/816 (84%)] loss: 0.0335 L_si: 0.0075 L_grad: 0.0260 
Train Epoch: 60 [720/816 (88%)] loss: 0.0220 L_si: 0.0027 L_grad: 0.0193 
Train Epoch: 60 [756/816 (93%)] loss: 0.0329 L_si: 0.0059 L_grad: 0.0269 
Train Epoch: 60 [792/816 (97%)] loss: 0.0316 L_si: 0.0052 L_grad: 0.0264 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
Saving checkpoint: /root/autodl-tmp/train_il_200e/train_s2d_SpikeTransformer/checkpoint-epoch060-loss-0.0283.pth.tar ...
all losses in batch in validation:  {'loss': [0.03779188543558121, 0.039514169096946716, 0.04128928482532501, 0.03931082785129547, 0.03769495710730553, 0.04024891555309296, 0.03936340659856796, 0.040835026651620865, 0.042013783007860184, 0.03754546493291855, 0.04585227742791176, 0.03354363888502121, 0.039459872990846634, 0.040720850229263306, 0.036622170358896255, 0.04008614271879196, 0.04292166605591774, 0.044016435742378235, 0.013884784653782845], 'L_si': [0.007092587649822235, 0.007727876305580139, 0.00814354233443737, 0.007582830265164375, 0.007390381768345833, 0.008553212508559227, 0.006700186058878899, 0.007210187613964081, 0.007941614836454391, 0.006620541214942932, 0.010669033974409103, 0.005934201180934906, 0.00824803113937378, 0.008127249777317047, 0.006647661328315735, 0.006913015618920326, 0.008549164980649948, 0.008567219600081444, 0.006583496928215027], 'L_grad': [0.03069929964840412, 0.03178629279136658, 0.03314574062824249, 0.03172799572348595, 0.030304575338959694, 0.03169570118188858, 0.032663218677043915, 0.033624839037656784, 0.03407216817140579, 0.030924921855330467, 0.035183243453502655, 0.027609439566731453, 0.031211841851472855, 0.03259360045194626, 0.02997450903058052, 0.033173128962516785, 0.03437250107526779, 0.03544921427965164, 0.007301287725567818]}
Train Epoch: 61 [0/816 (0%)] loss: 0.0297 L_si: 0.0035 L_grad: 0.0261 
Train Epoch: 61 [36/816 (4%)] loss: 0.0430 L_si: 0.0105 L_grad: 0.0325 
Train Epoch: 61 [72/816 (9%)] loss: 0.0260 L_si: 0.0031 L_grad: 0.0229 
Train Epoch: 61 [108/816 (13%)] loss: 0.0352 L_si: 0.0101 L_grad: 0.0250 
Train Epoch: 61 [144/816 (18%)] loss: 0.0280 L_si: 0.0050 L_grad: 0.0230 
Train Epoch: 61 [180/816 (22%)] loss: 0.0247 L_si: 0.0034 L_grad: 0.0213 
Train Epoch: 61 [216/816 (26%)] loss: 0.0426 L_si: 0.0105 L_grad: 0.0321 
Train Epoch: 61 [252/816 (31%)] loss: 0.0245 L_si: 0.0030 L_grad: 0.0215 
Train Epoch: 61 [288/816 (35%)] loss: 0.0291 L_si: 0.0049 L_grad: 0.0242 
Train Epoch: 61 [324/816 (40%)] loss: 0.0312 L_si: 0.0052 L_grad: 0.0260 
Train Epoch: 61 [360/816 (44%)] loss: 0.0314 L_si: 0.0067 L_grad: 0.0247 
Train Epoch: 61 [396/816 (49%)] loss: 0.0221 L_si: 0.0023 L_grad: 0.0198 
Train Epoch: 61 [432/816 (53%)] loss: 0.0234 L_si: 0.0028 L_grad: 0.0206 
Train Epoch: 61 [468/816 (57%)] loss: 0.0330 L_si: 0.0065 L_grad: 0.0265 
Train Epoch: 61 [504/816 (62%)] loss: 0.0190 L_si: 0.0019 L_grad: 0.0171 
Train Epoch: 61 [540/816 (66%)] loss: 0.0258 L_si: 0.0033 L_grad: 0.0225 
Train Epoch: 61 [576/816 (71%)] loss: 0.0227 L_si: 0.0037 L_grad: 0.0190 
Train Epoch: 61 [612/816 (75%)] loss: 0.0270 L_si: 0.0034 L_grad: 0.0236 
Train Epoch: 61 [648/816 (79%)] loss: 0.0274 L_si: 0.0047 L_grad: 0.0227 
Train Epoch: 61 [684/816 (84%)] loss: 0.0239 L_si: 0.0032 L_grad: 0.0207 
Train Epoch: 61 [720/816 (88%)] loss: 0.0239 L_si: 0.0030 L_grad: 0.0208 
Train Epoch: 61 [756/816 (93%)] loss: 0.0281 L_si: 0.0041 L_grad: 0.0240 
Train Epoch: 61 [792/816 (97%)] loss: 0.0307 L_si: 0.0040 L_grad: 0.0267 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.03990662470459938, 0.03740798681974411, 0.04096763953566551, 0.04862593859434128, 0.043707821518182755, 0.04501427337527275, 0.04307355359196663, 0.04321390017867088, 0.04591207206249237, 0.04053908586502075, 0.04282139241695404, 0.04182320460677147, 0.04249795153737068, 0.028546929359436035, 0.03861332684755325, 0.03883194923400879, 0.04000246524810791, 0.04154675826430321, 0.019343165680766106], 'L_si': [0.008350010961294174, 0.006430992856621742, 0.008081678301095963, 0.01023866981267929, 0.009423691779375076, 0.008167114108800888, 0.008613735437393188, 0.008262641727924347, 0.010612145066261292, 0.008345192298293114, 0.009565787389874458, 0.008581232279539108, 0.008320096880197525, 0.0025823721662163734, 0.007712269201874733, 0.007832473143935204, 0.0075582824647426605, 0.008244697004556656, 0.010305505245923996], 'L_grad': [0.031556613743305206, 0.030976993963122368, 0.03288596123456955, 0.03838726878166199, 0.03428412973880768, 0.03684715926647186, 0.03445981815457344, 0.034951258450746536, 0.03529992699623108, 0.03219389170408249, 0.03325560688972473, 0.03324197232723236, 0.03417785465717316, 0.025964558124542236, 0.03090105950832367, 0.030999474227428436, 0.03244418278336525, 0.03330206125974655, 0.00903766043484211]}
Train Epoch: 62 [0/816 (0%)] loss: 0.0230 L_si: 0.0024 L_grad: 0.0206 
Train Epoch: 62 [36/816 (4%)] loss: 0.0220 L_si: 0.0024 L_grad: 0.0196 
Train Epoch: 62 [72/816 (9%)] loss: 0.0353 L_si: 0.0045 L_grad: 0.0308 
Train Epoch: 62 [108/816 (13%)] loss: 0.0303 L_si: 0.0068 L_grad: 0.0235 
Train Epoch: 62 [144/816 (18%)] loss: 0.0250 L_si: 0.0031 L_grad: 0.0219 
Train Epoch: 62 [180/816 (22%)] loss: 0.0347 L_si: 0.0089 L_grad: 0.0258 
Train Epoch: 62 [216/816 (26%)] loss: 0.0213 L_si: 0.0024 L_grad: 0.0190 
Train Epoch: 62 [252/816 (31%)] loss: 0.0341 L_si: 0.0074 L_grad: 0.0267 
Train Epoch: 62 [288/816 (35%)] loss: 0.0300 L_si: 0.0060 L_grad: 0.0240 
Train Epoch: 62 [324/816 (40%)] loss: 0.0297 L_si: 0.0051 L_grad: 0.0246 
Train Epoch: 62 [360/816 (44%)] loss: 0.0313 L_si: 0.0043 L_grad: 0.0271 
Train Epoch: 62 [396/816 (49%)] loss: 0.0235 L_si: 0.0027 L_grad: 0.0208 
Train Epoch: 62 [432/816 (53%)] loss: 0.0237 L_si: 0.0032 L_grad: 0.0205 
Train Epoch: 62 [468/816 (57%)] loss: 0.0277 L_si: 0.0036 L_grad: 0.0240 
Train Epoch: 62 [504/816 (62%)] loss: 0.0241 L_si: 0.0032 L_grad: 0.0209 
Train Epoch: 62 [540/816 (66%)] loss: 0.0300 L_si: 0.0052 L_grad: 0.0247 
Train Epoch: 62 [576/816 (71%)] loss: 0.0267 L_si: 0.0031 L_grad: 0.0237 
Train Epoch: 62 [612/816 (75%)] loss: 0.0302 L_si: 0.0061 L_grad: 0.0241 
Train Epoch: 62 [648/816 (79%)] loss: 0.0271 L_si: 0.0028 L_grad: 0.0243 
Train Epoch: 62 [684/816 (84%)] loss: 0.0280 L_si: 0.0055 L_grad: 0.0225 
Train Epoch: 62 [720/816 (88%)] loss: 0.0239 L_si: 0.0024 L_grad: 0.0215 
Train Epoch: 62 [756/816 (93%)] loss: 0.0356 L_si: 0.0074 L_grad: 0.0282 
Train Epoch: 62 [792/816 (97%)] loss: 0.0238 L_si: 0.0022 L_grad: 0.0216 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.04085991159081459, 0.04662884399294853, 0.04349244013428688, 0.04146320000290871, 0.04284210503101349, 0.04015004262328148, 0.04264622926712036, 0.040070563554763794, 0.04111878201365471, 0.04219469800591469, 0.04298669472336769, 0.04002286121249199, 0.03930392861366272, 0.042812321335077286, 0.04042363539338112, 0.04024409502744675, 0.044096410274505615, 0.04107140004634857, 0.014243938960134983], 'L_si': [0.007792238146066666, 0.010260697454214096, 0.008535753935575485, 0.009164702147245407, 0.008540362119674683, 0.00793541595339775, 0.007993757724761963, 0.007803160697221756, 0.008446130901575089, 0.008389536291360855, 0.008526396006345749, 0.007509991526603699, 0.007295429706573486, 0.008346367627382278, 0.007626384496688843, 0.007683590054512024, 0.008005306124687195, 0.00726848840713501, 0.006058329716324806], 'L_grad': [0.033067673444747925, 0.036368146538734436, 0.034956686198711395, 0.0322984978556633, 0.034301742911338806, 0.03221462666988373, 0.0346524715423584, 0.03226740285754204, 0.03267265111207962, 0.03380516171455383, 0.03446029871702194, 0.03251286968588829, 0.03200849890708923, 0.03446595370769501, 0.032797250896692276, 0.03256050497293472, 0.03609110414981842, 0.03380291163921356, 0.008185609243810177]}
Train Epoch: 63 [0/816 (0%)] loss: 0.0405 L_si: 0.0130 L_grad: 0.0275 
Train Epoch: 63 [36/816 (4%)] loss: 0.0294 L_si: 0.0058 L_grad: 0.0236 
Train Epoch: 63 [72/816 (9%)] loss: 0.0348 L_si: 0.0103 L_grad: 0.0245 
Train Epoch: 63 [108/816 (13%)] loss: 0.0248 L_si: 0.0033 L_grad: 0.0215 
Train Epoch: 63 [144/816 (18%)] loss: 0.0244 L_si: 0.0032 L_grad: 0.0212 
Train Epoch: 63 [180/816 (22%)] loss: 0.0369 L_si: 0.0074 L_grad: 0.0295 
Train Epoch: 63 [216/816 (26%)] loss: 0.0279 L_si: 0.0051 L_grad: 0.0228 
Train Epoch: 63 [252/816 (31%)] loss: 0.0277 L_si: 0.0045 L_grad: 0.0232 
Train Epoch: 63 [288/816 (35%)] loss: 0.0297 L_si: 0.0041 L_grad: 0.0256 
Train Epoch: 63 [324/816 (40%)] loss: 0.0252 L_si: 0.0028 L_grad: 0.0224 
Train Epoch: 63 [360/816 (44%)] loss: 0.0277 L_si: 0.0035 L_grad: 0.0242 
Train Epoch: 63 [396/816 (49%)] loss: 0.0253 L_si: 0.0039 L_grad: 0.0215 
Train Epoch: 63 [432/816 (53%)] loss: 0.0278 L_si: 0.0028 L_grad: 0.0251 
Train Epoch: 63 [468/816 (57%)] loss: 0.0297 L_si: 0.0037 L_grad: 0.0260 
Train Epoch: 63 [504/816 (62%)] loss: 0.0275 L_si: 0.0074 L_grad: 0.0201 
Train Epoch: 63 [540/816 (66%)] loss: 0.0245 L_si: 0.0045 L_grad: 0.0201 
Train Epoch: 63 [576/816 (71%)] loss: 0.0239 L_si: 0.0029 L_grad: 0.0210 
Train Epoch: 63 [612/816 (75%)] loss: 0.0336 L_si: 0.0104 L_grad: 0.0231 
Train Epoch: 63 [648/816 (79%)] loss: 0.0223 L_si: 0.0024 L_grad: 0.0198 
Train Epoch: 63 [684/816 (84%)] loss: 0.0274 L_si: 0.0045 L_grad: 0.0229 
Train Epoch: 63 [720/816 (88%)] loss: 0.0329 L_si: 0.0057 L_grad: 0.0272 
Train Epoch: 63 [756/816 (93%)] loss: 0.0323 L_si: 0.0057 L_grad: 0.0265 
Train Epoch: 63 [792/816 (97%)] loss: 0.0223 L_si: 0.0021 L_grad: 0.0202 
Validation: [0/228 (0%)]
Validation: [36/228 (16%)]
Validation: [72/228 (32%)]
Validation: [108/228 (47%)]
Validation: [144/228 (63%)]
Validation: [180/228 (79%)]
Validation: [216/228 (95%)]
all losses in batch in validation:  {'loss': [0.039371415972709656, 0.040976352989673615, 0.03965071588754654, 0.034016866236925125, 0.039271511137485504, 0.03848830610513687, 0.03961782902479172, 0.03738018125295639, 0.04121562838554382, 0.03673165664076805, 0.04117614030838013, 0.03730904683470726, 0.04078361392021179, 0.040151987224817276, 0.04051198810338974, 0.04014645144343376, 0.042191702872514725, 0.04217524826526642, 0.014653708785772324], 'L_si': [0.007178802043199539, 0.00873778760433197, 0.007033383473753929, 0.004978506825864315, 0.007536768913269043, 0.004998341202735901, 0.007136527448892593, 0.005958296358585358, 0.007510652765631676, 0.0061873942613601685, 0.007110143080353737, 0.007023731246590614, 0.007166501134634018, 0.007261555641889572, 0.008281545713543892, 0.008427739143371582, 0.007300671190023422, 0.00768691673874855, 0.006050998345017433], 'L_grad': [0.03219261392951012, 0.032238565385341644, 0.03261733055114746, 0.029038360342383385, 0.03173474222421646, 0.03348996490240097, 0.032481301575899124, 0.03142188489437103, 0.033704973757267, 0.030544262379407883, 0.03406599536538124, 0.030285315588116646, 0.033617112785577774, 0.032890431582927704, 0.032230444252491, 0.03171871230006218, 0.0348910316824913, 0.03448833152651787, 0.00860271044075489]}
Train Epoch: 64 [0/816 (0%)] loss: 0.0306 L_si: 0.0059 L_grad: 0.0247 
Train Epoch: 64 [36/816 (4%)] loss: 0.0288 L_si: 0.0039 L_grad: 0.0250 
Train Epoch: 64 [72/816 (9%)] loss: 0.0266 L_si: 0.0036 L_grad: 0.0229 
Train Epoch: 64 [108/816 (13%)] loss: 0.0313 L_si: 0.0049 L_grad: 0.0264 
Train Epoch: 64 [144/816 (18%)] loss: 0.0272 L_si: 0.0036 L_grad: 0.0236 
Train Epoch: 64 [180/816 (22%)] loss: 0.0323 L_si: 0.0053 L_grad: 0.0270 
Train Epoch: 64 [216/816 (26%)] loss: 0.0310 L_si: 0.0050 L_grad: 0.0260 
Train Epoch: 64 [252/816 (31%)] loss: 0.0208 L_si: 0.0020 L_grad: 0.0188 
Train Epoch: 64 [288/816 (35%)] loss: 0.0295 L_si: 0.0038 L_grad: 0.0257 
Train Epoch: 64 [324/816 (40%)] loss: 0.0284 L_si: 0.0043 L_grad: 0.0241 
Train Epoch: 64 [360/816 (44%)] loss: 0.0288 L_si: 0.0042 L_grad: 0.0246 
Train Epoch: 64 [396/816 (49%)] loss: 0.0308 L_si: 0.0057 L_grad: 0.0251 
Train Epoch: 64 [432/816 (53%)] loss: 0.0349 L_si: 0.0078 L_grad: 0.0271 
Train Epoch: 64 [468/816 (57%)] loss: 0.0242 L_si: 0.0022 L_grad: 0.0219 
Train Epoch: 64 [504/816 (62%)] loss: 0.0238 L_si: 0.0025 L_grad: 0.0213 
Train Epoch: 64 [540/816 (66%)] loss: 0.0240 L_si: 0.0030 L_grad: 0.0210 
Train Epoch: 64 [576/816 (71%)] loss: 0.0374 L_si: 0.0084 L_grad: 0.0290 
Train Epoch: 64 [612/816 (75%)] loss: 0.0266 L_si: 0.0039 L_grad: 0.0227 
Train Epoch: 64 [648/816 (79%)] loss: 0.0230 L_si: 0.0034 L_grad: 0.0197 
Train Epoch: 64 [684/816 (84%)] loss: 0.0247 L_si: 0.0026 L_grad: 0.0222 
